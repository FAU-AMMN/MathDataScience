%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Mathematik für Data Science}
\date{Oct 01, 2021}
\release{}
\author{M.\@{} Burger}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


\noindent\sphinxincludegraphics{{intro_1_0}.png}

Diese Vorlesung gibt einen ersten Einblick in grundlegende mathematische Konzepte und
Denkweisen, die in der heutigen Mathematikausbildung in Vorlesungen zur Analysis und
linearen Algebra unterteilt werden. In dieser Vorlesung soll eine integrierte Darstellung
der wichtigsten Inhalte, wie sie in mathematiknahen Studiengängen benötigt werden, gegeben werden.


\chapter{Vorkurs}
\label{\detokenize{vorkurs/vorkurs:vorkurs}}\label{\detokenize{vorkurs/vorkurs::doc}}
In diesem Abschnitt werden wir uns mit einigen heranführenden Themen beschäftigen, u.a. werden wir zunächst eine kleine Motivation mathematischer Probleme in Data Science geben und danach einige Kapitel aus dem Schulstoff wiederholen, die wichtig für das weitere Verständnis sind.


\section{Regression}
\label{\detokenize{vorkurs/regression:regression}}\label{\detokenize{vorkurs/regression::doc}}
Wir beginnen mit einem der klassischen Probleme in der Datenanalyse, wie man es in vielen Disziplinen findet, nämlich die Konstruktion eines funktionalen Zusammenhangs aus einer endlichen Anzahl an Messdaten. Wir nehmen an, dass
wir Messdaten \(x_i\) als Eingabe und \(y_i\) als Ausgabe gegeben haben, für \(i=1,\ldots, N\). Nun suchen wir einen funktionalen Zusammenhang, also \(y=f(x)\). Dies geschieht bei der Regression durch einen parametrischen Ansatz
\begin{equation*}
\begin{split} f(x) = F(x,\theta_1,\ldots,\theta_M) \end{split}
\end{equation*}
mit einer gegebenen Form \(F\) und   zu bestimmenden Parametern \(\theta_j\). Das einfachste Beispiel ist die lineare Regression f”ur skalare Daten, d.h.
\begin{equation}\label{equation:vorkurs/regression:eq:linreg}
\begin{split}F(x,\theta_1,\theta_2) = \theta_1 x + \theta_2 .\end{split}
\end{equation}
Unter der Annahme der Exaktheit von funktionaler Form und Messung hätte man ein Gleichungssystem für \(\theta_1\) und \(\theta_2\) zu lösen, nämlich
\begin{equation}\label{equation:vorkurs/regression:eq:nonlinreg}
\begin{split} F(x_i,\theta_1,\ldots,\theta_M) = y_i, \qquad i=1,\ldots,N. \end{split}
\end{equation}
Im allgemeinen ist dieses System aber nicht lösbar oder eine Lösung gar nicht sinnvoll aus den folgenden Gründen:
\begin{itemize}
\item {} 
Die Messdaten \(x_i\), \(y_i\) weisen Fehler auf.

\item {} 
Die angenommene Funktionenklasse erklärt das Verhalten der Daten nur approximativ.

\item {} 
Man hat zusätzlich zu viele Messungen (\(N > M\)), z.B. \(N > 2\) bei der linearen Regression in \eqref{equation:vorkurs/regression:eq:linreg}. Damit hat man mehr Gleichungen als Unbekannte und das Gleichungssystem ist eventuell nicht mehr lösbar. In der Praxis versucht man oft zur Sicherheit lieber mehr Messungen zu machen, um Fehler auszugleichen, also ist dies ein sehr häufiger Fall.

\end{itemize}

\noindent\sphinxincludegraphics{{C:/Tim/Uni/Lectures/MathDataScience/_build/jupyter_execute\regression_1_0}.png}

Deshalb ist es naheliegend die Gleichheit nur approximativ zu erfüllen, man könnte also alle Parameter \(\theta_1,\ldots,\theta_M\) akzeptieren, die zu kleinen Fehlern im Gleichungssystem führen. Wie können wir aber solche Parameter mit einer nachvollziehbaren Rechenvorschrift berechnen ? Ein Ansatz dazu ist das Kleinstquadrate\sphinxhyphen{}Prinzip, dass schon von Gauss im frühen neunzehnten Jahrhundert angewandt wurde. Statt der Lösung eines Gleichungssystems betrachtet man nun die Lösung eines Optimierungsproblems, wir suchen \(\theta_1,\ldots,\theta_M\) als Lösung von
\begin{equation}\label{equation:vorkurs/regression:eq:nonlinregoptim}
\begin{split}\min \sum_{i=1}^N ( F(x_i,\theta_1,\ldots,\theta_M) - y_i)^2.\end{split}
\end{equation}
Wir minimieren also in Summe den quadratischen Abstand zwischen der Modellvorhersage und den gemessenen Daten.
Da wir eine Summe von Quadraten, also nichtnegativen Termen, haben, sehen wir sofort dass der Minimalwert selbst bei optimaler Wahl der \(\theta_i\) größer oder gleich Null liegen muss. Damit sehen wir sofort, dass eine Lösung des Gleichungssystems \eqref{equation:vorkurs/regression:eq:nonlinreg} auch immer eine Lösung des Optimierungsproblems \eqref{equation:vorkurs/regression:eq:nonlinregoptim} ist.

Mathematisch und praktisch stellen sich dabei sofort einige Fragen:
\begin{itemize}
\item {} 
Gibt es überhaupt eine Lösung des Problems ?

\item {} 
Wenn ja, wie können wir das Minimum berechnen ?

\item {} 
Ist das Problem robust gegenüber Messfehlern, d.h. ändert sich die Lösung nur wenig wenn wir die Daten \(x_i\) und \(y_i\) geringfügig ändern ?

\item {} 
Was passiert mit der Lösung wenn wir weitere Messungen machen, d.h. wenn sich \(N\) ändert ?

\end{itemize}


\subsection{Lineare Regression}
\label{\detokenize{vorkurs/regression:lineare-regression}}
Wir wollen die obigen Fragen nun im Rahmen der linearen Regression näher betrachten, dort wollen wir eine Funktion von zwei Variablen
\begin{equation*}
\begin{split} f(\theta_1,\theta_2) =  \sum_{i=1}^N (\theta_1 x_i + \theta_2 -y_i)^2 \end{split}
\end{equation*}
minimieren.

Um eine erste Idee zu bekommen, nehmen wir an \(\theta_2 = 0\) ist schon bekannt. Dann bleibt uns nur eine eindimensionale Minimierung der quadratischen Funktion
\begin{equation*}
\begin{split} g(\theta_1) =  \sum_{i=1}^N (\theta_1 x_i   -y_i)^2 .\end{split}
\end{equation*}
Wir bestimmen das Minimum durch Ableitung mit Kettenregel:
\begin{equation*}
\begin{split} g'(\theta_1) = 2 \sum_{i=1}^N x_i (\theta_1 x_i   -y_i) = (2 \sum_i x_i^2) \theta_1 - 2 \sum_i x_i y_i. \end{split}
\end{equation*}
Also folgt als stationärer Punkt \(g'(\overline{\theta}_1) = 0\)
\begin{equation*}
\begin{split} \overline{\theta}_1 = \frac{\sum_i x_i y_i}{\sum_i x_i^2}.\end{split}
\end{equation*}
Zur Überprüfung berechnen wir auch noch die zweite Ableitung
\begin{equation*}
\begin{split} g'(\theta_1) =  2 \sum_{i=1}^N x_i^2, \end{split}
\end{equation*}
die offensichtlich positiv ist, wenn wir nicht alle Messungen bei \(x_i = 0\) durchgeführt haben. Also ist der stationäre Punkt auch wirklich ein Minimierer von \(g\).

Nun betrachten wir analog den Fall, dass \(\theta_1 = 0\) schon bekannt ist. In diesem Fall bleibt die quadratische Funktion
\begin{equation*}
\begin{split}  g(\theta_2) =  \sum_{i=1}^N (\theta_2   -y_i)^2 .\end{split}
\end{equation*}
Hier erhalten wir als stationären Punkt
\begin{equation*}
\begin{split} \overline{\theta}_2 = \frac{1}N  \sum_{i=1}^N y_i, \end{split}
\end{equation*}
also den Mittelwert der Daten \(y_i\).

Daraus versuchen wir uns zu überlegen wie die Lösung im allgemeinen Fall aussehen würde. Um das optimale \(\theta_1\) zu bestimmen, haben wir eigentlich den Fall oben, wenn wir statt \(y_i\) die verschobenen Daten \(y_i - \theta_2\) betrachten.
Dies f”uhrt auf
\begin{equation*}
\begin{split} \overline{\theta}_1 = \frac{\sum_{i=1}^N  x_i (y_i-\overline{\theta}_2)}{\sum_{i=1}^N  x_i^2}. \end{split}
\end{equation*}
Analog können wir im Problem f”ur \(\theta_2\) die verschobenen Daten \(y_i - \theta_1 x_i\) betrachten und erhalten
\begin{equation*}
\begin{split} \overline{\theta}_2 = \frac{1}N \sum_{i=1}^N   (y_i - \overline{\theta}_1 x_i). \end{split}
\end{equation*}
Wir haben also eigentlich ein Gleichungssystem der Form
\label{equation:vorkurs/regression:cecadb3a-3731-4682-ab77-516d581e0bb2}\begin{align}
(\sum_{i=1}^N x_i^2) \theta_1 + (\sum_{i=1}^N x_i) \theta_2 &= \sum_{i=1}^N x_i y_i \\
(\sum_{i=1}^N x_i) \theta_1 + N \theta_2 &= \sum_{i=1}^N y_i
\end{align}
zu lösen. Daraus erhalten wir
\label{equation:vorkurs/regression:4a1595a2-887c-40d5-a341-01f9c08dea66}\begin{align}
\overline{\theta}_1 &=  \frac{N \sum_{i=1}^N x_i y_i - (\sum_{i=1}^N x_i) (\sum_{i=1}^N y_i)}{N \sum_{i=1}^N x_i^2 - (\sum_{i=1}^N x_i)^2} \\
\overline{\theta}_2 &=  \frac{(\sum_{i=1}^N x_i^2) (\sum_{i=1}^N y_i) - (\sum_{i=1}^N x_i y_i) (\sum_{i=1}^N x_i)}{N \sum_{i=1}^N x_i^2 - (\sum_{i=1}^N x_i)^2}.
\end{align}
Wir beachten, dass die Ungleichung
\begin{equation*}
\begin{split} (\sum_{i=1}^N x_i)^2 \leq N \sum_{i=1}^N x_i^2, \end{split}
\end{equation*}
auch bekannt als arithmetisch\sphinxhyphen{}quadratische Mittelungleichung
\begin{equation*}
\begin{split} (\frac{1}N \sum_{i=1}^N x_i)^2 \leq \frac{1}N  \sum_{i=1}^N x_i^2\end{split}
\end{equation*}
gilt, mit Gleichheit nur wenn \(x_1=x_2=\ldots=x_N\). Hat man also nicht alle Messungen an der gleichen Stelle \(x_1\) durchgeführt, dann ist der Nenner positiv, insbesondere nicht Null.


\section{Wiederholung: Koordinatensysteme und der Satz des Pythagoras}
\label{\detokenize{vorkurs/koord:wiederholung-koordinatensysteme-und-der-satz-des-pythagoras}}\label{\detokenize{vorkurs/koord::doc}}
Eine elementares Resultat der Euklidischen Geometrie ist der Satz des Pythagoras, der meist als
\begin{equation*}
\begin{split} a^2 + b^2 = c^2 \end{split}
\end{equation*}
formuliert wird, wobei \(a\) und \(b\) die Katheten und \(c\) die Hypothenuse eines rechtwinkeligen Dreiecks sind.
Wir können den Satz von Pythagoras aber auch völlig anders interpretieren, indem wir ein Koordinatensystem in die Katheten legen, sodass die Eckpunkte \((0,0)\), \((a,0)\), und \((a,b)\) sind. Dann sind die Katheten die Vektoren \((a,0)\) und \((0,b)\) und die Hypothenuse der Vektor \((a,b)\). Der Satz des Pythagoras sagt dann, dass die Länge des Vektors \(v=(a,b)\) gleich \(\sqrt{a^2+b^2}\) ist. Dies bezeichnen wir als die Euklidische Norm eines Vektors (in der Schule auch Betrag eines Vektors)
\begin{equation*}
\begin{split} \Vert (x_1,x_2) \Vert = \sqrt{x_1^2+x_2^2}. \end{split}
\end{equation*}
\noindent{\hspace*{\fill}\sphinxincludegraphics{{pythagoras}.png}\hspace*{\fill}}

Basiereend auf der Vektordarstellung können wir auch einen einfachen Beweis des Satz des Pythagoras in einer allgemeinen Version geben. Seien \(v=(x_1,x_2)\) und \(w=(y_1,y_2)\) zwei Vektoren im \(\R^2\). Wie üblich definieren wir ihr Skalarprodukt als
\begin{equation*}
\begin{split} v \cdot w = x_1 y_1 + x_2 y_2 . \end{split}
\end{equation*}
Wir beachten, dass \(\Vert v \Vert^2 = v \cdot v\) gilt.
Wir nennen zwei Vektoren orthogonal zueinander, wenn \(v \cdot w = 0\) gilt, im \(\R^2\) bedeutet dies einen Winkel von 90 Grad.
Nun können wir den Satz des Pythagoras folgenderma\{\textbackslash{}ss\}en formulieren: Sind \(v,w \in \R^2\) zwei zueinander orthogonale Vektoren, dann gilt in der Euklidischen Norm
\begin{equation*}
\begin{split} \Vert v - w \Vert^2 =  \Vert v \Vert^2 + \Vert w \Vert^2. \end{split}
\end{equation*}
Wir benutzen die Darstellung über die Euklidische Norm
\begin{equation*}
\begin{split} \Vert v - w \Vert^2 = (v-w) \cdot (v-w) = v \cdot v + w\cdot w - 2 v \cdot w = v \cdot v + w\cdot w  \Vert v \Vert^2 + \Vert w \Vert^2. \end{split}
\end{equation*}
Wir werden sehen, dass wir solche Strukturen stark verallgemeinern können. Ein analoges Resultat gilt im \(\R^n\) für beliebiges \(n\) mit dem Euklidischen Skalarprodukt
\begin{equation*}
\begin{split} v \cdot w = \sum_{i=1}^n x_i y_i . \end{split}
\end{equation*}
Tatsächlich kann man dies viel weiter verallgemeinern, es genügt einen Raum zu haben, in dem man ein Skalarprodukt definieren kann, das in jeder Komponente linear und symmetrisch ist, sowie \(v \cdot v > 0 \) für \(v \neq 0\) erfüllen.
Dann ist durch
\begin{equation*}
\begin{split} \Vert v \Vert = \sqrt{ v \cdot v} \end{split}
\end{equation*}
immer eine Norm definiert.

Eine wichtige Ungleichung in diesem Zusammenhang ist die Cauchy\sphinxhyphen{}Schwarz Ungleichung
\begin{equation*}
\begin{split} v \cdot w \leq \Vert v \Vert~ \Vert w  \Vert\end{split}
\end{equation*}
bzw. im \(\R^n\)
\begin{equation*}
\begin{split} \sum_{i=1}^n x_i y_i \leq \sqrt{\sum_{i=1}^n x_i^2} \sqrt{\sum_{i=1}^n y_i^2} .\end{split}
\end{equation*}
Für \(n=1\) ist die Cauchy\sphinxhyphen{}Schwarz Ungleichung besonders einfach: Sie besagt, dass das Produkt zweier reeller Zahlen kleiner als das Produkt ihrer Beträge ist.

Eine verwandte Ungleichung ist die Young’sche Ungleichung
\begin{equation*}
\begin{split}  v \cdot w \leq \frac{1}2 \Vert v \Vert^2 + \frac{1}2\Vert w  \Vert^2, \end{split}
\end{equation*}
die aus der Cauchy\sphinxhyphen{}Schwarz Ungleichung und der elementaren Ungleichung
\begin{equation*}
\begin{split} ab \leq \frac{1}2 (a+b) \end{split}
\end{equation*}
für \(a,b \in \R\) folgt. Insgesamt ist die Young’sche Ungleichung aber nur eine Umformulierung von
\begin{equation*}
\begin{split} \frac{1}2 \Vert v - w \Vert^2 \geq 0, \end{split}
\end{equation*}
was wegen der Nichtnegativität der Norm klar ist. Noch einfacher als die Young’sche Ungleichung ist die Dreiecksungleichung
\begin{equation*}
\begin{split} \Vert v + w\Vert \leq \Vert v \Vert + \Vert w \Vert, \end{split}
\end{equation*}
die in einer Dimension (für den Betrag) durch Fallunterscheidung nachgewiesen werden kann. Im Fall der Euklidischen Norm kann man wegen der Nichtnegativität äquivalent beide Seiten quadrieren und die Quadrate der Normen wegkürzen, man endet dann genau bei der Cauchy\sphinxhyphen{}Schwarz Ungleichung.

Für den Beweis der Cauchy\sphinxhyphen{}Schwarz Ungleichung betrachten wir zunächst die triviale Ungleichung
\begin{equation*}
\begin{split} 0 \leq (v-\lambda w)\cdot (v-\lambda w) = \Vert v \Vert^2 - 2 \lambda v \cdot w + \lambda^2 \Vert w \Vert^2, \end{split}
\end{equation*}
für \(\lambda \in \R\). Wir können uns auf den Fall \(w \neq 0\) beschränken, da die Cauchy\sphinxhyphen{}Schwarz Ungleichung sonst klarerweise erfüllt ist, und wählen \(\lambda = \frac{v\cdot w}{\Vert w \Vert^2}\),
\begin{equation*}
\begin{split} 0 \leq  \Vert v \Vert^2 - 2 \frac{v\cdot w}{\Vert w \Vert^2}  v \cdot w + \frac{(v\cdot w)^2}{\Vert w \Vert^2} , \end{split}
\end{equation*}
und nach Umstellen folgt die Ungleichung.


\section{Folgen und Konvergenz}
\label{\detokenize{vorkurs/folgen:folgen-und-konvergenz}}\label{\detokenize{vorkurs/folgen::doc}}
Eine Folge \((x_n)\) reeller Zahlen ist eine Zuordnung \(x_n \in \R\) zu jedem \(n \in \N\). Allgemeiner kann man auch Folgen anderer Objekte betrachten, aber wir belassen es hier bei reellen Folgen. Diese dienen uns als Einstieg ins \sphinxstyleemphasis{Unendliche}, wir wollen daran ein wenig das Konzept von Konvergenz und Grenzwerten beleuchten.

Wir interessieren uns besonders für die Konvergenz (oder auch Divergenz) von Folgen, d.h. das Verhalten für \(n\) gegen unendlich (geschrieben als \(n \rightarrow \infty\)). Eine \sphinxstyleemphasis{konvergente} Folge besitzt einen Grenzwert \(\overline{x}\)
(geschrieben \(x_n \rightarrow \overline{x}\)), wenn für größer werdende \(n\) die Folgenglieder dem Wert \(\overline{x}\) immer näher kommen. Nun stellt sich aber die Frage wie wir diese Aussage mathematisch präzise formalisieren können. Zunächst erkennen wir, dass \sphinxstyleemphasis{immer näher kommen}  bedeutet, dass
\begin{equation}\label{equation:vorkurs/folgen:eq:folgengrenzewert}
\begin{split}\vert x_n - \overline{x} \vert < \epsilon\end{split}
\end{equation}
für \(\epsilon > 0\) beliebig klein und \(n\) hinreichend groß gilt. \sphinxstyleemphasis{Beliebig klein} und  \sphinxstyleemphasis{hinreichend groß} sind mathematisch aber immer noch zu schwammige Begriffe, die wir weiter präzisieren müssen. Tatsächlich tun wir dies, indem wir uns einen Zweifler an der Konvergenz vorstellen, der ein \(\epsilon > 0\) vorgibt und wir müssen \eqref{equation:vorkurs/folgen:eq:folgengrenzewert} nun für alle \(n\) groß genug erfüllen (wobei das kleinste \(n\), für das \eqref{equation:vorkurs/folgen:eq:folgengrenzewert} gelten muss natürlich von \(\epsilon\) abhängen kann). Wir können also folgende Definition geben:
\label{vorkurs/folgen:definition-0}
\begin{sphinxadmonition}{note}{Definition 1.1}



Eine reelle Folge \((x_n)\) heißt konvergent gegen den Grenzwert \(\overline{x}\), geschrieben
\begin{equation*}
\begin{split}x_n \rightarrow \overline{x} \qquad \text{oder} \lim_{n \rightarrow \infty} x_n = \overline{x},\end{split}
\end{equation*}
genau dann, wenn für alle \(\epsilon > 0\) ein \(n_0 \in \N\) existiert, sodass für alle \(n \geq n_0\) \eqref{equation:vorkurs/folgen:eq:folgengrenzewert} erfüllt ist.
Eine Folge heißt konvergent, wenn es ein \(\overline{x}\) gibt, sodass \(x_n \rightarrow x\).
\end{sphinxadmonition}

Bei dieser Formalisierung haben wir zwei wesentliche logische Aussagen verwendet, nämlich \sphinxstyleemphasis{für alle} und \sphinxstyleemphasis{es existiert}. Diese werden wir in der Vorlesung durch sogenannte Quantoren abkürzen:
\begin{itemize}
\item {} 
\(\forall \equiv\) für alle,

\item {} 
\(\exists \equiv\) es existiert.

\end{itemize}

Damit können wir obige Aussage kompakt als
\begin{equation*}
\begin{split}\forall \epsilon > 0 ~\exists n_0 \in \N ~\forall n \geq n_0: ~\vert x_n - \overline{x} \vert < \epsilon\end{split}
\end{equation*}
schreiben. Dabei haben wir den Doppelpunkt verwendet um die eigentliche Aussage zu beginnen, \sphinxstyleemphasis{:} ersetzt also das sprachliche \sphinxstyleemphasis{gilt}.

Ist eine Folge nicht konvergent, so nennen wir sie \sphinxstyleemphasis{divergent}. Wir beachten, dass nach unserer Definition \(\overline{x} \in \R\) gelten muss. Es gibt also im strengen Sinn keine Konvergenz \(x_n \rightarrow \infty\) (oder analog \(x_n \rightarrow -\infty\)), wie man intuitiv zum Beispiel von der Folge \((x_n) = (n)\) erwarten würde. Das Problem dabei ist, dass \(x_n\) ja dem Wert \(\overline{x}=\infty\) nicht beliebig nahe kommt, der Unterschied ist immer unendlich groß. Deshalb sagen wir \(x_n\) konvergiert gegen unendlich, \(x_n \rightarrow \infty\), genau dann wenn
\begin{equation*}
\begin{split} \forall \epsilon > 0 ~\exists n_0 \in \N ~\forall n \geq n_0: ~ x_n > \epsilon .\end{split}
\end{equation*}
Hier hat \(\epsilon\) eine andere Rolle als vorhin, da wir uns ja dafür interessieren, dass \(\epsilon\) beliebig groß wird (nicht beliebig klein wie bei der üblichen Konvergenz), die Definition über alle \(\epsilon\) deckt dies aber gut ab. Als Übung lassen wir den Fall einer Definition von \(x_n \rightarrow -  \infty\). Zum Verständnis betrachten wir einige Beispiele:
\label{vorkurs/folgen:example-1}
\begin{sphinxadmonition}{note}{Example 1.1}



Sei \((x_n)\) die konstante Folge \(x_n = 1 \) für alle \(n\). Diese konvergiert natürlich gegen \(\overline{x} = 1\). Es gilt tatsächlich:
\begin{equation*}
\begin{split} \forall \epsilon > 0 ~\forall n \geq 0:  \vert x_n - \overline{x}\vert < \epsilon, \end{split}
\end{equation*}
da \(\vert x_n - \overline{x}\vert =0\), d.h. wir können immer \(n_0=0\) wählen.
\end{sphinxadmonition}
\label{vorkurs/folgen:example-2}
\begin{sphinxadmonition}{note}{Example 1.2}



Sei \((x_n)\) die  Folge \(x_n = \frac{1}n \) für alle \(n>0\) mit \(x_0\) beliebig definiert. Diese konvergiert natürlich gegen \(\overline{x} = 0\). Es gilt
\begin{equation*}
\begin{split} \forall \epsilon > 0 ~\forall n \geq n_0:  \frac{1}n = \vert x_n - \overline{x}\vert < \epsilon, \end{split}
\end{equation*}
wenn \(n > \frac{1}\epsilon\) ist. Wir wählen also \(n_0\) als die nächstgrößte natürliche Zahl zu \(\frac{1}\epsilon\), damit erhalten wir die Konvergenz.
\end{sphinxadmonition}
\label{vorkurs/folgen:example-3}
\begin{sphinxadmonition}{note}{Example 1.3}



Sei \((x_n)\) die  Folge \(x_n = (-1)^n \) für alle \(n\geq 0\). Diese alternierende Folge divergiert. Ist \(\overline{x} = 1\), dann gilt für ungerades \(n\) beliebig groß \(\vert x_n - \overline{x}\vert = 2\), also erhalten wir mit \(\epsilon =1\) bereits eine Verletzung der Konvergenzbedingung. Ist \(\overline{x} \neq 1\), so gilt für gerades \(n\) immer \(\vert x_n - \overline{x} \vert =  \vert 1-\overline{x} \vert \neq 0\). Damit erhalten wir eine Verletzung der Konvergenzbedingung mit \(\epsilon = \frac{\vert 1-\overline{x} \vert}2\).

In diesem Fall gibt es aber zwei konvergente Teilfolgen \((y_n) = (x_{2n})\) mit Grenzwert \(1\) und \((z_n) = (x_{2n+1})\) mit Grenzwert \(-1\). Wie werden später noch häufiger mit konvergenten Teilfolgen zu tun haben.
\end{sphinxadmonition}
\label{vorkurs/folgen:example-4}
\begin{sphinxadmonition}{note}{Example 1.4}



Sei \((x_n)\) die  Folge \(x_n = \frac{n^2+1}{2n^2+1} \) für alle \(n\geq 0\). Diese konvergiert  gegen \(\overline{x} = \frac{1}2\). Es gilt
\begin{equation*}
\begin{split} \vert x_n - \overline{x}\vert = \frac{1}{4n^2+2}, \end{split}
\end{equation*}
dies ist kleiner \(\epsilon\), wenn \(n \geq n_0 > \sqrt{\frac{1}{4\epsilon}-\frac{1}2}\) (bzw. \(n_0 \geq 0\) wenn \(\epsilon > \frac{1}2\)) gilt.
\end{sphinxadmonition}

Als Verallgemeinerung des letzten Beispiels können wir zeigen, dass
\begin{equation*}
\begin{split} \frac{x_n}{y_n} \rightarrow \frac{\overline{x}}{\overline{y}} \end{split}
\end{equation*}
gilt, wenn \(x_n \rightarrow \overline{x}\) und \(y_n \rightarrow \overline{y} \neq 0\). Dazu machen wir mit Hilfe der Dreiecksungleichung folgende Abschätzung:
\begin{equation*}
\begin{split}\left\vert \frac{x_n}{y_n} -\frac{\overline{x}}{\overline{y}} \right\vert =
\left\vert \frac{x_n}{y_n} -\frac{\overline{x}}{y_n}+\frac{\overline{x}}{y_n}-\frac{\overline{x}}{\overline{y}} \right\vert \leq 
\left\vert \frac{x_n}{y_n} -\frac{\overline{x}}{y_n}\right\vert+\left\vert\frac{\overline{x}}{y_n}-\frac{\overline{x}}{\overline{y}} \right\vert = \frac{1}{\vert y_n \vert} \vert x_n - \overline{x}\vert+
\frac{\vert \overline{x} \vert}{\vert \overline{y} \vert~\vert y_n \vert} \vert y_n - \overline{y}\vert.\end{split}
\end{equation*}
Gilt nun
\begin{equation*}
\begin{split}  \vert x_n - \overline{x}\vert < \epsilon_1, \qquad  \vert y_n - \overline{y}\vert < \epsilon_2 \end{split}
\end{equation*}
Dann folgt mit der Dreiecksungleichung auch
\begin{equation*}
\begin{split} \vert y_n \vert \geq \vert \overline{y}\vert - \vert y_n - \overline{y}\vert \geq \vert \overline{y}\vert - \epsilon_2 . \end{split}
\end{equation*}
Damit ist
\begin{equation*}
\begin{split} \left\vert \frac{x_n}{y_n} -\frac{\overline{x}}{\overline{y}} \right\vert \leq \frac{\epsilon_1}{\vert \overline{y} \vert - \epsilon_2} + \frac{\overline{x}\epsilon_2}{\overline{y}(\vert \overline{y} \vert - \epsilon_2)}  . \end{split}
\end{equation*}
Nun wählen wir
\begin{equation*}
\begin{split} \epsilon_1 <\frac{\vert \overline{y} \vert }4,  \qquad \epsilon_2 < \min\{\frac{\vert \overline{y} \vert }2, \frac{\vert \overline{y} \vert^2}{4 \vert \overline{x} \vert} \epsilon \} , \end{split}
\end{equation*}
dann gibt es \(n_0^1\) und \(n_0^2\), sodass
\begin{equation*}
\begin{split} \forall n > n_0^1:~\vert x_n - \overline{x} \vert < \epsilon_1 \end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} \forall n > n_0^2:~\vert y_n - \overline{y} \vert < \epsilon_2 .\end{split}
\end{equation*}
Nun sei \(n_0 = \max\{n_0^1,n_0^2\}\), dann folgt durch Einsetzen der Schranken für \(\epsilon_1\) und \(\epsilon_2\) in die obige Abschätzung
\begin{equation*}
\begin{split}  \left\vert \frac{x_n}{y_n} -\frac{\overline{x}}{\overline{y}} \right\vert < \epsilon \end{split}
\end{equation*}
für alle \(n \geq n_0\). Als Übung überlassen wir den Fall \(\overline{x} \neq 0\) und \(\overline{y}=0\), hier erhält man für den Grenzwert Konvergenz gegen \(+\infty\) oder \(-\infty\) je nach Vorzeichen von \(\overline{x}\).
\label{vorkurs/folgen:example-5}
\begin{sphinxadmonition}{note}{Example 1.5}



Sei \((x_n)\) die  Folge \(x_n = n \) für alle \(n\geq 0\). Es gilt \(x_n \rightarrow \infty\), da \(\vert x_n \vert > \epsilon\) für \(n\geq n_0\), wenn wir \(n_0\) als die nächstgrößte natürliche Zahl zu \(\epsilon\) wählen.
\end{sphinxadmonition}

Wir können die Definition der Konvergenz auch direkt auf Folgen \(x_n \in \R^N\) übertragen wenn wir den Betrag durch die (Euklidische Norm) ersetzen:
\begin{equation*}
\begin{split} \forall \epsilon > 0 ~\exists n_0 \in \N ~\forall n \geq n_0: ~\Vert x_n - \overline{x} \Vert < \epsilon \end{split}
\end{equation*}\label{vorkurs/folgen:example-6}
\begin{sphinxadmonition}{note}{Example 1.6}



Als Beispiel betrachten wir eine Folge \((x_n)\) im \(\R^2\) gegeben durch \(x_n= \veczwei{\frac{n}{n+1}}{2^{-n}}\). Dies konvergiert gegen \(\overline{x}=\veczwei{1}{0}\). Es gilt
\begin{equation*}
\begin{split} \Vert x_n - \overline{x} \Vert = \sqrt{\frac{1}{(n+1)^2} + 2^{-2n}} \leq \frac{\sqrt{2}}{n+1} \end{split}
\end{equation*}
für \(n \geq 1\), da \(2^n \geq n+1\). Also folgt
\begin{equation*}
\begin{split} \Vert x_n - \overline{x} \Vert < \epsilon, \end{split}
\end{equation*}
für \(n \geq n_0\), wenn \(n_0\) größer als \(\frac{\sqrt{2}}\epsilon -1\) ist.
\end{sphinxadmonition}

Wir können auch noch andere Eigenschaften von Folgen untersuchen. Eine erste ist die Anordnung, wir sagen eine Folge ist
\begin{itemize}
\item {} 
\sphinxstyleemphasis{monoton steigend}, wenn \(x_n \leq x_{n+1}\) für alle \(n\in \N\) gilt,

\item {} 
\sphinxstyleemphasis{monoton fallend}, wenn \(x_n \geq x_{n+1}\) für alle \(n\in \N\) gilt.
\textbackslash{}end\{*ize\}

\end{itemize}

Dazu können wir auch das Supremum (\(\sup\)) und Infimum (\(\inf\)) einer Folge definieren. Bei einer endlichen Menge erreicht man nach endlich vielen Schritten ja immer ein Minimum oder Maximum, bei einer unendlichen Folge ist das nicht zwangsläufig der Fall. So kommt etwa \(x_n = \frac{1}n\) dem Wert \(0\) beliebig nahe, erreicht ihn aber nicht. Deshalb machen wir folgende Definition:
\label{vorkurs/folgen:definition-7}
\begin{sphinxadmonition}{note}{Definition 1.2}



Wir nennen \(x_* \in \R\) das Infimum der reellen Folge \(x_n\) (geschrieben \(x_* = \inf_n x_n\)), genau dann wenn die beiden folgenden Bedingungen erfüllt sind:
\begin{itemize}
\item {} 
\(\forall n \in \N:~x_* \leq x_n\)

\item {} 
\(\forall \epsilon > 0~\exists n_0 \in \N: x_{n_0} < x_* + \epsilon\).

\end{itemize}

Wir nennen \(x^* \in \R\) das Supremum der reellen Folge \(x_n\) (geschrieben \(x^* = \sup_n x_n\)), genau dann wenn die beiden folgenden Bedingungen erfüllt sind:
\begin{itemize}
\item {} 
\(\forall n \in \N:~x^* \geq x_n\)

\item {} 
\(\forall \epsilon > 0~\exists n_0 \in \N: x_{n_0} > x^* -\epsilon\).

\end{itemize}
\end{sphinxadmonition}

Analog zur Konvergenz können wir auch die Fälle \(\inf_n x_n = - \infty\) und \(\sup x_n = +\infty\) definieren, letzteres ist der Fall wenn
\begin{equation*}
\begin{split} \forall \epsilon > 0~\exists n_0 \in \N: x_{n_0} > \epsilon . \end{split}
\end{equation*}
Im Fall einer monotonen Folge können wir nun direkt die Konvergenz beweisen:
\label{vorkurs/folgen:theorem-8}
\begin{sphinxadmonition}{note}{Theorem 1.1}



Sei \((x_n)\) eine monoton steigende reelle Folge. Dann gibt es ein \(\overline{x} \in [x_0,+\infty]\) mit \(x_n \rightarrow \overline{x}\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Wir definieren \(\overline{x}= \sup_n x_n\) und unterscheiden zwei Fälle. Ist die Folge unbeschränkt, d.h. \(\overline{x}=\infty\), so gilt
\begin{equation*}
\begin{split} \forall \epsilon > 0~\exists n_0 \in \N: x_{n_0} > \epsilon. \end{split}
\end{equation*}
Da die Folge monoton steigend ist, gilt \(x_n \geq x_{n_0}\) für alle \(n \geq n_0\). Damit ist die Bedingung
\begin{equation*}
\begin{split} \forall \epsilon > 0~\exists n_0 \in \N~\forall n\geq n_0: x_{n_0} > \epsilon  \end{split}
\end{equation*}
erfüllt.

Ist \(\overline{x} < \infty\), dann gilt
\begin{equation*}
\begin{split} \forall \epsilon > 0~\exists n_0 \in \N: x_{n_0} > \overline{x} -\epsilon\end{split}
\end{equation*}
und da wieder \(x_n \geq x_{n_0}\) für alle \(n \geq n_0\) gilt erhalten wir auch \( x_n > \overline{x}-\epsilon\). Andererseits ist \(x_n \leq \overline{x}\), also gilt für alle \(n \geq n_0\) auch \( \vert x_n - \overline{x} \vert < \epsilon. \)  \(\square\)
\end{sphinxadmonition}

In obigem Beweis haben wir verwendet, dass aus \(x_{n+1} \geq x_n\) f”ur alle \(n\) auch folgt
\begin{equation*}
\begin{split} \forall m \geq n: x_m \geq x_n. \end{split}
\end{equation*}
Dies scheint offensichtlich, benutzt aber ein nichttriviales logisches Prinzip, die sogenannte Induktion. Dabei schließt man auf eine Aussage für alle natürlichen Zahlen, wenn man aus der Aussage für ein \(n \in \N\) auch die Aussage für \(n+1\) folgern kann.


\section{Funktionen}
\label{\detokenize{vorkurs/funktionen:funktionen}}\label{\detokenize{vorkurs/funktionen::doc}}
Eine Funktion ist in allgemeinster Weise eine Abbildung \(f: M_1 \rightarrow M_2\), die jedem Wert \(x \in M_1\) einen \sphinxstyleemphasis{eindeutigen} Wert \(y \in M_2\) zuordnet. In der Schule betrachtet man üblicherweise skalare reelle Funktionen, d.h. \(M_1=M_2=\R\), manchmal auch \(M_1\) ein Intervall in \(\R\). Im Prinzip sind aber auch Folgen nichts anderes als Funktionen von \(\N\) nach \(\R\).
\label{vorkurs/funktionen:definition-0}
\begin{sphinxadmonition}{note}{Definition 1.3}



Eine Funktion \(f:M_1 \rightarrow M_2\) heißt
\begin{itemize}
\item {} 
\sphinxstyleemphasis{injektiv}, wenn aus \(x \neq y\) auch \(f(x) \neq f(y)\) folgt.

\item {} 
\sphinxstyleemphasis{surjektiv}, wenn zu jedem \(z \in M_2\) ein \(x \in M_1\) existiert mit \(f(x)=z\).

\item {} 
\sphinxstyleemphasis{bijektiv}, wenn sie injektiv und surjektiv ist.

\end{itemize}

\textbackslash{}end\{*ize\}
\end{sphinxadmonition}

Eine bijektive Funktion ordnet jedem Element aus \(M_1\) genau ein Element aus \(M_2\) zu und umgekehrt. Damit existiert  auch eine Umkehrfunktion \(f^{-1}: M_2 \rightarrow M_1\) (wir beachten, dass die Umkehrfunktion \(f^{-1}(x)\) nicht zu verwechseln ist mit \(f(x)^{-1} = \frac{1}{f(x)}\)). Die Umkehrfunktion erfüllt
\begin{equation*}
\begin{split} f^{-1}(f(x)) = x, \qquad f(f^{-1}(y) = y \end{split}
\end{equation*}
für alle \(x \in M_1\) und alle \(y \in M_2\).


\subsection{Lineare Funktionen}
\label{\detokenize{vorkurs/funktionen:lineare-funktionen}}
Wir beginnen mit einigen wichtigen Beispielen skalarer Funktionen in \(\R\). Die wohl einfachsten sind lineare bzw. affin\sphinxhyphen{}lineare Funktionen. Eine lineare Funktion ist von der Form
\begin{equation*}
\begin{split} f:x \mapsto a_1 x, \end{split}
\end{equation*}
mit einer Konstanten \(a_1 \in \R\). Eine affin\sphinxhyphen{}lineare Funktion ist um eine Konstante verschoben, d.h.
\begin{equation*}
\begin{split} f:x\mapsto a_1 x + a_0. \end{split}
\end{equation*}
Für \(a_1 =0\) ist \(f\) eine konstante Funktion.

Linearität lässt sich durch zwei Eigenschaften charakterisieren:
\begin{itemize}
\item {} 
Für alle \(x,y \in M_1\) gilt \(f(x+y)=f(x) + f(y) \).

\item {} 
Für alle \(x \in M_1\) und \(c \in \R\), sodass \(cx \in M_1\) ist, gilt \(f(cx) = c f(x)\).

\end{itemize}

In \(\R\) folgt daraus trivialerweise die obige Form, da wir \(x=1\) wählen können und damit ist \(a_1 = f(0)\). Aber auch im \(\R^n\) reicht dies um eine ähnliche Form herzuleiten.


\subsection{Polynome und Rationale Funktionen}
\label{\detokenize{vorkurs/funktionen:polynome-und-rationale-funktionen}}
Eine allgemeinere Klasse als lineare Funktionen sind die sogenannten Polynome. Ein Polynom vom Grad \(k\) ist die Funktion
\begin{equation*}
\begin{split} f:x\mapsto a_k x^k + a_{k-1}x^{k-1} + \ldots + a_1 x + a_0. \end{split}
\end{equation*}
Eine wichtige Eigenschaft von Polynomen ist der Nullstellensatz: ein Polynom vom Grad \(k\) kann höchstens \(k\) Nullstellen (mit Vielfachheit gezählt haben), mit Ausnahme des trivialen Nullpolynoms. Hat ein Polynom \(k\) reelle Nullstellen \(x_0, \ldots x_{k-1}\), dann gilt
\begin{equation*}
\begin{split} f(x) = a_k (x - x_{k-1}) \ldots (x-x_0). \end{split}
\end{equation*}
Grundlage dafür ist, dass ein Polynom vom Grad \(k\) mit Nullstelle \(x_0\) immer geschrieben werden kann als
\begin{equation*}
\begin{split} f(x) = (x-x_0) g(x) \end{split}
\end{equation*}
mit einem Polynom \(g\) vom Grad \(k-1\).

Dies ist ein Spezialfall der sogenannten Polynomdivision, ist \(f\) ein Polynom vom Grad \(k\) und \(g\) ein Polynom vom Grad \(m \leq k\), dann existiert ein Polynom \(h\) vom Grad \(m-k\) und ein Polynom \(r\) vom Grad höchstens \(m-1\), sodass
\begin{equation*}
\begin{split} f(x) = h(x) g(x) + r(x).\end{split}
\end{equation*}
Die Division funktioniert dabei genauso wie schriftliche Division bei Dezimalzahlen, diese sind ja von der Form
\begin{equation*}
\begin{split}z = a_k 10^k + a_{k-1} 10^{k-1} + \ldots + a_1 10^1 + a_0. \end{split}
\end{equation*}
Das Polynom \(r\) hat dabei die Rolle des Rests bei der Division.  Wir sehen auch, dass die sogenannten \sphinxstyleemphasis{Monome} \(x^k\) in dieser Darstellung eine wichtige Rolle einnehmen, wir schreiben jedes Polynom als gewichtete Summe von Monomen, wir werden dies allgemeiner \sphinxstyleemphasis{Linearkombination} nennen.

Es gelten auch sonst analoge Eigenschaften wie bei der Rechung mit ganzen Zahlen:
\begin{itemize}
\item {} 
Summen von Polynomen sind Polynome.

\item {} 
Produkte von Polynonem sind Polynome.

\item {} 
Es gibt ein neutrales Element der Addition, das Nullpolynom \(p_0(x)=0\): Es gilt \(p(x) + p_0(x) = p(x)\) für jedes Polynom.

\item {} 
Es gibt ein neutrales Element der Multiplikation, das konstante Polynom \(p_1(x)=1\): Es gilt \(p(x) p_1(x) = p(x)\) für jedes Polynom.

\item {} 
Es gibt ein inverses Element der Addition, d.h. zu jedem Polynom \(p\) gibt es ein Polynom \(q\) mit \(p(x) + q(x) =0\) für alle \(x\).

\item {} 
Es gelten Kommutativ\sphinxhyphen{}, Assoziativ\sphinxhyphen{} und Distributivgesetz.

\end{itemize}

Wir beachten, dass als Spezialfall dieser Rechenregeln auch die Multiplikation von Polynomen mit reellen Zahlen beinhaltet sind, da wir reelle Zahlen mit konstanten Polynomen identifizieren können.

Wir kehren nochmal zum Nullstellensatz zurück und wollen uns zunächst überlegen, warum ein Polynom mit Nullstelle \(x_0\) durch \((x-x_0)\) ohne Rest dividierbar ist. Nach der Formel für Polynomdivision gilt
\begin{equation*}
\begin{split} f(x) = h(x) (x-x_0) + r(x).\end{split}
\end{equation*}
Da \(x-x_0\) ein Polynom vom Grad \(1\) ist, muss \(r\) Grad \(0\) haben, d.h. \(r\) ist eine konstante. Sezten wir \(x=x_0\) ein, dann folgt
\begin{equation*}
\begin{split} 0 = 0 + r(x_0) . \end{split}
\end{equation*}
Also folgt \(r(x) =r(x_0) = 0\) für alle \(x\). Damit sehen wir auch sofort, dass ein nichtriviales Polynom vom Grad \(k\) höchstens \(k\) Nullstellen (mit Vielfachheit gezählt) haben kann. Gäbe es \(k+1\) Nullstellen, dann könnten wir durch die ersten \(k\) dividieren und es muss ein Polynom vom Grad \(0\) übrig bleiben. Ist dieses gleich Null, dann war \(f\) schon das Nullpolynom. Andernfalls kann das Polynom vom Grad \(0\) keine weitere Nullstelle haben.

Wenn man Polynome mit nichtverschwindendem Rest dividiert, erhält man \sphinxstyleemphasis{rationale Funktionen}.
Eine rationale Funktion ist von der Form
\begin{equation*}
\begin{split} f:x\mapsto  \frac{p(x)}{q(x)}, \end{split}
\end{equation*}
wobei \(p\) und \(q\) Polynome sind. Im Gegensatz zu Polynomen ist der Definitionsbereich rationaler Funktionen nicht ganz \(\R\), da die Nullstellen von \(q\) ausgenommen werden müssen, an denen \(f\) Pole hat. Wegen dem Nullstellensatz ist die Anzahl der Nullstellen von \(f\) durch den Grad von \(p\) begrenzt, die Anzahl der Pole durch den Grad von \(q\).

Bei rationalen Polynomen gelten analoge Rechenregeln wie bei rationalen Zahlen, insbesondere gibt es, wenn \(f\) nicht die Nullfunktion ist, auch ein inverses Element der Multiplikation. Für jedes rationale Funktion \(f\) gibt es eine rationale Funktion \(g\) mit \(f(x) g(x) = 1\) für alle \(x\).


\subsection{Exponentialfunktion und Logarithmus}
\label{\detokenize{vorkurs/funktionen:exponentialfunktion-und-logarithmus}}
Eine andere wichtige Klasse von Funktionen sind \sphinxstyleemphasis{Exponentialfunktionen}, die durch \(f: x\mapsto a^x\) für ein \(a \in \R\) definiert sind. Für \(n \in \N\) ist \(a^n\) die \((n-1)\)\sphinxhyphen{}malige Multiplikation von \(a\) mit sich selbst und wir sehen, dass dann
\begin{equation*}
\begin{split} f(n+m) = a^{n+m}  = a^n a^m  = f(n) f(m) \end{split}
\end{equation*}
gilt. Diese Eigenschaft kann tastsächlich als allgemeine Charakterisierung einer Exponentialfunktion verwendet werden, wir betrachten einfach eine Funktion \(f\) auf \(\R\) für die
\begin{equation*}
\begin{split} f(x+y) =  f(x) f(y) \end{split}
\end{equation*}
für alle \(x,y \in \R\) sowie \(f(0)=1\)
gilt. Damit ist automatisch \(f(x+1) = f(x) f(1)\) und durch Hintereinanderausführung \(f(x+m) = f(x) f(1)^m\) für \(m \in \N\). Definieren wir also \(a=f(1)\), so sehen wir dass für alle \(n \in \N\) die Form \(f(n) = a^n\) folgt. Ist \(a \neq 0\) so können wir auch dividieren und erhalten die selbe Form für \(n \in \Z\). Als nächsten Schritt können wir \(x= \frac{n}m\) für \(n \in \Z, m \in \N\) betrachten. Nun wählen wir \(y=(m-1) x\) und erhalten
\begin{equation*}
\begin{split} f(m x) = f((m-1)x) f(x) \end{split}
\end{equation*}
und durch Hintereinanderausführung
\begin{equation*}
\begin{split} a^n = f(n) = f(mx) = f(x)^m. \end{split}
\end{equation*}
Also gilt \(f(x) = a^{\frac{n}m}. \) Für die Erweiterung auf reelles \(x\) benötigt man noch einen Grenzwert von rationalen gegen reelle Werte, dies werden wir später auch basierend auf der Annahme der Stetigkeit kennen lernen.

Besonders interessant ist  der Fall \(a=e\), wie wir sehen werden lassen sich alle Exponentialfunktionen mit \(a > 0\) als \(f(x) = e^{cx}\) mit \(c\in \R\) schreiben. Die Euler’sche Zahl \(e\) können wir als
\begin{equation*}
\begin{split} e= \lim_{n \rightarrow \infty} \left(1 + \frac{1}n\right)^n \end{split}
\end{equation*}
definieren. Alternativ können wir \(e^x\) auch über die Potenzreihe
\begin{equation*}
\begin{split} e^{x} = 1 + x+ \frac{x^2}2 + \frac{x^3}{3!} + \frac{x^4}{4!} + \ldots \end{split}
\end{equation*}
definieren, allerdings muss dann die Konvergenz der Reihe auch für jedes \(x\) zeigen, was wir später in der Vorlesung tun werden.

Die Exponentialfunktion ist eine monoton steigende Funktion, d.h.
\begin{equation*}
\begin{split} f(x) \geq f(y) \quad \text{für } x > y, \end{split}
\end{equation*}
es gilt sogar \(f(x) > f(y)\) für \(x > y\) und damit ist \(f\) injektiv. Andererseits ist die Exponentialfunktion auch surjektiv und damit invertierbar. Die inverse Funktion nennen wir Logarithmus (\(\log x\)). Es gilt
\begin{equation*}
\begin{split} \log e^x = x \qquad e^{\log y} = y. \end{split}
\end{equation*}
Die Rechenregeln für den Logarithmus können wir aus den Regeln für die Exponentialfunktion herleiten. Es gilt
\begin{equation*}
\begin{split} \log (y_1 y_2) = \log (e^{x_1} e^{x_2}) = \log e^{x_1 + x_2} = x_1 + x_2 = \log y_1 + \log y_2. \end{split}
\end{equation*}
Setzen wir \(y_1 = y_2 =y\) dann folgt auch
\begin{equation*}
\begin{split}  \log ( y^2) = 2 \log y \end{split}
\end{equation*}
und induktiv
\begin{equation*}
\begin{split}  \log ( y^n) = n \log y. \end{split}
\end{equation*}
Mit einer entsprechenden Konstruktion sehen wir auch
\begin{equation*}
\begin{split} \log (y^x) = x \log y, \end{split}
\end{equation*}
für \(x \in \R\). Damit ist
\begin{equation*}
\begin{split} \log (a^x) = x \log a \end{split}
\end{equation*}
und es folgt
\begin{equation*}
\begin{split} a^x = e^{c x}, \end{split}
\end{equation*}
mit \(c= \log a\). Also können wir wie angekündigt alle Exponentialfunktionen in der Form \(e^{cx}\) schreiben.


\subsection{Sinus\sphinxhyphen{} und Cosinusfunktion}
\label{\detokenize{vorkurs/funktionen:sinus-und-cosinusfunktion}}
Die Sinus\sphinxhyphen{} und Cosinusfunktion sind periodische Funktionen in \(\R\), deren Wertebereich \([-1,1]\), es gibt daher nur Umkehrfunktionen wenn wir sie im Fall des Sinus von \([-\frac{\pi}2,\frac{\pi}2]\) nach \([-1,1]\) und im Fall des Cosinus von \([0,\pi]\) nach \([-1,1]\) betrachten.

Es gilt
\begin{equation*}
\begin{split} \sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \ldots \end{split}
\end{equation*}
und
\begin{equation*}
\begin{split}\cos x = 1- \frac{x^2}{2!} + \frac{x^4}{4!} - \ldots \end{split}
\end{equation*}
Der Sinus ist eine ungerade Funktion, d.h. \(\sin (-x) = -  \sin x\), während der Cosinus eine gerade Funktion ist, d.h. \(\cos(-x) = \cos(x)\). Sinus uns Cosinus erfüllen einige interessante Eigenschaften wie Additionstheoreme, wir werden später sehen, dass diese überraschenderweise aus den Eigenschaften der Exponentialfunktion folgen, dies basiert auf den komplexen Zahlen. Mit der imaginären Zahl \(\i\), die \(\i^2 = -1\) erfüllt, gilt
\begin{equation*}
\begin{split} e^{\i x}  = \cos x + \i \sin x. \end{split}
\end{equation*}

\section{Stetigkeit}
\label{\detokenize{vorkurs/stetigkeit:stetigkeit}}\label{\detokenize{vorkurs/stetigkeit::doc}}
In diesem Abschnitt wollen wir den Begriff der Stetigkeit, der intuitiv relativ klar ist, mathematisch etwas klarer formulieren. Alle Beispiele an Funktionen, die wir bisher gesehen haben, sind stetig. Eine Funktion ist stetig in einem Punkt \(x\), wenn aus \(|y-x|\) klein auch \(|f(y)-f(x)|\) klein folgt. Dies können wir sauberer über Folgen definieren:
\label{vorkurs/stetigkeit:definition-0}
\begin{sphinxadmonition}{note}{Definition 1.4}



Eine Funktion \(f: I \subset \R \rightarrow \R\) heisst stetig in \(x \in I\), genau dann wenn für alle Folgen \(x_n \rightarrow x\) gilt: \(f(x_n) \rightarrow f(x)\).
\end{sphinxadmonition}

Alternativ können wir auch eine Definition von Stetigkeit über die Nähe geben: egal welche kleine Schranke wir an die Unterschiede der Funktionswerte vorgeben, können wir diese garantieren, solange die Differenz der Argumente klein genug ist:
\label{vorkurs/stetigkeit:definition-1}
\begin{sphinxadmonition}{note}{Definition 1.5}



Eine Funktion \(f: I \subset \R \rightarrow \R\) heisst stetig in \(x \in I\), genau dann wenn
\begin{equation*}
\begin{split} \forall \epsilon > 0~\exists \delta > 0 ~\forall y \in I, |y-x| < \delta: |f(x) - f(y)|<\epsilon. \end{split}
\end{equation*}\end{sphinxadmonition}

Wir werden später sehen, dass diese beiden Definitionen äquivalent sind. Hier betrachten wir aber noch einen wichtigen Spezialfall, sogenannte Lipschitz\sphinxhyphen{}stetige Funktionen:
\label{vorkurs/stetigkeit:definition-2}
\begin{sphinxadmonition}{note}{Definition 1.6}



Eine Funktion \(f: I \subset \R \rightarrow \R\) heisst Lipschitz\sphinxhyphen{}stetig, genau dann wenn
\begin{equation*}
\begin{split} \ \exists L > 0 ~\forall x,y \in I: |f(x) - f(y)| \leq L |x-y|.\end{split}
\end{equation*}\end{sphinxadmonition}

Eine Lipschitz\sphinxhyphen{}stetige Funktion ist immer stetig, für alle \(x \in I\). Wir sehen, dass aus \(|x-y| < \delta\) immer \(|f(x) - f(y)| < L \delta\) folgt. Also können wir zu jedem \(\epsilon > 0\) ein \(\delta = \frac{\epsilon}{L}\) wählen, sodass die Definition der Stetigkeit erfüllt ist.
\label{vorkurs/stetigkeit:example-3}
\begin{sphinxadmonition}{note}{Example 1.7}



Sei \(f(x) = a_1 x + a_0\), dann ist \(f\) Lipschitz\sphinxhyphen{}stetig auf \(\R\) mit \(L=\vert a_1 \vert\).
\end{sphinxadmonition}
\label{vorkurs/stetigkeit:example-4}
\begin{sphinxadmonition}{note}{Example 1.8}



Sei \(f(x) = x^2\), dann ist \(f\) Lipschitz\sphinxhyphen{}stetig auf \(I=[-b,b]\) mit \(L= 2b\).
\end{sphinxadmonition}


\section{Differentiation und Integration}
\label{\detokenize{vorkurs/diffnint:differentiation-und-integration}}\label{\detokenize{vorkurs/diffnint::doc}}
Wir kommen nun zu einem der wesentlichsten Themen der Analysis, der Differential\sphinxhyphen{} und Integralrechnung. Grundlage der Differentialrechnung ist eine lokal lineare Approximation der Funktion \(f\) nahe eines Punkts \(x_0\). Dazu stellen wir die Geradengleichung durch den Punkt \((x_0,f(x_0))\) auf als
\begin{equation*}
\begin{split} g(x) = f(x_0) + k_0 (x-x_0) \end{split}
\end{equation*}
und fragen uns wie wir die Steigung \(k_0\) am sinnvollsten wählen. Da \(g(x) \approx f(x)\) gelten sollte, möchten wir
\begin{equation*}
\begin{split} k_0 \approx \frac{f(x) - f(x_0)}{x-x_0} \end{split}
\end{equation*}
Der richtige Wert ist also der Grenzwert \(x\) gegen \(x_0\).  Dementsprechend definieren wir diesen Grenzwert als Ableitung
\begin{equation*}
\begin{split} f'(x_0) = \lim_{x \rightarrow x_0} \frac{f(x) - f(x_0)}{x-x_0}. \end{split}
\end{equation*}
Wir sagen, dass der Grenzwert \(\lim_{x \rightarrow x_0}\) existiert, wenn der Grenzwert für alle Folgen \(x_n \rightarrow x_0\) existiert und den gleichen Wert annimmt. In diesem Fall nennen wir die Funktion differenzierbar in \(x_0\). Ist \(f\) differenzierbar für jedes \(x_0 \in I\), so nennen wir \(f\) differenzierbar in \(I\).
\label{vorkurs/diffnint:example-0}
\begin{sphinxadmonition}{note}{Example 1.9}



Sei \(f(x) = a_1 x + a_0\), dann ist \(f\) differenzierbar in \(\R\) mit \(f'(x) = a_1\).
\end{sphinxadmonition}
\label{vorkurs/diffnint:example-1}
\begin{sphinxadmonition}{note}{Example 1.10}



Sei \(f(x) = x^2\), dann ist \(f\) differenzierbar in \(\R\) mit \(f'(x) = 2x\).
\end{sphinxadmonition}

Eine differenzierbare Funktion ist immer stetig, wenn \(f\) in einem Intervall differenzierbar und die Ableitung betragsmäßig beschränkt ist, dann ist \(f\) sogar Lipschitz\sphinxhyphen{}stetig. Dies werden wir später noch mit Hilfe der Integralrechnung sehen.

Die zentralen Regeln bei der Differentiation sind die Produkt\sphinxhyphen{} und Kettenregel. Sind \(f, g:I \rightarrow \R\) zwei Funktionen, die in \(x_0\) differenzierbar sind, dann ist auch \(h = f g\) in \(x_0\) differenzierbar und es gilt die Produktregel:
\begin{equation*}
\begin{split}h'(x_0) = f'(x_0) g(x_0) + f(x_0) g'(x_0) \end{split}
\end{equation*}
Die sehen wir durch eine Betrachtung des Genzwerts
\begin{align*}
\lim_{x \rightarrow x_0} \frac{h(x) - h(x_0)}{x-x_0} &= \lim_{x \rightarrow x_0} \frac{f(x)g(x) - f(x_0)g(x_0) }{x-x_0} \\
&= \lim_{x \rightarrow x_0} \frac{f(x)g(x) - f(x_0) g(x) +f(x_0) g(x) - f(x_0)g(x_0) }{x-x_0} \\
&= \lim_{x \rightarrow x_0} \frac{f(x)  - f(x_0)   }{x-x_0} g(x)+
\lim_{x \rightarrow x_0} f(x_0) \frac{  g(x) -  g(x_0) }{x-x_0}
\end{align*}
Dabei haben wir benutzt, dass Grenzwerte von Summen und Produkten gleich Summen und Produkten von Grenzwerten sind.
\label{vorkurs/diffnint:example-2}
\begin{sphinxadmonition}{note}{Example 1.11}



Sei \(h(x) = x^2 e^x. \) Dann ist \(h'(x) = 2x e^x + x^2 e^x\).
\end{sphinxadmonition}

Die Kettenregel gilt für die Hintereinanderausführung differenzierbarer Funktionen. Ist \(f\) bei \(x_0\) differenzierbar und \(g\) bei \(y_0=f(x_0)\), dann ist \(h = g \circ f, \) d.h. \(h(x) = g(f(x))\), bei \(x_0\) differenzierbar und es gilt
\begin{equation*}
\begin{split} h'(x_0) = g'(f(x_0)) f'(x_0). \end{split}
\end{equation*}
Aus der Ketten\sphinxhyphen{} und Produktregel, lassen sich auch weitere Differentiationsregeln herleiten, etwa die Quotientenregel für \(h= \frac{f}g\). Dazu wenden wir die Produktregel auf \(f \tilde g\) mit \(\tilde g = \frac{1}g\) an und die Kettenregel auf die Hintereinanderausführung von \(y \mapsto \frac{1}y\) und \(g\).

Die Integration wird üblicherweise als Umkehrung der Differentiation eingeführt. Das unbestimmte Integral, d.h. die Stammfunktion \(F\) einer Funktion \(f\) ist definiert durch die Eigenschaft \(F'=f\). Wir beachten, dass die Stammfunktion nur bis auf eine additive Konstante definiert ist. Für \(a < b \in I\) gilt
\begin{equation}\label{equation:vorkurs/diffnint:eq:hauptsatzintegral}
\begin{split}\int_a^b f(x) ~dx = \int_a^b F'(x) ~dx = F(b) - F(a). \end{split}
\end{equation}
Dies wird auch als Hauptsatz der Differential\sphinxhyphen{} und Integralrechnung bezeichnet. Wir beachten, dass beim bestimmten Integral von \(a\) nach \(b\) laut \eqref{equation:vorkurs/diffnint:eq:hauptsatzintegral} die additive Konstante in der Differenz rausfällt, deshalb ist das bestimmte Integral auch eindeutig bestimmt.

Das Integral ist das kontinuierliche Analogon zur Summation, was aus der Definition über sogenannte Riemann\sphinxhyphen{}Summen klar wird. Wir erhalten das Integral einer stetigen Funktion als Grenzwert
\begin{equation*}
\begin{split} \int_a^b f(x)~dx = \lim_{|\Delta_n| \rightarrow 0} \sum_{j=1}^n (x_j - x_{j-1}) f(\xi_j) \end{split}
\end{equation*}
wobei \(\Delta_n = \{x_j,\xi_j\}\), sodass
\begin{equation*}
\begin{split} a = x_0 < x_1 < \ldots < x_n =b, \qquad \xi_j \in [x_{j-1},x_j]\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} \vert \Delta_n \vert = \max_j (x_j - x_{j-1}). \end{split}
\end{equation*}
Daraus sehen wir auch die Linearität des Integrals:
\begin{equation*}
\begin{split} \int_a^b  c f(x) ~dx = c \int_a^b   f(x) ~dx, \end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} \int_a^b   ( f(x) + g(x)) ~dx = \int_a^b   f(x) ~dx + \int_a^b   g(x) ~dx. \end{split}
\end{equation*}
Darüber hinaus pflanzt sich auch die Monotonie und Dreiecksungleichung fort, es gilt
\begin{equation*}
\begin{split} \int_a^b   f(x) ~dx \leq  \int_a^b   g(x) ~dx  \end{split}
\end{equation*}
falls \(f(x) \leq g(x)\) für alle \(x \in (a,b)\) gilt, sowie
\begin{equation*}
\begin{split} \left\vert \int_a^b   f(x) ~dx \right\vert \leq  \int_a^b  \vert  f(x) \vert ~dx . \end{split}
\end{equation*}
Aus dem Hauptsatz der Differential\sphinxhyphen{} und Integralrechnung sowie dieser Ungleichung folgt
\begin{equation*}
\begin{split}\vert F(x_2) - F(x_1) \vert = \left\vert \int_{x_1}^{x_2}   F'(x) ~dx \right\vert
\leq  \int_{x_1}^{x_2}  \vert F'(x) \vert ~dx.\end{split}
\end{equation*}
Ist \(F'\) beschränkt, d.h. \(\vert F'(x) \vert \leq L\) für alle \(x\), dann folgt
\begin{equation*}
\begin{split} \vert F(x_2) - F(x_1) \vert \leq  \int_{x_1}^{x_2}  \vert F'(x) \vert ~dx
\leq  \int_{x_1}^{x_2}  L ~dx = L \vert x_2 - x_1 \vert.\end{split}
\end{equation*}
Die Differentiationsregeln münden auch direkt Eigenschaften der Integration. Besonders interessant ist die partielle Integration
\begin{equation*}
\begin{split}  \int_a^b f'(x) g(x)~dx = f(b) g(b) - f(a) g(a) - \int_a^b f(x) g'(x)~dx, \end{split}
\end{equation*}
die direkt aus der Produktregel und dem Hauptsatz der Differential\sphinxhyphen{} und Integralrechnung folgt.


\chapter{Grundlagen}
\label{\detokenize{grundlagen/grundlagen:grundlagen}}\label{\detokenize{grundlagen/grundlagen::doc}}
In diesem ersten Teil der Vorlesung beginnen wir mit einigen grundlegenden mathematischen Begriffen, die uns helfen Objekte als Mengen und Aussagen in strikter Logik zu formalisieren. Danach wenden wir uns verschiedenen Zahlensystemen zu, insbesondere den wohlbekannten natürlichen, rationalen und reellen Zahlen.


\section{Mengenlehre und Logik}
\label{\detokenize{grundlagen/mengenlogik:mengenlehre-und-logik}}\label{\detokenize{grundlagen/mengenlogik::doc}}
Ein wichtiges Konzept um Mathematik aufzuschreiben sind sogenannte \sphinxstyleemphasis{Mengen}. Eine Menge ist eine Sammlung unterscheidbarer Objekte, die wir innerhalb der Mengenklammern \(\{ \ldots \}\) notieren. Wir unterscheiden endliche Mengen, wie
\begin{equation*}
\begin{split} M_1 =  \{1,2,3\}, \qquad M_2=\{\text{Baum},\text{Wiese}\} ,\end{split}
\end{equation*}
oder
\begin{equation*}
\begin{split} M_3=\{\text{James}, \text{Davis}, \text{Caruso}, \text{Caldwell-Pope},\text{Green}\}\end{split}
\end{equation*}
und unendliche Mengen wie natürlichen Zahlen
\begin{equation*}
\begin{split}\N =\{0,1,2,3,4,\ldots\}.\end{split}
\end{equation*}
Dabei ist die Reihenfolge der Elemente unerheblich, also können wir genauso \( M_1 = \{1,3,2\}\) oder \(M_1=\{3,2,1\}\) schreiben. Auch \(M_1=\{1,2,2,3\}\) ist immer noch die gleiche Menge, da wir nur unterscheidbare Objekte betrachten.
Eine Menge \(M\) besteht aus ihren Elementen \(x\), wir schreiben \(x \in M\). Ist \(x\) nicht in \(M\) enthalten, so schreiben wir \(x \notin M\).
\label{grundlagen/mengenlogik:definition-0}
\begin{sphinxadmonition}{note}{Definition 2.1}



Seien \(M\) und \(\tilde M\) Mengen, dann heißt \(\tilde M\) \sphinxstyleemphasis{Teilmenge} von \(M\), wenn für alle \(x \in \tilde M\) folgt, dass \(x \in M\) gilt. Gibt es ein \(x \in M\) mit \(x \notin \tilde M\), dann heißt \(\tilde M\) \sphinxstyleemphasis{echte Teilmenge}.
\end{sphinxadmonition}

Jede Menge hat zumindest eine Teilmenge, die sogenannte leere Menge \(\emptyset\). Ein einfaches Beispiel für eine Teilmenge ist \(\N \subset \Z\), es ist auch eine echte Teilmenge, da z.b. \(-1 \in \Z\) und \(-1 \notin \N\).
Wir beachten auch den Unterschied zwischen \(x\) und der Menge \(\{x\}\). So gilt im obigen Beispiel etwa \(1 \in M_1\), aber \(\{1\} \subset M_1\).

Wir können (Teil)Mengen auch durch Aussagen definieren. Gegeben eine Menge \(M\) und eine Aussage \(A(x)\) über Elemente in \(x\), die entweder wahr oder falsch ist, erhalten wir
\begin{equation*}
\begin{split} \tilde M = \{ x\in M ~|~A(x)\}\end{split}
\end{equation*}
als Teilmenge jener Elemente von \(M\), für die \(A(x)\) wahr ist. Als Beispiel betrachten wir \(M_1\) wie oben und
\begin{equation*}
\begin{split} \tilde M_1 = \{x \in M_1 ~|~x \text{ ist gerade.}\} = \{2\}\end{split}
\end{equation*}
oder als Teilmenge der reellen Zahlen
\begin{equation*}
\begin{split} M = \{ x \in \N~|~x \text{ ist Quadratzahl. }\} = \{0,1,4,9,16,25,\ldots\}.\end{split}
\end{equation*}

\subsection{Mengensysteme und Mengenoperationen}
\label{\detokenize{grundlagen/mengenlogik:mengensysteme-und-mengenoperationen}}
In dem wir Mengen selbst in Mengen stecken, erhalten wir Mengensysteme. Als Beispiel betrachten wir
\begin{equation*}
\begin{split} S = \emptyset,\{1\_,\{1,2\},\{1,2,3\} \}.\end{split}
\end{equation*}
\(S\) ist eine Teilmenge der sogenannten Potenzmenge von \(\{1,2,3\}\) Allgemein nennen wir die Menge aller Teilmengen von \(M\) die Potenzmenge \({\cal P}(M)\) (oder auch \(2^M\)). So gilt etwa
\begin{equation*}
\begin{split} {\cal P}(\{1,2,3\}) = \emptyset,\{1\_,\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\}\{1,2,3\} \}.\end{split}
\end{equation*}
Ist \(M\) endlich und bezeichnet \(\vert M \vert\) die Anzahl der Elemente in \(M\), so hat \({\cal P}(M)\) genau \(2^{\vert M \vert}\) Elemente.
Auf Mengensystemen können wir auch Operationen definieren wie etwa den Durchschnitt oder die Vereinigung:
\label{grundlagen/mengenlogik:definition-1}
\begin{sphinxadmonition}{note}{Definition 2.2}



Sei \(S\) ein Mengensystem. Dann ist der Durchschnitt definiert als
\begin{equation*}
\begin{split} \bigcap_{M \in S} M = \{x ~|~ \forall M \in S: x \in M \}\end{split}
\end{equation*}
und die Vereinigung als
\begin{equation*}
\begin{split} \bigcup_{M \in S} M = \{x ~|~ \exists M \in S: x \in M \}.\end{split}
\end{equation*}\end{sphinxadmonition}

Für endliche Systeme \(S=\{M_1,M_2,\ldots,M_n\}\) schreiben wir auch
\begin{equation*}
\begin{split} \bigcap_{M \in S} M  = M_1 \cap M_2 \cap \ldots \cap M_n\end{split}
\end{equation*}
bzw.
\begin{equation*}
\begin{split} \bigcup_{M \in S} M  = M_1 \cup M_2 \cup \ldots \cup M_n.\end{split}
\end{equation*}
Gilt \(M_1 \cap M_2 = \emptyset\), so heißen \(M_1\) und \(M_2\) disjunkt. Gilt \(M_1 \cap M_2 = \emptyset\) für alle \(M_1\) und \(M_2\) in \(S\), so heißen die Elemente von \(S\) paarweise disjunkt.
Wir können auch die Teilmengeneigenschaft über den Schnitt definieren:
\label{grundlagen/mengenlogik:lemma-2}
\begin{sphinxadmonition}{note}{Lemma 2.1}



\(\tilde M \subset M\) gilt genau dann wenn \(\tilde M \cap M = \tilde M\)
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. \(\tilde M  \subset M\) ist äquivalent zu
\begin{equation*}
\begin{split} \forall x \in \tilde M: x \in M,
 \end{split}
\end{equation*}
dies ist gilt natürlich auch genau dann wenn
\begin{equation*}
\begin{split} \forall x \in \tilde M \cap M: x \in M.
 \end{split}
\end{equation*}
Die letzte Aussage ist gleichbedeutend mit \(\tilde M \cap M = \tilde M\).
\(\square\)
\end{sphinxadmonition}

Wir definieren noch einige weitere Operationen auf Paaren von Mengen:
\begin{itemize}
\item {} 
Die Differenzmenge \(M \setminus \tilde M\) ist gegeben durch

\end{itemize}
\begin{equation*}
\begin{split} M \setminus \tilde M = \{ x \in M ~|~ x \notin \tilde M\}
 \end{split}
\end{equation*}\begin{itemize}
\item {} 
Die symmetrische Differenz \(M \Delta \tilde M\) ist gegeben durch

\end{itemize}
\begin{equation*}
\begin{split} M \Delta \tilde M = (M \setminus \tilde M ) \cup (\tilde M \setminus M)\end{split}
\end{equation*}\begin{itemize}
\item {} 
Das kartesische Produkt zweier Mengen ist die Menge aller geordneten Paare aus Elementen, d.h.

\end{itemize}
\begin{equation*}
\begin{split} M \times \tilde M = \{ (m,\tilde m) ~|~ m \in M, \tilde m \in \tilde M\}
 \end{split}
\end{equation*}
Dies kennen wir auch aus der Definition des \(\R^2 = \R \times \R\) über zwei kartesische Koordinaten.

Neben Operationen können wir auch sogenannte Relationen auf Paaren von Mengen oder einzelnen Mengen definieren. Dazu konstruieren wir eine neue Menge \(R\), in die wir alle geordneten Paare aus \(M \times \tilde M\) schreiben, für die eine Relation gelten soll:
\label{grundlagen/mengenlogik:definition-3}
\begin{sphinxadmonition}{note}{Definition 2.3}



Eine Teilmenge \(R \subset M \times \tilde M\) heißt Relation zwischen \(M\) und \(\tilde M\). Eine Teilmenge \(R \subset M \times M\) heißt Relation auf \(M\).
\end{sphinxadmonition}

Als Beispiel können wir die Menge \(M\) der Bundesliga\sphinxhyphen{}Teams betrachten und definieren eine Relation wenn zwei Teams bereits gegeneinander gespielt haben. \(R\) besteht also aus allen Spielpaaren \((x,y)\). Wollen wir nicht nach Heim\sphinxhyphen{} oder Auswärtsspielen unterscheiden, so schreiben wir neben \((x,y)\) auch \((y,x)\) in \(R\).
\label{grundlagen/mengenlogik:definition-4}
\begin{sphinxadmonition}{note}{Definition 2.4}



Eine Relation \(R \subset M \times M\) heißt\textbackslash{}begin\{*ize\}
\begin{itemize}
\item {} 
\sphinxstyleemphasis{reflexiv}, wenn für alle \(x \in M\) gilt: \((x,x) \in R\)

\item {} 
\sphinxstyleemphasis{symmetrisch}, wenn \((x,y) \in R\) impliziert: \((y,x) \in R\)

\item {} 
\sphinxstyleemphasis{antisymmetrisch}, wenn \((x,y) \in R\) und \((y,x) \in R\) impliziert, dass \(y=x\) gilt

\item {} 
\sphinxstyleemphasis{transitiv}, wenn aus \((x,y) \in R\) und \((y,z) \in R\) folgt, dass \((x,z)  \in R\) gilt.
\textbackslash{}end\{*ize

\end{itemize}
\end{sphinxadmonition}

Das obige Beispiel der Bundesligamannschaften ist nie reflexiv und nur dann symmetrisch, wenn wir nicht zwischen Heim\sphinxhyphen{} und Auswärtsspielen unterscheiden. Als weiteres Beispiel betrachten wir wieder \(M_1 = \{1,2,3\}\) und
\begin{equation*}
\begin{split} R=\{(x,y) \in M_1 \times M_1~|~x \leq y\} = \{ (1,1),(1,2),(1,3),(2,2),(2,3),(3,3)\}.
 \end{split}
\end{equation*}
Diese Relation ist reflexiv, anti\sphinxhyphen{}symmetrisch und transitiv.
Relationen mit diesen drei Eigenschaften (Reflexivität, Anti\sphinxhyphen{}Symmetrie, Transitivität) nennen wir Ordnungsrelation (oder Halbordnung). Statt \((x,y) \in R\) schreiben wir auch \(x \preceq y\). Eine Halbordnung heißt Ordnung, wenn sich alle Paare vergleichen lassen, d.h. es gilt \(x \preceq y\) oder \(y \preceq x\).
\label{grundlagen/mengenlogik:example-5}
\begin{sphinxadmonition}{note}{Example 2.1}



Sei \(M= \N \setminus \{0\}\) und \(R=\{(x,y) \in M \times M~|~x \text{ teilt } y\}\). Dies ist eine Halbordnung auf \(\N \setminus 0\), die keine Ordnung ist, da z.B. \(2\) und \(3\) nicht vergleichbar sind.
\end{sphinxadmonition}

Eine weitere wichtige Klasse sind Äquivalenzrelationen:
\label{grundlagen/mengenlogik:definition-6}
\begin{sphinxadmonition}{note}{Definition 2.5}



Eine Relation \(R \subset M \times M\) heißt Äquivalenzrelation, wenn sie reflexiv, symmetrisch und transitiv ist. Für \((x,y) \in R\) schreiben wir dann auch \(x \sim y\).
\end{sphinxadmonition}
\label{grundlagen/mengenlogik:example-7}
\begin{sphinxadmonition}{note}{Example 2.2}



Sei \(M= \Z\) und \(x \sim y\) wenn
\begin{equation*}
\begin{split}x-y \in 2 \Z = \{ 2 z ~|~z \in \Z \}\end{split}
\end{equation*}
gilt. D.h. \(x \sim y\) wenn beide gerade oder beide ungerade sind. In diesem Beispiel kennen wir schon die Einteilung in sogenannte Äquivalenzklassen, nämlich die geraden und die ungeraden Zahlen. Solche kann man auch für allgemeine Äquivalenzrelationen definieren
\end{sphinxadmonition}
\label{grundlagen/mengenlogik:definition-8}
\begin{sphinxadmonition}{note}{Definition 2.6}



Die von \(m \in M\) erzeugte Äquivalenzklasse unter einer Äquivalenzrelation ist gegeben durch
\begin{equation*}
\begin{split} [m] = \{ n \in M ~|~n \sim m\}.\end{split}
\end{equation*}
Jedes Element einer Äquivalenzrelation heißt Repräsentant
\end{sphinxadmonition}

Im Beispiel der Aufteilung in gerade und ungerade Zahlen sind typische Repräsentanten die Zahlen \(0\) (für gerade) und \(1\) (für ungerade). Wir wissen auch, dass eine Zahl entweder gerade oder ungerade ist und eine solche Eigenschaft gilt ebenfalls für allgemeine Äquivalenzrelationen:
\label{grundlagen/mengenlogik:lemma-9}
\begin{sphinxadmonition}{note}{Lemma 2.2}



Gegeben sei eine Äquivalenzrelation \(\sim\) auf \(M\). Dann ist jedes Element \(m \in M\) in genau einer Äquivalenzklasse enthalten. Für alle \(m,n \in M\) gilt entweder
\begin{equation*}
\begin{split}[m] = [n] \quad \text{oder} \quad [m] \cap [n] = \emptyset .\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  \([m]\) ist wegen der Reflexivität nicht leer (\(m \in [m]\)) und jedes Element ist damit auch in mindestens einer Äquivalenzklasse enthalten. Sei nun \(m \in [n]\) für \(n \neq m\), dann folgt wegen der Symmetrie auch \(n \in [m]\). Wegen der Transitivität gilt
\begin{equation*}
\begin{split}p \in [m] \Leftrightarrow p \sim m \Rightarrow p \sim n \Rightarrow p \in [n]\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} p \in [n] \Leftrightarrow p \sim n \Rightarrow p \sim m \Rightarrow p \in [m].\end{split}
\end{equation*}
Also folgt \([m]=[n]\). Sei nun \(m \not\sim n\). Dann gilt für \(p \sim m\) auch \(p \not\sim n\), da sonst wegen der Transitivität \(m \sim n\) folgt. Also gilt für jedes \(p \in [m]\) auch \(p \notin [n]\). Mit exakt demselben Argument folgern wir aus \(p \in [n]\) dann \(p \notin [m]\). Damit ist \([n] \cap [m] = \emptyset\)
\end{sphinxadmonition}

Bei der Relation
\begin{equation*}
\begin{split} x \sim y \Leftrightarrow x-y \in 2 \Z ,\end{split}
\end{equation*}
gibt es zwei Äquivalenzklassen, die geraden und die ungeraden Zahlen. Allgemeiner gibt es für \(n \in \N \setminus \{0,1\}\) und
\begin{equation*}
\begin{split}x \sim y \Leftrightarrow x - y \in n \Z\end{split}
\end{equation*}
genau \(n\) Äquivalenzklassen, die charakterisiert sind durch den Rest bei der Division durch \(n\), d.h. die Repräsentanten \(0,1,\ldots,n-1\). Man spricht deshalb auch von Restklassen.

Für eine Relation \(R\) auf \(M\) (oder kurz \(\sim\)) bezeichnen wir mit der Faktormenge \(M/R\) (oder \(M/\sim\)) die Menge der Äquivalenzklassen. Wir beachten, dass wir jede Äquivalenzklasse mit einem beliebigen Repräsentanten identifizieren können. So gilt bei der Einteilung in gerade und ungerade Zahlen \(M/\sim = \{[0],[1]\}\), was wir auch mit \(\{0,1\}\) identifizieren können.


\subsection{Abbildungen}
\label{\detokenize{grundlagen/mengenlogik:abbildungen}}
Eine Abbildung oder Funktion \(f:M_1 \rightarrow M_2\) ordnet jedem Element \(m_1 \in M_1\) ein Element \(m_2 =f(m_1) \in M_2\) zu.
Die Bedeutung von \sphinxstyleemphasis{Zuordnung} ist zwar relativ klar, mathematisch sauberer  ist eine Betrachtung als Mengen bzw. wieder als spezielle Relation:
\label{grundlagen/mengenlogik:definition-10}
\begin{sphinxadmonition}{note}{Definition 2.7}



Eine Relation \(f \subset M_1 \times M_2\) heißt Funktion (oder Abbildung) von \(M_1\) nach \(M_2\), wenn

*\(i)\) \(D(f) = \{m_1 \in M_1~|~\exists m_2 \in M_2: (m_1,m_2) \in f \} = M_1 \)

*\(ii)\) \((m,n) \in f\) und \((m,p) \in f\) impliziert \(n=p\).\textbackslash{}end\{*ize\}
\end{sphinxadmonition}

Wir nennen \(M_1\) den Definitionsbereich und \(M_2\) den Bildbereich von \(f\).
\begin{equation*}
\begin{split}f(M_1) = \{m_2 \in M_2~|~\exists m_1 \in M_1: (m_1,m_2) \in f)\}\end{split}
\end{equation*}
heißt Wertebereich (oder Bild) von \(f\).
Da zu jedem \(m_1 \in M_1\) nun genau ein \(m_2 \in M_2\) mit \((m_1,m_2) \in f\) existiert, können wir hier auch die Zuordnung \(m_2=f(m_1)\) definieren. Damit haben wir die gewohnte Form
\begin{equation*}
\begin{split}f: M_1 \rightarrow M_2, m \mapsto f(m),\end{split}
\end{equation*}
die wir im Folgenden für Funktionen auch meist verwenden werden. Wir beachten, dass die Sichtweise über die Relation dem Funktionsgraphen bestehend aus den Punkten \((m,f(m))\) entspricht.
\label{grundlagen/mengenlogik:definition-11}
\begin{sphinxadmonition}{note}{Definition 2.8}



Eine Funktion \(f:M_1 \rightarrow M_2\) heißt
\begin{itemize}
\item {} 
\sphinxstyleemphasis{surjektiv}, wenn Bildbereich und Wertebereich übereinstimmen, d.h. \(f(M_1)=M_2\)

\item {} 
\sphinxstyleemphasis{injektiv}, wenn aus \((m_1,n) \in f\) und \((m_2,n) \in f\) (bzw. \(f(m_1) =f(m_2)\)) folgt \(m_1=m_2\).

\item {} 
\sphinxstyleemphasis{bijektiv}, wenn sie injektiv und surjektiv ist.

\end{itemize}
\end{sphinxadmonition}

Ist \(f\) bijektiv, dann erfüllt die Relation
\begin{equation*}
\begin{split} f^{-1} = \{(m_2,m_1) ~|~ (m_1,m_2) \in f\}\end{split}
\end{equation*}
auch die Eigenschaften einer Funktion und wir nennen \(f^{-1}\) die Umkehrfunktion von \(f\). Es gilt dann
\begin{equation*}
\begin{split}\forall m_1 \in M_1: f^{-1}(f(m_1))=m_1\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split}\forall m_2 \in M_2: f(f^{-1}(m_2))=m_2.\end{split}
\end{equation*}
Wir beachten, dass \(f^{-1}\) als Relation auch definiert ist, wenn \(f\) nicht bijektiv ist, \(f^{-1}\) ordnet dann jedem \(m_2\) eine Teilmenge von \(M_1\) zu (die auch leer sein kann).
Die Einschränkung einer Funktion auf \(U \subset M_1\) bezeichnen wir mit
\begin{equation*}
\begin{split}f|_U = \{ (m_1,m_2) \in f~|~ m_1 \in U\}.\end{split}
\end{equation*}

\subsection{Mächtigkeit von Mengen}
\label{\detokenize{grundlagen/mengenlogik:machtigkeit-von-mengen}}
Mit Hilfe von Abbildungen können wir die Mächtigkeit, d.h. die Größe, von Mengen vergleichen. Für endliche Mengen ist die Mächtigkeit genau die Anzahl der Elemente, die wir einfach durchzählen können. Dies bedeutet wir konstruieren eine bijektive Abbildung von \(\{1,\ldots,n\}\) auf \(M\) und schreiben als Mächtigkeit dann \(\vert M \vert\). Für die leere Menge setzen wir \(\vert \emptyset \vert =0\). Dies können wir allgemeiner zum Vergleich der Mächtigkeit verwenden. Wir sagen, dass zwei Mengen \(M_1\) und \(M_2\) die gleiche Mächtigkeit haben, wenn es eine Bijektion \(f: M_1 \rightarrow M_2\) gibt.
Die \sphinxstyleemphasis{kleinste} undendliche Menge ist \(\N\), wir definieren die Mächtigkeit von \(\N\) als \(\aleph_0\) (gesprochen Aleph Null) als kleinstes Maß für Unendlichkeit, diese nennen wir erste Kardinalzahl. Mengen \(M\), die durch eine Bijektion auf \(\N\) abgebildet werden können, nennen wir abzählbar (oder abzählbar unendlich) und schreiben dementsprechend \(|M|=\aleph_0\).
\label{grundlagen/mengenlogik:example-12}
\begin{sphinxadmonition}{note}{Example 2.3}



Es gilt \(|\Z|=\aleph_0\). Wir betrachten dazu die bijektive Abbildung \(f: \N \rightarrow \Z\),\$\$ f(n) = \textbackslash{}left\{ \textbackslash{}begin\{array\}\{ll\} \textbackslash{}frac\{n\}2 \& \textbackslash{}text\{falls \} n \textbackslash{}text\{ gerade\} \textbackslash{}
\begin{itemize}
\item {} 
\textbackslash{}frac\{n+1\}2 \& \textbackslash{}text\{falls \} n \textbackslash{}text\{ ungerade\} \textbackslash{}end\{array\} \textbackslash{}right. \$\$

\end{itemize}
\end{sphinxadmonition}
\label{grundlagen/mengenlogik:example-13}
\begin{sphinxadmonition}{note}{Example 2.4}



Sei \(M= \N \times \N = \N^2\), dann gilt auch \(|M|=\aleph_0\). Wir betrachten dazu die bijektive Abbildung
\begin{equation*}
\begin{split}f: M \rightarrow \N, \quad (m,n) \mapsto \frac{1}2(m+n)(m+n+1) + n.\end{split}
\end{equation*}
Es gilt mit Hintereinanderausführung ähnlicher Abbildung übrigens auch \(|\N^n|=\aleph_0\).
\end{sphinxadmonition}

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Bertrand Russell}

\sphinxhref{https://de.wikipedia.org/wiki/Bertrand\_Russell}{Bertrand Arthur William Russell} (* 18. Mai 1872 bei Trellech, Monmouthshire, Wales; † 2. Februar 1970 in Penrhyndeudraeth, Gwynedd, Wales) war ein britischer Philosoph, Mathematiker, Religionskritiker und Logiker.
\end{sphinxShadowBox}

Die Mengenlehre stößt an ihre Grenzen, wenn man Selbstbezüge in der Definition von Mengen einbaut. Dies zeigt ein bekanntes Beispiel von Bertrand Russell.

Sei
\begin{equation*}
\begin{split} M=\{ x ~|~x \text{ ist } Menge \}\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split}N = \{x \in M~|~x \notin x\}.\end{split}
\end{equation*}
Damit können wir die unbeantwortbare Frage \sphinxstyleemphasis{Gilt \(N \in N\) ?} stellen. Nehmen wir an es gilt \(N \in N\), dann ist per Definition von \(N\) auch \(N \notin N\) und umgekehrt. Diese sogenannte Russell’sche Antinomie legt nahe, dass man Selbstbezüge in der Definition von Mengen vermeiden sollte um logisch konsistent zu bleiben.


\subsection{Logik und Aussagen}
\label{\detokenize{grundlagen/mengenlogik:logik-und-aussagen}}
Im Folgenden werden wir nun etwas näher die Aussagenlogik betrachten. Eine Aussage \(A\) ist für uns ein Element der Menge \(\{\text{wahr},\text{falsch}\}\). Eine Aussageform ist eine Abbildung von einer Menge an Möglichkeiten in die Menge \(\{\text{wahr},\text{falsch}\}\). So ist etwa \(4\) ist gerade eine (wahre) Aussage, und \(x \in N, x\) gerade eine Aussageform. Wir identifizieren im Folgenden meist die Aussage \(A\) mit der Abbildung \(M_A:M \rightarrow \{\text{wahr},\text{falsch}\}\), solange der Zusammenhang klar ist.
Sind \(A,B\) zwei Aussagen, dann können wir mit sogenannten Junktoren neue Aussagen herleiten:
\begin{itemize}
\item {} 
\sphinxstyleemphasis{Negation:} nicht \(A\), geschrieben \(\lnot A\), entspricht der Abbildung, die falsch liefert wenn \(A\) wahr liefert und umgekehrt.

\item {} 
\sphinxstyleemphasis{Konjunktion:} \(A\) und \(B\), geschrieben \(A \land B\), ist eine Abbildung, die wahr liefert wenn \(A\) und \(B\) wahr sind, sonst erhalten wir falsch.

\item {} 
\sphinxstyleemphasis{Disjunktion:} \(A\) oder \(B\), geschrieben \(A \lor B\), ist eine Abbildung, die falsch liefert wenn \(A\) und \(B\) falsch sind, sonst erhalten wir wahr.

\item {} 
\sphinxstyleemphasis{Implikation:} wenn \(A\), dann \(B\), geschrieben \(A \Rightarrow B\).

\item {} 
\sphinxstyleemphasis{Äquivalenz:} \(A\) äquivalent zu \(B\), \(A \Leftrightarrow B\).

\end{itemize}

Bei zwei Aussagen können wir die Verknüpfung durch Junktoren mit Fallunterscheidungen charakterisieren, die sogenannten Wahrheitstafeln, eine Tabelle der Form


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
A
&\sphinxstyletheadfamily 
B
&\sphinxstyletheadfamily 
J(A,B)
\\
\hline
W
&
W
&
*
\\
\hline
W
&
F
&
*
\\
\hline
F
&
W
&
*
\\
\hline
F
&
F
&
*
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

wobei \(J(A,B)\) die Junktion bezeichnet. Als Beispiel betrachten wir das logische Und:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|\X{33}{99}|\X{33}{99}|\X{33}{99}|}
\hline
\sphinxstyletheadfamily 
A
&\sphinxstyletheadfamily 
B
&\sphinxstyletheadfamily 
A \(\land\) B
\\
\hline
W
&
W
&
W
\\
\hline
W
&
F
&
F
\\
\hline
F
&
W
&
F
\\
\hline
F
&
F
&
F
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}

Mit Hilfe von Wahrheitstafeln können wir auch Verknüpfungen von Junktoren betrachten und deren Äquivalenz zu anderen Formen nachweisen. Als Beispiel betrachten wir \(\lnot (A \land B)\).
Die Wahrheitstafel liefert


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
A
&\sphinxstyletheadfamily 
B
&\sphinxstyletheadfamily 
\(\lnot (A \land B)\)
&\sphinxstyletheadfamily 
\((\lnot A) \lor (\lnot B)\)
\\
\hline
W
&
W
&
F
&
F
\\
\hline
W
&
F
&
W
&
W
\\
\hline
F
&
W
&
W
&
W
\\
\hline
F
&
F
&
W
&
W
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Damit sehen wir
\begin{equation*}
\begin{split}\lnot (A \land B) = (\lnot A) \lor (\lnot B).\end{split}
\end{equation*}
Wie bereits vorher verwendet können wir Quantoren wie \(\forall\) und \(\exists\) verwenden und diese auch verknüpfen.
\begin{itemize}
\item {} 
\(\forall x \in M: A(x) \) bedeutet, dass die Aussageform \(A\) für alle \(x\) in \(M\) wahr ist.

\item {} 
\(\exists x \in M: A(x) \) bedeutet, dass die Aussageform \(A\) für zumindest ein \(x\) in \(M\) wahr ist (oder äquivalent nicht für alle \(x \in M\) falsch ist).

\end{itemize}

Mit offensichtlichen Ergebnissen erhalten wir auch die Negation, Konjunktion etc. von Quantoren. So gilt etwa
\begin{equation*}
\begin{split}\lnot( \forall x \in M: A(x) ) = \exists x \in M: \lnot A(x)\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split}\lnot( \exists x \in M: A(x) ) = \forall x \in M: \lnot A(x)\end{split}
\end{equation*}

\section{Zahlensysteme}
\label{\detokenize{grundlagen/zahlensysteme:zahlensysteme}}\label{\detokenize{grundlagen/zahlensysteme::doc}}
Im Folgenden betrachten wir die wichtigsten Zahlensysteme und ihre Eigenschaften. Wir beginnen bei den natürlichen Zahlen und machden den Weg bis zu den reellen und komplexen Zahlen.


\subsection{Natürliche Zahlen}
\label{\detokenize{grundlagen/zahlensysteme:naturliche-zahlen}}
Die bisher verwendete Aufzählung \(\N = \{0,1,2,3,\ldots\}\) ist etwas unbefriedigend, da wird die Punkte ja nicht bis unendlich auffüllen können. Deshalb ist es besser die natürlichen Zahlen axiomatisch zu definieren (über die sogenannten Dedekind\sphinxhyphen{}Peano Axiome):
\label{grundlagen/zahlensysteme:definition-0}
\begin{sphinxadmonition}{note}{Definition 2.9}



Die natürlichen Zahlen sind eine Menge mit den folgenden Eigenschaften:
\begin{itemize}
\item {} 
Es gibt ein ausgezeichnetes Element \(0\)

\item {} 
Es gibt eine Nachfolgerabbildung \({\cal N}:\N \rightarrow \N\), sodass \(0 \notin {\cal N}(\N)\) und \({\cal N}\) injektiv ist.

\item {} 
Falls für \(M \subset \N\) gilt: \(0 \in M\) und \({\cal N}(M) \subset M\), dann ist \(M=\N\).

\end{itemize}
\end{sphinxadmonition}

Man kann zeigen, dass damit die natürlichen Zahlen wohldefiniert sind. Nun definieren wir die Addition und Multiplikation mit
\begin{equation*}
\begin{split} m + {\cal N}(n) = {\cal N}(m+n)\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} m 0 = 0, \quad m {\cal N}(n) = mn +m.\end{split}
\end{equation*}
Wir nennen \({\cal N}(0)=1\) und haben daher auch
\( {\cal N}(n) = n+1\).


\subsection{Vollständige Induktion}
\label{\detokenize{grundlagen/zahlensysteme:vollstandige-induktion}}
Die Definition der natürlichen Zahlen impliziert auch die Beweistechnik der vollständigen Induktion:
\label{grundlagen/zahlensysteme:theorem-1}
\begin{sphinxadmonition}{note}{Theorem 2.1}



\(A(0)\) sei wahr und aus \(A(n)\) wahr folgt \(A(n+1)\) wahr für alle \(n \in \N\). Dann ist die Aussage \(A(m)\) wahr für alle \(m \in N\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(N=\{m \in \N~|~A(m) \text{ ist wahr }\}\). Nun gilt nach unseren Voraussetzungen \(0 \in N\) und für jedes \(n \in N\) auch \({\cal N}(n) \in N\). Also gilt \(N= \N\) und die Aussage ist für alle \(m \in \N\) wahr
\end{sphinxadmonition}

Damit haben wir ein sehr nützliches Prinzip, mit dem wir Aussagen über den natürlichen Zahlen zeigen können. Die Überprüfung, dass \(A(0)\) wahr ist, nennen wir Induktionsanfang, die Folgerung von \(n\) auf \(n+1\) den Induktionsschritt. Beim Induktionsschritt wird die angenommene Gültigkeit von \(A(n)\) auch als Induktionsannahme bezeichnet. Natürlich können wir Induktionsbeweise auch für Teilmengen der Form \(N= \{n \in \N~|~n \geq k \}\)
anwenden, denn durch die Variablentransformation \(m=n-k\) kommen wir auf die ursprüngliche Version. Damit können wir also den Induktionsanfang bei \(k\) und den Induktionsschritt für \(n \geq k\) machen um eine Aussage auf \(N\) zu zeigen.Wir illustrieren das Induktionsprinzip an zwei Beispielen:
\label{grundlagen/zahlensysteme:example-2}
\begin{sphinxadmonition}{note}{Example 2.5}



Wir beweisen das Kommutativgesetz der Addition, d.h. wir zeigen \(n+m=m+n\) für alle \(n,m \in \N\). Wir beginnen mit der einfacheren Version \(n+1=1+n\) für alle \(n\in\N\). Der Induktionsanfang bei \(n=0\) liefert \(1=1\), also ist \(A(0)\) wahr. Sei nun die Induktionsannahme \(n+1=1+n\), dann wollen wir folgern
\begin{equation*}
\begin{split} (n+1)+1=1+(n+1).\end{split}
\end{equation*}
Dazu können wir aber die Induktionsannahme verwenden, denn es gilt \(n+1+1 = 1+n+1\) und damit folgt \(A(n+1)\) aus \(A(n)\).
Mit einem zweiten Induktionsbeweis zeigen wir nun die eigentliche Aussage \(n+m=m+n\), wobei wir jetzt Induktion in \(m\) verwenden. Für \(m=0\) ist die Aussage wahr, da sie zu \(n=n\) reduziert. Nehmen wir nun an, dass \(n+m=m+n\) gilt, so wollen wir zeigen
\begin{equation*}
\begin{split} n+(m+1) = (m+1) + n\end{split}
\end{equation*}
Dazu verwenden wir die oben bewiesene Eigenschaft \(m+1 =1+m\) bzw. \(n+1=1+m\) sowie die Induktionsannahme, also gilt
\begin{equation*}
\begin{split} n+m+1=n+1+m=1+n+m=1+m+n=m+1+n\end{split}
\end{equation*}
und wir haben den Induktionsschritt erfolgreich durchgeführt
\end{sphinxadmonition}
\label{grundlagen/zahlensysteme:example-3}
\begin{sphinxadmonition}{note}{Example 2.6}



Wir beweisen die Gauss’sche Summationsformel für die Summe der ersten \(n\) Zahlen
\begin{equation*}
\begin{split}  \sum_{i=1}^n i = \frac{n(n+1)}2.\end{split}
\end{equation*}
Hier sind wir also im Fall \(k=1\) wie oben beschrieben. Wir definieren die Aussage \(A(n)\) als \sphinxstyleemphasis{Die Summenformel gilt für die ersten \(n\) Zahlen}.
Den Induktionsanfang \(A(1)\) können wir durch direktes Ausrechnen nachprüfen
\begin{equation*}
\begin{split}  1 = \sum_{i=1}^n 1 = \frac{1(1+1)}2 = 1.\end{split}
\end{equation*}
Nun führen wir den Induktionsschritt durch, es gelte \(A(n)\). Wir berechnen dann
\begin{equation*}
\begin{split} \sum_{i=1}^{n+1} i = \sum_{i=1}^n i  + (n+1) = \frac{n(n+1)}2 + (n+1) = \frac{(n+1)(n+2)}2,\end{split}
\end{equation*}
wobei wir in der Mitte die bereits gültige Aussage \(A(n)\)  eingesetzt haben.Damit haben wir \(A(n+1)\) gefolgert und somit die Summenformel für alle \(n \in \N \setminus \{0\}\) gültig
\end{sphinxadmonition}


\subsection{Ganze Zahlen}
\label{\detokenize{grundlagen/zahlensysteme:ganze-zahlen}}
Nachdem wir die natürlichen Zahlen bereits definiert haben, können wir nun die ganzen Zahlen als
\begin{equation*}
\begin{split} Z = \N \cup -\N = \{n ~|~n \in \N \text{ oder } -n \in \N \}\end{split}
\end{equation*}
definieren mit den gleichen Regeln für die Addition und Multiplikation wie in \(\N\), wobei \(-n\) das zu \(n\) inverse Element bezüglich der Addition ist, d.h.
\begin{equation*}
\begin{split}\forall n \in \Z: -n + n = n + (-n) = 0.\end{split}
\end{equation*}
Die \(0\) nennen wir neutrales Element, da \(n+0=0+n=n\), d.h. Addition mit \(0\) verändert die Zahl nicht.
Daraus folgern wir übrigens auch die Regel \sphinxstyleemphasis{Minus mal Minus ist Plus}. Es gilt ja, dass \(-(-n))\) das inverse Element zu \(-n\) ist und das ist wegen \(n+(-n) = (-n)+n=0\) durch \(n\) gegeben.
Die Addition und Multiplikation auf den ganzen Zahlen liefern eine interessante algebraische Struktur.
Die ganzen Zahlen mit der Operation der Addition sind eine sogenannte Gruppe:
\label{grundlagen/zahlensysteme:definition-4}
\begin{sphinxadmonition}{note}{Definition 2.10}



Eine Menge \(G\) zusammen mit einer Abbildung \(\circ: G \times G \rightarrow G\) heißt \sphinxstyleemphasis{Gruppe}, wenn die folgenden Eigenschaften erfüllt sind:
\begin{itemize}
\item {} 
\( \forall a,b,c \in G: a \circ (b \circ c) = (a\circ b) \circ c\) (Assoziativität)

\item {} 
\(\exists n \in G~ \forall a \in G: n \circ a = a\) (neutrales Element)

\item {} 
\(\forall a \in G~\exists a' \in G: a' \circ a = n\) (Existenz eines inversen Elements)

\end{itemize}

Die Gruppe heißt abelsch (oder kommutativ), wenn für alle \(a,b \in G\) gilt: \(a \circ b = b \circ a\).
\end{sphinxadmonition}

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Niels Abel}

\sphinxhref{https://de.wikipedia.org/wiki/Niels\_Henrik\_Abel}{Niels Henrik Abel} (* 5. August 1802 auf der Insel Finnøy, Ryfylke, Norwegen; † 6. April 1829 in Froland, Aust\sphinxhyphen{}Agder, Norwegen) war ein norwegischer Mathematiker.
\end{sphinxShadowBox}

Wir beachten, dass wir die Eigenschaft des neutralen und inversen Elements nur einseitig definiert haben, bei abelschen Gruppen folgt sofort \(a \circ n = a\) und \(a \circ a'=n\). Diese Eigenschaft folgt aber auch bei allgemeinen Gruppen. Es gilt ja dann
\begin{equation*}
\begin{split}a' \circ a  \circ a' = n \circ a' = a'\end{split}
\end{equation*}
und damit
\begin{equation*}
\begin{split}(a \circ a') \circ a \circ a' = a \circ a' .\end{split}
\end{equation*}
Nun gibt es ein inverses Element \(b\) zu \((a \circ a')\) also folgt
\begin{equation*}
\begin{split}a \circ a' = b \circ (a \circ a') \circ a \circ a' = b \circ (a \circ a') = n.\end{split}
\end{equation*}
Analog zeigt man auch \(a \circ n = a\).
Es gibt in einer Gruppe nur ein neutrales Element und zu jedem \(a\) ein eindeutiges inverses Element. Seien \(n_1,n_2\) neutrale Elemente, dann gilt ja
\begin{equation*}
\begin{split}n_1 = n_1 \circ n_2 = n_2,\end{split}
\end{equation*}
also sind sie gleich. Seien \(a_1'\) und \(a_2'\) inverse Elemente zu \(a\), dann gilt
\begin{equation*}
\begin{split}a_1' = a_1' \circ (a \circ a_2') = (a_1' \circ a) \circ a_2' = a_2'.\end{split}
\end{equation*}
Neben \(\Z\) können wir noch andere Beispiele von Gruppen finden.
\label{grundlagen/zahlensysteme:example-5}
\begin{sphinxadmonition}{note}{Example 2.7}



Sei \(p \in \N \setminus \{0\}\). Dann ist die Faktormenge \(\Z_p = (\Z/p\Z, +)\) eine Gruppe, die wir mit der Restklasse
\( \{0,1,\ldots,p-1\}\) identifizieren können. Im Sinne einer Restklasse ergibt sich automatisch die Addition modulo \(p\). Das neutrale Element ist \(n=0\), das zu \(m\) inverse Element ist \(m'=p-m\). Wir betrachten als konkretes Beispiel \(p=3\), hier ist die Addition gegeben durch


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
+
&\sphinxstyletheadfamily 
\(0\)
&\sphinxstyletheadfamily 
\(1\)
&\sphinxstyletheadfamily 
\(2\)
\\
\hline
\(0\)
&
\(0\)
&
\(1\)
&
\(2\)
\\
\hline
\(1\)
&
\(1\)
&
\(2\)
&
\(0\)
\\
\hline
\(2\)
&
\(2\)
&
\(0\)
&
\(1\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\end{sphinxadmonition}

Wir können \(\Z\) auch mit den beiden Operationen \(+\) und \(\cdot\) betrachten, dann ergibt sich die Struktur eines Rings:
\label{grundlagen/zahlensysteme:definition-6}
\begin{sphinxadmonition}{note}{Definition 2.11}



Ein Ring \(R\) ist eine bezüglich der Operation \(+\) abel’sche Gruppe mit einer zweiten Operation \(\cdot : R \times R \rightarrow R\), sodass folgende Eigenschaften erfüllt sind:
\begin{itemize}
\item {} 
\( \forall a,b,c \in R: a \cdot (b \cdot c) = (a\cdot b) \cdot c\) (Assoziativität)

\item {} 
\( \forall a,b,c \in R: a \cdot (b+c) = a\cdot b + a \cdot c\) und \((b+c) \cdot a = b \cdot a + c \cdot a\) (Distributivgesetz)

\end{itemize}

Wir bezeichnen das neutrale Element der Addition \(+\) als 0.
Falls ein zusätzliches neutrales Element \(1\) existiert, sodass für alle \(a  \in R\) gilt \(1 \cdot a = a \cdot 1\), dann
heißt \(R\) Ring mit Einselement. Gilt für alle \(a, b  \in R\), dass \(a \cdot b = b \cdot a\), dann heißt \(R\) kommutativ
\end{sphinxadmonition}

Wir beachten, dass in einem Ring aus \(a_1 \cdot b = a_2 \cdot b\) nicht \(a_1=a_2\) folgen muss, auch wenn \(b\neq 0\) gilt. Aus diesen Gründen müssen wir beim neutralen Element der Multiplikation auch die beidseitige Definition verwenden.
\label{grundlagen/zahlensysteme:example-7}
\begin{sphinxadmonition}{note}{Example 2.8}



Wir betrachten \((\Z/2\Z, +,\cdot)=(\{0,1\},+,\cdot)\). Hier gilt


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\(+\)
&\sphinxstyletheadfamily 
\(0\)
&\sphinxstyletheadfamily 
\(1\)
&

&\sphinxstyletheadfamily 
\(\cdot\)
&\sphinxstyletheadfamily 
\(0\)
&\sphinxstyletheadfamily 
\(1\)
\\
\hline
\(0\)
&
\(0\)
&
\(1\)
&

&
\(0\)
&
\(0\)
&
\(0\)
\\
\hline
\(1\)
&
\(1\)
&
\(0\)
&

&
\(1\)
&
\(0\)
&
\(1\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Wir sehen, dass \(0\) das neutrale Element der Addition und \(1\) das neutrale Element der Multiplikation ist. Dazu sind die Operationen kommutativ, wir haben also einen kommutativen Ring mit Einselement.
\end{sphinxadmonition}
\label{grundlagen/zahlensysteme:example-8}
\begin{sphinxadmonition}{note}{Example 2.9}



Wir betrachten \((\Z/4\Z, +,\cdot)=(\{0,1,2,3\},+,\cdot)\). Hier gilt


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\(+\)
&\sphinxstyletheadfamily 
\(0\)
&\sphinxstyletheadfamily 
\(1\)
&\sphinxstyletheadfamily 
\(2\)
&\sphinxstyletheadfamily 
\(3\)
\\
\hline
\(0\)
&
\(0\)
&
\(1\)
&
\(2\)
&
\(3\)
\\
\hline
\(1\)
&
\(1\)
&
\(2\)
&
\(3\)
&
\(0\)
\\
\hline
\(2\)
&
\(2\)
&
\(3\)
&
\(0\)
&
\(1\)
\\
\hline
\(3\)
&
\(3\)
&
\(0\)
&
\(1\)
&
\(2\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\(\cdot\)
&\sphinxstyletheadfamily 
\(0\)
&\sphinxstyletheadfamily 
\(1\)
&\sphinxstyletheadfamily 
\(2\)
&\sphinxstyletheadfamily 
\(3\)
\\
\hline
\(0\)
&
\(0\)
&
\(0\)
&
\(0\)
&
\(0\)
\\
\hline
\(1\)
&
\(0\)
&
\(1\)
&
\(2\)
&
\(3\)
\\
\hline
\(2\)
&
\(0\)
&
\(2\)
&
\(0\)
&
\(2\)
\\
\hline
\(3\)
&
\(0\)
&
\(3\)
&
\(2\)
&
\(1\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Wir sehen, dass \(0\) das neutrale Element der Addition und \(1\) das neutrale Element der Multiplikation ist. Dazu sind die Operationen kommutativ, wir haben also einen kommutativen Ring mit Einselement.
\end{sphinxadmonition}


\subsection{Rationale Zahlen}
\label{\detokenize{grundlagen/zahlensysteme:rationale-zahlen}}
Die rationalen Zahlen definieren wir üblicherweise als Brüche von ganzen Zahlen. Dabei haben wir natürlich das Problem des \sphinxstyleemphasis{Kürzens}. Zwei Brüche der Form \(\frac{p_1}{q_1}\) und \(\frac{p_2}{q_2}\) mit \(p_i, q_i \in \Z\),\(q_i \neq 0\) (für \(i=1,2\)) betrachten wir als gleich, wenn nach ausmultiplizieren
\(p_1 q_2= p_2 q_1\) gilt.
Die mathematische Grundlage zur Betrachtung dieses Problems haben wir bereits gelegt, nämlich die Faktorisierung bezüglich einer Äquivalenzrelation. Wir definieren zunächst die rationalen Zahlen als \(\Z \times (\N \setminus \{0\})\), d.h. alle möglichen Zähler\sphinxhyphen{}Nenner Paare. Nun definieren wir eine Äquivalenzrelation
\begin{equation*}
\begin{split}\frac{p_1}{q_1} \sim \frac{p_2}{q_2}  \quad \Leftrightarrow \quad p_1 q_2= p_2 q_1.\end{split}
\end{equation*}
Reflexivität, Symmetrie und Transitivität sind wegen den Eigenschaft der Multiplikation in \(\N\) gegeben.
Nun definieren wir die rationalen Zahlen als Faktorgruppe
\begin{equation*}
\begin{split}\Q = ( \Z \times (\N \setminus \{0\}) /  \sim).\end{split}
\end{equation*}
Eine rationale Zahl entspricht also der Klasse an Brüchen, die den gleichen Wert ergeben. Wir können als Repräsentanten dann den gekürzten Bruch, also teilerfremde \(p\) und \(q\) wählen. Wir schreiben dann statt \((p,q)\) einfach \(\frac{p}q\). Auch bei Addition und Multiplikation betrachten wir Äquivalenzklassen und definieren
\begin{equation*}
\begin{split} \frac{p_1}{q_1} +   \frac{p_2}{q_2}  = \frac{p_1 q_2 + p_2 q_1}{q_1q_2}\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} \frac{p_1}{q_1}  \cdot  \frac{p_2}{q_2}  = \frac{p_1   p_2  }{q_1q_2} .\end{split}
\end{equation*}
Mit diesen Gesetzen weisen wir leicht nach, dass \(\Q\) ein Ring ist, es gilt aber sogar noch die zusätzliche Eigenschaft, dass es ein inverses Element der Multiplikation gibt. Dies führt uns auf die nächste Definition:
\label{grundlagen/zahlensysteme:definition-9}
\begin{sphinxadmonition}{note}{Definition 2.12}



Ein kommutativer Ring \(\K\) mit Einselement \(1 \neq 0\) heißt Körper, wenn
\begin{equation*}
\begin{split} \forall a \in\K \setminus \{0\} ~\exists b \in K:  a b = 1.\end{split}
\end{equation*}\end{sphinxadmonition}

In \(\Q\) ist das inverse Element für \(\frac{p}q \neq 0\) gegeben durch \(\frac{q}p\). Allgemein gilt in einem Körper, dass \(x\) das inverse Element zu \(1/x\). Darüber hinaus erfüllt ein Körper die sogenannte Nullteilerfreiheit, d.h.
\begin{equation*}
\begin{split}x \cdot y = 0 \quad \Rightarrow x = 0 \text{ oder } y =0.\end{split}
\end{equation*}
Wir haben am Beispiel des Rings \(\Z_4=\Z/4\Z\) (dort ist \(2 \cdot 2 = 0\)) schon gesehen, das die Nullteilerfreiheit nicht in jedem Ring gilt.
Neben der Körpereigenschaft haben wir mit der üblichen Ordnungsrelation \(\leq\), d.h.
\begin{equation*}
\begin{split}\frac{p_1}{q_1} \leq \frac{p_2}{q_2} \quad \Leftrightarrow \quad p_1 q_2 \leq p_2 q_1\end{split}
\end{equation*}
auch eine vollständige Ordnung auf \(\Q\). Diese verträgt sich in gewisser Weise mit den Rechenoperationen, was uns auf folgende Definition führt:
\label{grundlagen/zahlensysteme:definition-10}
\begin{sphinxadmonition}{note}{Definition 2.13}



Ein Körper \(\K\) mit vollständiger Ordnung \(\preceq\) bzw. \(\prec\)  heißt angeordnet, wenn die beiden folgenden Bedingungen erfüllt sind:
\begin{itemize}
\item {} 
\(\forall~x,y,z \in \K, x \prec y: x+z \prec y+z\)

\item {} 
\(\forall x,y \in \K, 0 \prec x, 0 \prec y: 0 \prec x \cdot y\).

\end{itemize}
\end{sphinxadmonition}

Für einen angeordneten Körper erhalten wir verschiedene intuitive Eigenschaften:
\begin{itemize}
\item {} 
Für \(x \in \K \setminus \{0\}\) gilt entweder \(0 \prec x\) oder \(0 \prec -x\). Dies sehen wir aus der ersten Eigenschaft, denn falls \(0 \prec x\) gilt können wir \(z=-x\) addieren. Damit sehen wir auch, dass \( 0 \preceq 1\) gelten muss, sonst wäre \(0 \preceq -1\) und damit wegen der zweiten Eigenschaft für \(0 \prec x \) auch \(0 \prec -x\).

\item {} 
\(0 \prec x^2 = (-x)^2\). Dies sehen wir aus der zweiten Eigenschaft, entweder für \(0 \prec x\) oder \(0 \prec -x\).

\item {} 
Aus \(x_1 \prec x_2\) und \(y_1 \prec y_2\) folgt \(x_1 + y_1 \prec x_2 + y_2\).

\item {} 
Aus \(0 \prec x\) folgt \(0 \prec \frac{1}x\).

\end{itemize}

In einem angeordneten Körper kann man den Betrag als
\begin{equation*}
\begin{split}\vert x \vert := \left\{ \begin{array}{rl} x & 0 \preceq x \\ -x & x \prec 0 \end{array} \right.\end{split}
\end{equation*}
und das Vorzeichen (Signum) als
\begin{equation*}
\begin{split}\text{sign}(x) =  \left\{ \begin{array}{rl} 1 & 0 \prec  x \\ 0 & x =0\\-1 & x \prec 0 \end{array}\right.\end{split}
\end{equation*}
definieren. Auch für Betrag und Vorzeichen gelten die üblichen Eigenschaften wie
\begin{itemize}
\item {} 
\(x = |x|\) sign\((x)\), \(|-x|=|x|\).

\item {} 
\(|x|~|y|=|xy|\), sign\((x)\)sign\((y) = \)sign\((xy)\).

\item {} 
\(0 \preceq |x|\) und \(|x|=0 \Leftrightarrow x =0\).

\item {} 
\(|x+y| \preceq |x|+|y|\) (Dreiecksungleichung)

\end{itemize}

Da die ersten Eigenschaften sehr einfach zu zeigen sind, beweisen wir nur kurz die Dreiecksungleichung. Wegen \(x \preceq |x|\) und \(-x \preceq |x|\) folgt
\begin{equation*}
\begin{split}x+y \preceq |x|+|y|, \qquad -x-y \preceq |x|+|y|\end{split}
\end{equation*}
und damit direkt die Dreiecksungleichung.
Neben Betrag und Vorzeichen können wir auch das Maximum und Minimum definieren durch
\begin{equation*}
\begin{split}\max\{x,y\} = \left\{ \begin{array}{rl} x & y \preceq x \\ y & x \prec y \end{array} \right.\end{split}
\end{equation*}
bzw.
\begin{equation*}
\begin{split}\min\{x,y\} = \left\{ \begin{array}{rl} y & y \preceq x \\ x & x \prec y. \end{array} \right.\end{split}
\end{equation*}
Durch eine Hintereinanderausführung erhalten wir dann auch Maximum und Minimum endlicher Mengen, z.B.
\begin{equation*}
\begin{split}\max\{x_1,\ldots,x_n\} = \max\{x_1,\max\{x_2, \ldots, \max\{x_{n-1},x_n\} \ldots\}\}.\end{split}
\end{equation*}
\(\Q\) ist nicht nur ein angeordneter Körper, sondern wird sogar als \sphinxstyleemphasis{archimedisch angeordnet} bezeichnet. Dies bedeutet, dass für alle \(x \in \K\) ein \(n \in \N\) existiert mit \(x \prec n\). Bei einer rationalen Zahl \(\frac{p}q\) können wir einfach \(n = |p|+1\) wählen.
Wir betrachten zum Abschluss noch die Mächtigkeit von \(\Q\). Wir haben schon gesehen, dass es eine bijektive Abbildung \(f: \N \mapsto \N \times \N\) gibt, und ähnlich können wir eine bijektive Abbildung von \(f: \N \rightarrow \Z \rightarrow \Z \times Z\) konstruieren. Da die Abbildung von \(\Z \times \Z \setminus \{0\}\) auf die obigen Äquivalenzklassen, d.h. \(\Q\) surjektiv ist, erhalten wir eine surjektive Abbildung von \(\N\) nach \(\Q\).

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Georg Cantor}

\sphinxhref{https://de.wikipedia.org/wiki/Georg\_Cantor}{Georg Ferdinand Ludwig Philipp Cantor} (* 3. März 1845 in Sankt Petersburg; † 6. Januar 1918 in Halle an der Saale) war ein deutscher Mathematiker.
\end{sphinxShadowBox}

Andererseits ist die Identität von \(\N\) nach \(\Q\) eine injektive Abbildung. Also erwarten wir, dass es auch eine Bijektion gibt, d.h. \(|\Q|=\aleph_0\). Dies liefert das sogenannte erste Cantor’sche Diagonalverfahren:
\label{grundlagen/zahlensysteme:theorem-11}
\begin{sphinxadmonition}{note}{Theorem 2.2}



Es gibt eine bijektive Abbildung \(f: \N \rightarrow \Q\), d.h. \(\Q\) ist abzählbar.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Wir konstruieren eine bijektive Abbildung in zwei Schritten. Zuerst verwenden wir die bereits bekannte Bijektion \(f_1 : \N \rightarrow \Z\) und konstruieren danach eine Abbildung \(f_2: \Z \rightarrow \Q\). Danach definieren wir \(f: \N \rightarrow \Q\) als Hintereinanderausführung \(f_2 \circ f_1\).
Es genügt \(f_2\) auf \(\N \setminus \{0\}\) mit Werten in \(\Q_+\). Danach definieren wir \(f_2(0)=0\) und \(f_2(-n)=-f_2(n)\) für alle \(n \in \N\) um eine Bijektion von \(\Z\) nach \(\Q\) zu erhalten. Dazu schreiben wir alle rationalen Zahlen zeilenweise an, in die erste Zeile jene mit Zähler \(1\), in die zweite mit Zähler \(2\) usw.:
\begin{equation*}
\begin{split}{\Large 
\begin{matrix}  
\frac{1}1 & \frac{1}2 & \frac{1}3 & \frac{1}4 & \ldots  \\
& & & &\\
\frac{2}1 & \frac{2}3 & \frac{2}5 & \frac{2}7 & \ldots \\
& & & &\\
\frac{3}1 & \frac{3}2 & \frac{3}4 & \frac{3}5 & \ldots \\
& & & &\\
\frac{4}1 & \frac{4}3 & \frac{4}5 & \frac{4}7 & \ldots \\
& & & &\\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{matrix}}\end{split}
\end{equation*}
Diese zählen wir mit dem Cantor’schen Diagonalverfahren ab, dies ist analog zur Bijektion von \(\N\) nach \(\N \times \N\):
\begin{equation*}
\begin{split}{\Large 
 \begin{matrix}    
\frac{1}1 & & \frac{1}2 & \rightarrow &\frac{1}3 & &\frac{1}4 & \ldots  \\
\downarrow & \nearrow & & \swarrow  & & \nearrow & & \\
\frac{2}1 & &\frac{2}3 & &\frac{2}5 & &\frac{2}7 & \ldots \\
& \swarrow & & \nearrow&&  \swarrow & & \\
\frac{3}1 & &\frac{3}2 & &\frac{3}4 & &\frac{3}5 & \ldots \\
\downarrow & \nearrow & & \swarrow  & & \nearrow & & \\
\frac{4}1 & &\frac{4}3 & &\frac{4}5 & &\frac{4}7 & \ldots \\
\vdots & &\vdots & &\vdots & & \vdots & \ddots
\end{matrix}}\end{split}
\end{equation*}
Damit haben wir insgesamt eine Bijektion gefunden, da wir jede Zahl genau einmal nach endlich vielen Schritten erreichen. Also ist \(\Q\) abzählbar
\end{sphinxadmonition}


\subsection{Reelle Zahlen}
\label{\detokenize{grundlagen/zahlensysteme:reelle-zahlen}}
Wir konstruieren im Folgenden die reellen Zahlen und ihre Eigenschaften, indem wir Lücken zwischen den rationalen Zahlen füllen. Solche können wir uns als nichtperiodische Dezimalzahlen mit unendlich vielen Kommastellen vorstellen. Da wir die unendlich vielen Stellen ja nie alle hinschreiben können, haben wir hier einen natürlichen Grenzwertprozess. Wir können uns eine Folge aus Dezimalzahlen mit \(n\) Kommastellen vorstellen, wobei \(n\) größer wird. Für \(n \rightarrow \infty\) erhalten wir dann die reelle Zahl, quasi als Grenzwert. Wir beachten, dass der Unterschied zwischen zwei Versionen einer Dezimalzahl mit \(n\) bzw. \(m > n\) Stellen betragsmäßig immer kleiner als \(10^{-n+1}\) ist, d.h. auf jeden Fall gegen Null konvergiert mit \(n \rightarrow \infty\). Dies wird uns auf das Konzept der sogenannten Cauchy\sphinxhyphen{}Folgen führen, über die wir reelle Zahlen dann konstruieren. Wir werden auch sehen, dass sehr viele irrationale relle Zahlen gibt, tatsächlich gilt \(|\R| > |\Q|\).
Die notwendigkeit der Betrachtung irrationaler Zahlen \(x \in \R \setminus \Q\) sehen wir aus der Tatsache, dass wir in den rationalen Zahlen nicht immr eine Wurzel aus einer positiven Zahl ziehen können. Wir sehen dies bereits bei der Quadratwurzel aus \(2\), eine Beobachtung der Griechen, der wir hier in einem Beweis von Euklid folgen:
\label{grundlagen/zahlensysteme:theorem-12}
\begin{sphinxadmonition}{note}{Theorem 2.3}



Es gibt kein \(x \in \Q\) mit \(x^2 = 2\)
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(x = \frac{p}q\) mit \((p,q) \in \N \times \N \setminus \{0\}\) (wir können O.B.d.A. annehmen, dass \(x\) und damit \(p\) positiv ist). Wir können den Repräsentanten als teilerfremd auswählen, d.h. es gibt keine natürliche Zahl, die sowohl \(p\) als auch \(q\) teilt. Aus \(x^2 =2\) folgt \(p^2 = 2 q^2\) und dies bedeutet, dass \(p^2\) gerade ist. Damit muss natürlich auch \(p\) gerade sein, da das Quadrat einer ungeraden Zahl ungerade ist. Also existiert ein \(\tilde p \in \N\) mit \(p = 2 \tilde p^2\). Setzen wir dies in die obige Identität ein, so folgt
\begin{equation*}
\begin{split} 4 \tilde p^2 = 2 q^2 \Rightarrow q^2 = 2 \tilde p^2.\end{split}
\end{equation*}
Damit können wir mit den gleichen Argumenten wie vorher folgern, dass \(q\) gerade ist. Also ist \(2\) ein gemeinsamer Teiler von \(p\) und \(q\), was der Teilerfremdheit widerspricht. Damit kann kein solches \(x \in \Q\) existieren.

Die Griechen hatten auch bereits ein iteratives Verfahren zur Annäherung der Quadratwurzel, das sogenannte Heron\sphinxhyphen{}Verfahren, in dem man die Folgenglieder nacheinander definiert als
\begin{equation*}
\begin{split} x_{n+1}=\frac{x_n}2 + \frac{1}x_n,\end{split}
\end{equation*}
mit einem beliebigen Startwert \(x_0 \in \Q_+\), z.B. \(x_0=1\) oder \(x_0=2\). Nehmen wir an es gilt \(x_n \rightarrow \overline{x}\), dann gilt natürlich auch \( x_{n+1} \rightarrow \overline{x}\) für \(n\rightarrow \infty\) und damit muss gelten
\begin{equation*}
\begin{split} \overline{x}=\frac{\overline{x}}2 + \frac{1}{\overline{x}} \Leftrightarrow \overline{x}^2=2.\end{split}
\end{equation*}
Die tatsächliche Konvergenz der Folge werden wir später nachweisen.Da bei der Definition des Heron\sphinxhyphen{}Verfahrens nur Additionen, Multiplikationen und Divisionen passieren, ist \(x_n \in \Q_+\) für alle \(n\). Damit erhalten wir tatsächlich Folgen rationaler Zahlen, die gegen \(\sqrt{2}\) konvergieren. Wir sehen aber auch die Nichteindeutigkeit, für jeden Anfangswert \(x_0\) erhalten wir eine andere Folge mit dem gleichen Grenzwert. Deshalb werden wir besonders darauf achten müssen, dass wir die reellen Zahlen unabhängig von
\textbackslash{}subsubsection\{Folgen in Körpern\}
\label{grundlagen/zahlensysteme:definition-13}
\begin{sphinxadmonition}{note}{Definition 2.14}



Sei \(\K\) ein archimedisch angeordneter Körper.
\begin{itemize}
\item {} 
Eine Folge in \(\K\) ist eine Abbildung \(x: \N \rightarrow \K\), sie wird geschrieben als \((x_n)_{n \in \N}\) bzw.
\((x_0,x_1,\ldots)\).

\item {} 
Eine Folge \((x_n)\) konvergiert gegen \(\overline{x} \in \K\), genau dann wenn

\end{itemize}
\begin{equation*}
\begin{split} \forall \epsilon \in K, 0 \prec \epsilon ~ \exists n_0 \in \N ~\forall n \geq n_0: |x_n - \overline{x}| \prec \epsilon.\end{split}
\end{equation*}
In diesem Fall nennen wir \(\overline{x}\) Grenzwert (oder Limes) von \((x_n)\) und schreiben \(x_n \rightarrow \overline{x}\) oder \(\overline{x}= \lim_{n \rightarrow \infty} x_n. \)

Insgesamt nennen wir eine Folge konvergent, wenn es irgendeinen Grenzwert \(\overline{x} \in \K\) der Folge gibt.
\end{sphinxadmonition}
\end{sphinxadmonition}

Eine Folge rationaler Zahlen konvergiert gegen \(\overline{x} \in \Q\), genau dann wenn in der normalen Ordnung gilt
\begin{equation*}
\begin{split} \forall \epsilon \in K, 0 < \epsilon ~ \exists n_0 \in \N ~\forall n \geq n_0: |x_n - \overline{x}| < \epsilon.\end{split}
\end{equation*}
Allerdings haben wir mit dieser Definition noch das Problem, dass wir einen Grenzwert in \(\Q\) brauchen, wir hätten aber gerne auch Grenzwerte in \(\R \setminus \Q\), die wir eigentlich über die Folge definieren. Also brauchen wir eine alternative Definition, die keinen Grenzwert benutzt:
\label{grundlagen/zahlensysteme:definition-14}
\begin{sphinxadmonition}{note}{Definition 2.15}



Eine Folge \((x_n) \in \K\) heißt Cauchy\sphinxhyphen{}Folge, genau dann wenn
\begin{equation*}
\begin{split} \forall \epsilon \in K, 0 \prec \epsilon ~ \exists n_0 \in \N ~\forall n,m  \geq n_0: |x_n - x_m| \prec \epsilon.\end{split}
\end{equation*}\end{sphinxadmonition}

Wir sehen sofort eine einseitige Beziehung:
\label{grundlagen/zahlensysteme:theorem-15}
\begin{sphinxadmonition}{note}{Theorem 2.4}



Eine konvergente Folge \((x_n)\) in einem archimedisch angeordneten Körper \(\K\) ist eine Cauchy\sphinxhyphen{}Folge.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Ist \((x_n)\) konvergent, dann existiert für alle \(\epsilon \in \K\), \(0 \prec \epsilon\) ein \(n_0 \in \N\), sodass für alle \(n \geq n_0\) gilt:
\begin{equation*}
\begin{split}| x_n - \overline{x} | \prec  \epsilon',\end{split}
\end{equation*}
wobei \(\overline{x}\) der Grenzwert der Folge ist und \(\epsilon'+\epsilon' \preceq \epsilon\) gilt. Aus der Dreiecksungleichung folgt für \(n,m \geq n_0\)
\begin{equation*}
\begin{split}| x_n - x_m | \preceq  | x_n - \overline{x} |  +  | x_m - \overline{x} | \prec \epsilon,\end{split}
\end{equation*}
also ist \((x_n)\) Cauchy\sphinxhyphen{}Folge.
\end{sphinxadmonition}

Der folgende Satz liefert uns einige Eigenschaften von Cauchy\sphinxhyphen{}Folgen:
\label{grundlagen/zahlensysteme:theorem-16}
\begin{sphinxadmonition}{note}{Theorem 2.5}



Sei \((x_n)\) eine Cauchy\sphinxhyphen{}Folge in einem archimedisch angeordneten Körper \(\K\). Dann ist \((x_n)\) beschränkt, d.h. es gibt ein \(C \in \K\) mit
\begin{equation*}
\begin{split}|x_n| \preceq C \qquad \forall~n \in \N.\end{split}
\end{equation*}
Ist \((y_n)\) eine weitere Cauchy\sphinxhyphen{}Folge in \(\K\), dann sind auch \((x_n+y_n)\) und \((x_n y_n)\) Cauchy\sphinxhyphen{}Folgen.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Da \((x_n)\) Cauchy\sphinxhyphen{}Folge ist, gibt es ein \(n_0 \in \N\), sodass für alle \(n,m \geq n_0\) gilt
\begin{equation*}
\begin{split} | x_n - x_m | \preceq 1.\end{split}
\end{equation*}
Damit ist insbesondere für jedes \(n \geq n_0\)
\begin{equation*}
\begin{split}|x_n| \preceq |x_{n_0}| + |x_n - x_{n_0}| \prec |x_{n_0}| +1.\end{split}
\end{equation*}
Wählen wir nun
\begin{equation*}
\begin{split}C = \max_{1 \leq j \leq n_0} |x_j| + 1,\end{split}
\end{equation*}
dann folgt für alle \(n \in \N\): \(|x_j| \prec 1\).
Seien nun \((x_n)\) und \((y_n)\) Cauchy\sphinxhyphen{}Folgen. Dann existiert zu jedem \(\epsilon \in \K\) mit \(0 \prec \epsilon\) ein \(n_0^1\), sodass
\begin{equation*}
\begin{split}|x_n - x_m| \prec \epsilon', \qquad \forall n,m \geq n_0^1\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split}|y_n - y_m| \prec \epsilon' \qquad \forall n,m \geq n_0^2\end{split}
\end{equation*}
wobei \(\epsilon'+\epsilon' \preceq \epsilon\)
gilt. Für \(n_0 = \max\{n_0^1,n_0^2\}\) gilt
\begin{equation*}
\begin{split}|(x_n+y_n) - (x_m+y_m)| \preceq |x_n - x_m| + |y_n - y_m| \prec {\epsilon} , \qquad \forall n,m \geq n_0.\end{split}
\end{equation*}
Zum Beweis der Eigenschaft für das Produkt verwenden wir
\begin{align*}
|x_n y_n| &= | x_n(y_n -y_m) + y_m (x_n - x_m) | \\&\leq |x_n| ~|y_n - y_m| + |y_m|~|x_n - x_m|.\end{align*}
Nach dem ersten Teil des Satzes sind die Cauchy\sphinxhyphen{}Folgen \((x_n)\) und \((y_n)\) beschränkt. Ist \(C\) das Maximum der Schranken und \(n_0\) hinreichend groß, dann gilt für \(n,m \geq n_0\)
\begin{equation*}
\begin{split}|x_n| \preceq C, \quad |y_n| \preceq C, \quad |x_n - x_m| \prec \frac{\epsilon'}{C}, \qquad|y_n - y_m| \prec \frac{\epsilon'}{C},\end{split}
\end{equation*}
und daraus folgt\( |x_n y_n - x_m y_m | \preceq \epsilon.~\square \)
\end{sphinxadmonition}


\subsubsection{Konstruktion reeller Zahlen}
\label{\detokenize{grundlagen/zahlensysteme:konstruktion-reeller-zahlen}}
Nun beginnen wir die reellen Zahlen zu konstruieren ohne explizit den Grenzwert anzugeben. Dazu wollen wir reelle Folgen mit rationalen Cauchy\sphinxhyphen{}Folgen identifizieren. Für die Teilmenge der rationalen Zahlen ist dies einfach, dazu können wir z.b. jedes \(q \in \Q\) mit der konstanten Folge \(x_n = q\) für alle \(n \in \N\) identifizieren. Für irrationale Zahlen können wir uns die Dezimaldarstellung vorstellen, der Einfachheit halber für \(0\leq x <1\):
\begin{equation*}
\begin{split}x = 0,a_1a_2a_3 \ldots  \quad \text{ bzw. } x=\sum_{j=1}^\infty a_j 10^{-j}\end{split}
\end{equation*}
wobei die Ziffern \(a_i \in \{0,1,\ldots,9\}\) erfüllen. Ist \(x\) nicht rational, dann ist die Darstellung weder endlich noch periodisch, d.h. wir werden es nie schaffen diese vollständig hinzuschreiben, wir können uns nur darauf beschränken, die ersten \(n\) Dezimalstellen anzugeben. Damit konstruieren wir automatisch eine Folge
\begin{equation*}
\begin{split}x_n = 0,a_1a_2a_3 \ldots  \quad \text{ bzw. } x_n=\sum_{j=1}^n a_j 10^{-j} .\end{split}
\end{equation*}
Wir sehen für \(m \geq n\), dass
\begin{equation*}
\begin{split}|x_m -x_n| = | \sum_{j=n+1}^m a_j 10^{-j}| < 10^{-n}\end{split}
\end{equation*}
gilt. Damit ist \((x_n)\) natürlich Cauchy\sphinxhyphen{}Folge in \(\Q\), denn für jedes \(\epsilon \in \Q_+\) gibt es ein \(n_0\) mit
\(10^{-n_0} < \epsilon \) und damit ist auch \(10^{-n} < \epsilon\) für \(n \geq n_0\).
Nun haben wir noch das Problem, dass wir nicht direkt reelle Zahlen mit Cauchy\sphinxhyphen{}Folgen identifizieren können, da es mehrere Folgen mit dem gleichen Grenzwert gegen kann. Dies sehen wir etwa bei \(\sqrt{2}\), wenn wir Folgen aus dem Heron\sphinxhyphen{}Verfahren mit unterschiedlichen Startwerten betrachten oder sogar für eine reelle Zahl \(q\). Dort können wir etwa \(x_n=q\) oder \(\tilde x_n = q +\frac{1}n\) betrachten, die beide gegen \(q\) konvergieren. Um diese Nichteindeutigkeit zu beheben, müssen wir wieder eine geeignete Äquivalenzrelation einführen und bezüglich dieser faktorisieren.
\label{grundlagen/zahlensysteme:definition-17}
\begin{sphinxadmonition}{note}{Definition 2.16}



Eine Folge mit Grenzwert \(\overline{x}=0\) nennen wir Nullfolge. Dazu nennen wir
\begin{equation*}
\begin{split} {\cal F} = \{ (x_n): \N \rightarrow \Q~|~(x_n) \text{ ist Cauchy-Folge.}\}\end{split}
\end{equation*}\end{sphinxadmonition}

Auf \({\cal F}\) definieren wir eine Äquivalenzrelation
\begin{equation*}
\begin{split} (x_n) \sim (y_n) : \Leftrightarrow (x_n-y_n) \text{ ist Nullfolge. }\end{split}
\end{equation*}
Die Reflexivität ist klar, da \((x_n-x_n)\) die konstante Folge null ist, die Symmetrie folgt aus \(|x_n-y_n|=|y_n-x_n|\) eingesetzt in die Konvergenzbedingung. Die Transitivität erhalten wir wieder mit der Dreiecksungleichung
\begin{equation*}
\begin{split} 0 \leq  |x_n - z_n| \leq |x_n-y_n| + |y_n - z_n|.\end{split}
\end{equation*}\label{grundlagen/zahlensysteme:definition-18}
\begin{sphinxadmonition}{note}{Definition 2.17}



Mit der obigen Definition von \({\cal F}\) und \(\sim\) definieren wir die reellen Zahlen als \(\R = {\cal F}/\sim\), d.h. als Äquivalenzklassen rationaler Cauchy\sphinxhyphen{}Folgen.
Addition und Multiplikation auf \(\R\) sind definiert durch
\begin{align*}
[(x_n)] + [(y_n)] &:= [(x_n+y_n)] \\
[(x_n)] ~ [(y_n)] &:= [(x_n~y_n)]\end{align*}\end{sphinxadmonition}

Man prüft leicht nach, dass die Addition und Multiplikation wohldefiniert sind, also unabhängig von den Repräsentatnen ausgeführt werden können. Wir wollen nun auch noch eine Ordnungsrelation auf \(\R\) einführen, die natürlich auf dem Vergleich der Folgen basieren muss. Dabei können wir Positivität aber nicht einfach durch die Bedingung \(0 < x_n\) für alle \(n \in \N\) oder für \(n \geq n_0\) fordern, da es auch Folgen wie \(\frac{1}n\) gibt, die diese Bedingung erfüllen aber gegen \(0\) konvergieren.  Um dies zu verhindern definieren wir
\begin{equation*}
\begin{split} 0 < [(x_n)]:\Leftrightarrow \exists \epsilon > 0~\forall (\tilde x_n) \in [(x_n)]~\exists n_0 \in \N ~\forall n\geq n_0: \tilde x_n > \epsilon.\end{split}
\end{equation*}
Wir beachten, dass dabei \(n_0\), aber nicht \(\epsilon\) vom Repräsentanten abhängen darf. Diese Definition erweitern wir wie üblich auf \(\leq\), indem wir schreiben
\begin{equation*}
\begin{split}0 \leq  [(x_n)]:\Leftrightarrow 0 < [(x_n)] \text{ oder } 0 = [(x_n)].\end{split}
\end{equation*}
Die Relation ist dann definiert als
\begin{equation*}
\begin{split}[(x_n)] \leq [(y_n)] : \Leftrightarrow 0 \leq [(y_n-x_n)].\end{split}
\end{equation*}
Wir weisen noch kurz die Eigenschaften einer Ordnungsrelation nach:
\begin{itemize}
\item {} 
\sphinxstyleemphasis{Reflexivität} ist trivial, da wir die Gleichheit ja explizit in der Definition berücksichtigt haben.

\item {} 
\sphinxstyleemphasis{Antisymmetrie:} Sei \([(x_n)] < [(y_n)]\), dann gilt für alle Repräsentanten \((\tilde x_n) \in [(x_n)]\) und
\((\tilde y_n) \in [(y_n)]\);

\end{itemize}
\begin{equation*}
\begin{split}\exists \epsilon > 0~\exists n_0 \in \N~\forall n \geq n_0: \tilde y_n - \tilde x_n > \epsilon.\end{split}
\end{equation*}
Damit kann nicht \(\tilde x_n - \tilde y_n > \tilde \epsilon\) gelten für \(\tilde \epsilon >0\) und \(n \geq n_0\). Da \(\tilde x_n - \tilde y_n\) keine Nullfolge ist, gilt nicht \((y_n) \leq (x_n)\).
\begin{itemize}
\item {} 
\sphinxstyleemphasis{Transitivität:} Sei \([(x_n)] \leq [(y_n)]\) und \([(y_n)] \leq [(z_n)]\). Dann unterscheiden wir zunächst \([(x_n)]=[(y_n)]\) und \([(x_n)]< [(y_n)]\). Im ersten Fall ist \([(x_n)] \leq [(z_n)]\) trivial, also betrachten wir den zweiten. Genauso können wir eine Fallunterscheidung bei \([(y_n)]\) und \([(z_n)]\) machen und müssen nur den nichttrivialen Fall \([(y_n)]<[(z_n)]\) betrachten. Hier können wir \(\epsilon_1\) und \(\epsilon_2\) finden, sodass für \(n\geq n_0^1\)

\end{itemize}
\begin{equation*}
\begin{split}\tilde y_n - \tilde x_n > \epsilon_1\end{split}
\end{equation*}
und für \(n\geq n_0^2\)
\begin{equation*}
\begin{split}\tilde z_n - \tilde y_n > \epsilon_2\end{split}
\end{equation*}
gilt. Mit \(n_0=\max\{n_0^1,n_0^2\}\) und \(\epsilon = \epsilon_1 + \epsilon_2\) erhalten wir die gewünschte Eigenschaft
\begin{equation*}
\begin{split}\tilde z_n - \tilde x_n > \epsilon, \qquad \forall n \geq n_0.\end{split}
\end{equation*}
Man prüft leicht nach, dass die Grundrechenarten auf \(\R\) mit der Ordnung verträglich sind und dass \(\R\) ein archimedisch angeordneter Körper ist. Die Anordnung von \(\R\) und die Beziehung zu den natürlichen Zahlen erlaubt es uns auch zu runden:
\label{grundlagen/zahlensysteme:lemma-19}
\begin{sphinxadmonition}{note}{Lemma 2.3}



Für alle \(x \in \R\) existiert genau ein \(m\in \Z\) mit
\begin{equation*}
\begin{split} m-1 \leq x < m .\end{split}
\end{equation*}
Analog gibt es genau ein genau \(n \in \Z\) mit \(n-1 < x \leq n\)
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Sei \(x \in \R\). Wegen der archimedischen Anordnung existiert ein \(n_1 \in \N\) mit \(x < n_1\) und \(n_2 \in \N\) mit \(-x < n_2\). Wir betrachten die nichtleeren Mengen
\begin{equation*}
\begin{split}M_1 = \{ z \in \Z~|~ x < z\}, \qquad M_2 = \{ z \in \Z~|~ z \leq x\}.\end{split}
\end{equation*}
Diese sind disjunkt und es gilt \(\Z = M_1 \cup M_2\). Ist \(z \in M_1\), dann ist auch \(z+1 \in M_1\). Damit gibt es wegen der Anordnung nur ein \(m \in M_1\) mit \(m-1 \notin M_1\). Wegen \(\Z = M_1 \cup M_2\) gilt also \(m-1 \in M_1\), damit folgt \(  m-1 \leq x < m . \square\)
\end{sphinxadmonition}

Eine interessante Eigenschaft ist auch das Wachstum der Exponentialfunktion. Dazu benötigen wir zunächst ein Hilfsresultat, die sogenannte Bernoulli\sphinxhyphen{}Ungleichung
\label{grundlagen/zahlensysteme:lemma-20}
\begin{sphinxadmonition}{note}{Lemma 2.4}



Für \(x \in \R\), \(x \geq -1\) und \(n\in \N\) gilt
\begin{equation*}
\begin{split} (1+x)^n \geq 1 + n x.\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Wir können den Beweis per vollständiger Induktion führen. Der Anfang bei \(n=0\) liefert \(1 \geq 1\), ist also wahr. Nun nehmen wir an \((1+x)^n \geq 1 + n x\) und setzen dies ein in
\begin{equation*}
\begin{split} (1+x)^{n+1} = (1+x)^n (1+x) \geq (1+nx)(1+x) = 1+ (n+1)x +x^2.
 \end{split}
\end{equation*}
Wegen der Nichtnegativität von \(x^2\) folgt die Aussage auch für \(n+1\) und damit haben wir die Ungleichung für alle \(n\in \N\) bewiesen.
\end{sphinxadmonition}
\label{grundlagen/zahlensysteme:lemma-21}
\begin{sphinxadmonition}{note}{Lemma 2.5}



Sei \(x \in \R\), \(x  > 1\) und \(m\in \N\). Dann gibt es ein \(\ell \in \N\) mit
\begin{equation*}
\begin{split}x^\ell \geq m.\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Für \(m=0\) und \(m=1\) erfüllt offensichtlich \(\ell=1\) die Ungleichung. Für \(m > 1\) schreiben wir \(x=1+y\), \(y > 0\) und setzen \(l \geq \frac{m-1}y\). Aus der Bernoulli\sphinxhyphen{}Ungleichung folgt nun:
\begin{equation*}
\begin{split}x^\ell =(1+y)^\ell \geq 1+ \ell y \geq 1+  \frac{m-1}y y = m,\end{split}
\end{equation*}
also gilt die Aussage
\end{sphinxadmonition}


\subsubsection{Vollständigkeit}
\label{\detokenize{grundlagen/zahlensysteme:vollstandigkeit}}
Die reellen Zahlen \(\R\) sind ein Körper, also können wir wieder Cauchy\sphinxhyphen{}Folgen in \(\R\) betrachten. Nun stellt sich die Frage, ob \(\R\) vollständig ist, d.h. ob alle Cauchy\sphinxhyphen{}Folgen konvergieren. Dazu stellen wir zunächst Zusammenhänge zwischen den reellen und den ganzen Zahlen her. Zunächst sehen wir, dass Cauchy\sphinxhyphen{}Folgen in \(\Q\) auch Cauchy\sphinxhyphen{}Folgen in \(\R\) sind, indem wir \(\epsilon \in \R_+\) durch \(\epsilon' \in \Q\) ersetzen können. Ist \(\epsilon \in \R\), \(\epsilon \geq 1\), so können wir \(n,m\) so wählen, dass
\begin{equation*}
\begin{split}|x_n -x_m| < 1 \leq \epsilon\end{split}
\end{equation*}
gilt. Ist \(\epsilon < 1\), dann können wir \(k \in \N\) wählen, indem wir \(\frac{1}\epsilon\) aufrunden und damit ist
\(\frac{1}k \leq \epsilon\). Nun können wir \(n,m\) so wählen, dass
\begin{equation*}
\begin{split}|x_n -x_m| < \frac{1}k \leq \epsilon.\end{split}
\end{equation*}
Um nich immer explizit die Eigenschaft einer Cauchy\sphinxhyphen{}Folge nachweisen zu müssen, verwenden wir folgendes einfache Resultat:
\label{grundlagen/zahlensysteme:lemma-22}
\begin{sphinxadmonition}{note}{Lemma 2.6}



Sei \((y_n)\) eine Nullfolge in \(\R\) und für ein \(C \in \R_+\) gelte
\begin{equation*}
\begin{split}\forall m,n \in \N, m \geq n: |x_m-x_n| \leq C y_n.\end{split}
\end{equation*}
Dann ist \((x_n)\) Cauchy\sphinxhyphen{}Folge
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Da \((y_n)\) eine Nullfolge ist, existiert für jedes \(\epsilon > 0\) ein \(n_0 \in N\), sodass für alle \(n \geq n_0\) gilt \(y_n \leq |y_n| < \frac{\epsilon}C\). Damit folgt sofort für \(m \geq n \geq n_0\)
\begin{equation*}
\begin{split}|x_m-x_n| \leq C y_n < \epsilon,\end{split}
\end{equation*}
d.h. \((x_n)\) ist Cauchy\sphinxhyphen{}Folge
\end{sphinxadmonition}
\label{grundlagen/zahlensysteme:lemma-23}
\begin{sphinxadmonition}{note}{Lemma 2.7}



Für jedes \(x \in \R\) und \(\epsilon \in \R_+\) existiert ein ein \(q \in \Q\) mit \(|x-q|<\epsilon\)
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(k\) wie oben mit \(\frac{1}k \leq \epsilon\). Da \(x\) der Grenzwert einer Cauchy\sphinxhyphen{}Folge \((r_n)\) in \(\Q\) ist, gibt es ein \(n_0 \in \N\) mit
\begin{equation*}
\begin{split}|r_n - r_m| < \frac{1}{2k} \leq \frac{\epsilon}2,\end{split}
\end{equation*}
für alle \(m,n \geq n_0\). Wählen wir nun \(q=r_{n_0}\) und betrachten die konstante Folge \((q)\), dann gilt
\begin{equation*}
\begin{split}|r_n - q| <  \frac{\epsilon}2,\end{split}
\end{equation*}
für alle \(n \geq n_0\). Sei nun \((\tilde r_n)\) ein anderer Repräsentant von \(x\), dann ist \((r_n - \tilde r_n)\) Nullfolge, d.h. es gibt ein \(\tilde n_0\) mit
\begin{equation*}
\begin{split}|r_n - \tilde r_n|   < \frac{1}{2k} \leq \frac{\epsilon}2,\end{split}
\end{equation*}
für \(n \geq \tilde n_0\). Damit gilt für \(n\geq \max\{n_0,\tilde n_0\}\) auch
\begin{equation*}
\begin{split}|q-\tilde r_n| \leq |q -r_n|+|r_n - \tilde r_n| < \epsilon,\end{split}
\end{equation*}
d.h. \(|q-x| < \epsilon\).
\end{sphinxadmonition}

Damit können wir die Vollständigkeit von \(\R\) nachweisen.
\label{grundlagen/zahlensysteme:theorem-24}
\begin{sphinxadmonition}{note}{Theorem 2.6}



\(\R\) ist vollständig, d.h. jede Cauchy\sphinxhyphen{}Folge \((x_n)\) in \(\R\) ist konvergent
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Zu jedem Folgenelement \(x_n \in \R\) gibt es ein \(q_n \in \Q\) mit
\begin{equation*}
\begin{split}|x_n-q_n| < \frac{1}n.\end{split}
\end{equation*}
Sei nun \(\epsilon > 0\), dann gibt es ein \(n_0\), sodass für alle \(n,m \geq n_0\) gilt
\begin{equation*}
\begin{split}|x_n - x_m| < \frac{\epsilon}3.\end{split}
\end{equation*}
Sei darüber hinaus \(n_0\) so groß gewählt, dass \(\frac{1}{n_0} < \frac{\epsilon}3\) gilt. Dann folgt
\begin{equation*}
\begin{split}|q_n - q_m| \leq |q_n - x_n| + |x_n - x_m| + |x_m-q_m| <\epsilon,\end{split}
\end{equation*}
d.h. \((q_n)\) ist Cauchy\sphinxhyphen{}Folge in \(\Q\) und da \((x_n -q_n)\) eine Nullfolge ist, folgt
\begin{equation*}
\begin{split} x=\lim_n q_n = \lim_n x_n\end{split}
\end{equation*}
und damit ist \(x_n\) konvergent.
\end{sphinxadmonition}

Mit Hilfe der Vollständigkeit können wir nun Grenzwerte in \(\R\) definieren, indem wir nachweisen, dass Folgen Cauchy\sphinxhyphen{}Folgen sind. Als Anwendung betrachten wir die Exponentialfunktion:
\label{grundlagen/zahlensysteme:example-25}
\begin{sphinxadmonition}{note}{Example 2.10}



Sei \(x \in \R\). Wir definieren
\begin{equation*}
\begin{split} e^x = \sum_{k=0}^\infty \frac{x^k}{k!} = \lim_{n \rightarrow \infty} \sum_{k=0}^n \frac{x^k}{k!},\end{split}
\end{equation*}
wobei \(k! = \prod_{i=1}^k i\). Sei
\begin{equation*}
\begin{split}y_n = \sum_{k=0}^n \frac{x^k}{k!}\end{split}
\end{equation*}
und wählen wir \(m \geq n\). Dann ist
\begin{equation*}
\begin{split}|y_n - y_m| \leq \sum_{k=n+1}^m \frac{|x|^k}{k!}.\end{split}
\end{equation*}
Sei \(n_0 \in \N\) so, dass \(n_0 > 2{|x|} \), dann gilt
\begin{align*}
\sum_{k=n+1}^m \frac{|x|^k}{k!} &= \frac{|x|^{n_0}}{n_0!} \sum_{k=n+1}^m \frac{|x|^{k-n_0}}{(n_0+1)\ldots k} \\
&\leq \frac{|x|^{n_0}}{n_0!} \sum_{k=n+1}^m \left(\frac{1}2\right)^{k-n_0} \\
&= \frac{|2x|^{n_0}}{n_0!} \sum_{k=n+1}^m \left(\frac{1}2\right)^{k} \\
&= \frac{|2x|^{n_0}}{n_0!} \left(\frac{1}2\right)^{n+1} \sum_{k=0}^{m-n-1} \left(\frac{1}2\right)^{k} \\
&= \frac{|2x|^{n_0}}{n_0!} \left(\frac{1}2\right)^{n+1} \frac{1-\left(\frac{1}2\right)^{m-n-1}}{\frac{1}2} \\
&\leq \frac{|2x|^{n_0}}{n_0!} \left(\frac{1}2\right)^{n},
\end{align*}
Da der letzte Term gegen Null konvergiert, können wir \(m,n\) so groß wählen, dass er beliebig klein wird. Also ist \(y_n\) eine Cauchy\sphinxhyphen{}Folge.
\end{sphinxadmonition}

Wir betrachten nun noch die Mächtigkeit von \(\R\):
\label{grundlagen/zahlensysteme:theorem-26}
\begin{sphinxadmonition}{note}{Theorem 2.7}



Es gilt \(|\R| > |\N|\), d.h. \(\R\) ist nicht abzählbar
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.   Es genügt zu zeigen, dass
\begin{equation*}
\begin{split}I=[0,1) = \{ x \in \R~|~0 \leq x < 1\}\end{split}
\end{equation*}
nicht abzählbar ist. Wir können\(x \in I\) in Dezimaldarstellung \(0,a_1a_2 \ldots\) schreiben, indem wir iterativ abrunden:
\begin{equation*}
\begin{split}a_1 = \lfloor 10 x \rfloor, \quad a_{i+1} = \lfloor 10^{i+1}(x-\sum_{j=0}^i a_j 10^{-j})\rfloor.\end{split}
\end{equation*}
Nehmen wir an, es gäbe eine surjektive Abbildung \(f: \N \rightarrow [0,1)\). Dann schreiben wir die Dezimaldarstellung für den Wertebereich als
\begin{equation*}
\begin{split}f(\ell) = 0,a_1^\ell a_2^\ell \ldots\end{split}
\end{equation*}
mit \(a_i^\ell \in \{0,1,\ldots,9\}\). Nun definieren wir
\begin{equation*}
\begin{split}x=0,b_1 b_2 \ldots\end{split}
\end{equation*}
mit \(b_i \in \{1,\ldots,8\}\) so, dass \(|b_i - n_i^i| \geq 2\). Damit folgt \(|x-f(\ell)|\geq 10^{-\ell}\), also insbesondere \(x \neq f(\ell)\) für alle \(\ell \in \N\). Damit kann \(f\) nicht surjektiv sein und \(\R\) nicht abzählbar
\end{sphinxadmonition}

Wir sehen also, dass die Mächtigkeit von \(\R\) eine größere Art von Unendlich ist als die Mächtigkeit von \(\N\), dies gilt auch für die irrationalen Zahlen im Vergleich zu den rationalen. Wir definieren diese deshalb als zweite Kardinalzahl \(|\R|= \aleph_1\).


\subsection{Infima und Suprema}
\label{\detokenize{grundlagen/zahlensysteme:infima-und-suprema}}
Intuitiv würden wir gerne von einem größten und kleinsten Element einer Menge (Maximum bzw. Minimum) sprechen, wenn wir eine vollständige Ordnung haben. Dies ist aber nicht immer gegeben, wie eine Betrachtung verschiedener Intervalle in \(\R\) zeigt. Für \(a,b \in \R\) mit \(a < b\) definieren wir
\begin{align*}
[a,b] &:= \{x \in \R~|~a \leq x \leq b\} \\
(a,b) &:= \{x \in \R~|~a < x < b\} \\
[a,b) &:= \{x \in \R~|~a \leq x < b\} \\
(a,b] &:= \{x \in \R~|~a < x \leq b\}.
\end{align*}
Wir nennen \([a,b]\) geschlossenes Intervall, \((a,b)\) offenes Intervall und die anderen beiden halboffene Intervalle. Das geschlossene Intervall hat ein Maximum (\(b\)) und ein Minimum (\(a\)), das offene Intervall aber nicht, denn einerseits existiert für jedes \(\epsilon > 0\) ein \(x \in \R\) mit \(x>b-\epsilon\), also müsste ein Maximum größer gleich \(b\) sein. Alle reellen Zahlen größer gleich \(b\) liegen aber nicht in \((a,b)\). In einem solchen Fall werden wir \(b\) ein Supremum nennen (und analog \(a\) ein Infimum), dies führt auf die folgende Definition:
\label{grundlagen/zahlensysteme:definition-27}
\begin{sphinxadmonition}{note}{Definition 2.18}



Sei \(K\) eine Menge mit Ordnungsrelation \(\preceq\) und \(M \subset K\).
\begin{itemize}
\item {} 
\(z \in K\) heisst obere Schranke für \(M\), wenn für alle \(x \in M\) gilt: \(x \preceq z\).

\item {} 
\(z \in K\) heisst untere Schranke für \(M\), wenn für alle \(x \in M\) gilt: \(z \preceq x\).

\item {} 
\(M\) heisst nach oben / unten beschränkt, wenn eine obere / untere Schranke existiert. \(M\) heisst beschränkt, wenn sie nach oben und unten beschränkt ist.* Eine obere Schranke \(s \in K\) heisst Supremum, wenn für alle anderen oberen Schranken \(z\) gilt \(s \preceq z\).

\item {} 
Eine untere Schranke \(i \in K\) heisst Infimum, wenn für alle anderen unteren Schranken \(z\) gilt \(z \preceq i\).

\end{itemize}
\end{sphinxadmonition}

Wir wenden dies nun auf die reellen Zahlen an:
\label{grundlagen/zahlensysteme:theorem-28}
\begin{sphinxadmonition}{note}{Theorem 2.8}



\(M \subset \R\) ist beschränkt genau dann wenn es ein \(g \in \R_+\) gibt, sodass für alle \(x \in M\) gilt: \(|x|\leq g\).Ist \(M \neq \emptyset\) und \(M\) nach oben und unten beschränkt, dann besitzt \(M\) ein Supremum und ein Infimum. Diese sind jeweils eindeutig.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Zunächst sehen wir, dass für beschränkte Mengen in \(\R\) Schranken \(a,b \in \R\) existieren, mit \(a \leq x  \leq b\) für alle \(x \in M\). Nun wählen wir \(g=\max\{|a|,|b|\}\) und erhalten \(|x|\leq g\). Umgekehrt können wir im Fall \(|x|\leq g\) für alle \(x \in M\) einfach \(g\) als obere und \(-g\) als untere Schranke wählen.

Um ein Supremum zu konstruieren beginnen wir mit \(m_0 \in M\) und einer oberen Schranke \(s_0 \in \R\). Nun definieren wir iterativ zunächst \(x_i=\frac{m_i+s_i}2\). Ist \(x_i\) obere Schranke, dann setzen wir \(s_{i+1}=x_i\) und \(m_{i+1}=m_i\). Andernfalls setzen wir \(m_{i+1}=x_i, s_{i+1}=s_i\). In beiden Fällen ist \(M \cap [m_{i+1},s_{i+1}] \neq \emptyset\). Wir sehen auch
\begin{equation*}
\begin{split}|s_{i+1} - m_{i+1}| = \frac{1}2 |s_i - m_i|\end{split}
\end{equation*}
und daraus folgt
\begin{equation*}
\begin{split}|s_n - m_n| = \frac{1}{2^n} |s_0-m_0|,\end{split}
\end{equation*}
also ist \((s_n-m_n)\) ein Nullfolge. Darüber hinaus sehen wir
\begin{equation*}
\begin{split} |s_{i+1}-s_i| \leq \frac{1}2 |s_i - m_i|  = \frac{1}{2^{i+1}} |s_0-m_0|\end{split}
\end{equation*}
und erhalten für \(m\geq n\)
\begin{equation*}
\begin{split} |s_m - s_n| = |\sum_{i=n}^{m-1} s_{i+1}-s_i| \leq \sum_{i=n}^{m-1} |s_{i+1}-s_i| \leq  \sum_{i=n}^{m-1}\frac{1}{2^{i+1}} |s_0-m_0| = \frac{1}{2^{n+1}}  |s_0-m_0| \sum_{i=0}^{m-n-1}\frac{1}{2^{i}} \leq \frac{1}{2^{n}}  |s_0-m_0|.\end{split}
\end{equation*}
Also ist \((s_n)\) eine Cauchy\sphinxhyphen{}Folge und analog zeigen wir, dass auch \((m_n)\) eine Cauchy\sphinxhyphen{}Folge ist. Da jede Cauchy\sphinxhyphen{}Folge in \(\R\) konvergiert gibt es einen Grenzwert \(s\), der für beide Folgen gleich ist. Aus der Konstruktion sehen, wir dass \(s_n\) für jedes \(n\) eine obere Schranke ist. Nehmen wir an, dass es ein \(m \in M\) gibt mit \(m > s\), dann ist \(m=s+\epsilon\) für ein \(\epsilon > 0\). Andererseits existiert \(n_0\) mit \(|s-s_{n_0}|< \epsilon\) und damit\(s_{n_0} < s + \epsilon = m\). Dies ist ein Widerspruch dazu, dass \(s_{n_0}\) eine obere Schranke ist, also \(m \leq s_{n_0}\). Damit ist \(s\) eine obere Schranke und es kann keine kleinere obere Schranke geben, da es für jedes \(\epsilon > 0\) ein \(m \in M\) mit \(m > s-\epsilon\) gibt. Also ist \(s\) ein Supremum. Ist \(s'\) ein weiteres Supremum, dann gilt \(s \leq s'\) und \(s' \leq s\), also \(s=s'\). Die Existenz und Eindeutigkeit eines Infimums folgt analog
\end{sphinxadmonition}

Für unbeschränkte Mengen gibt es nicht unbedingt ein Infimum oder Supremum. Wir können dies erzwingen indem wir \(\R\) erweitern zu
\begin{equation*}
\begin{split} \overline{\R} = \R \cup \{+\infty,-\infty\},\end{split}
\end{equation*}
wobei wir \(\pm \infty\) so definieren, dass
\begin{equation*}
\begin{split} \forall x\in \R \cup \{+\infty\} : - \infty < x\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} \forall x\in \R \cup \{+\infty\} : x < \infty \end{split}
\end{equation*}
gilt.


\subsection{Komplexe Zahlen}
\label{\detokenize{grundlagen/zahlensysteme:komplexe-zahlen}}
In \(\R\) können wir die Quadratwurzel einer positiven Zahl ziehen, nicht aber aus einer negativen. Dies bedeutet, dass nicht jede quadratische Gleichung eine Lösung hat, z.B.
\begin{equation*}
\begin{split} x^2 +1 = 0\end{split}
\end{equation*}
ist nicht in \(\R\) lösbar. Allgemeiner gibt es viele algebraische Gleichungen, d.h. Gleichungen der Form
\begin{equation*}
\begin{split} \sum_{j=0}^n a_j x^j = 0,\end{split}
\end{equation*}
die nicht lösbar sind oder äquivalent hat nicht jedes nichtkonstante Polynom eine Nullstelle in \(\R\). Um dies zu beheben führen wir eine imaginäre Zahl \(\i\) ein, sodass
\begin{equation*}
\begin{split} \i^2 = (-\i)^2 = -1\end{split}
\end{equation*}
gilt. Dann können wir auch reelle Zahlen zu \(\i\) addieren oder reelle Vielfache von \(\i\) betrachten, z.B. sollte gelten \((2\i)^2=-4\). Also erhalten wir komplexe Zahlen in der Form
\begin{equation*}
\begin{split} z = a + b \i, \qquad a, b \in \R.\end{split}
\end{equation*}
Wir nennen \(a\) den Realteil und \(b\) den Imaginärteil, sehen also, dass im wesentlichen die komplexen Zahlen dem \(\R^2 = \R \times \R\) entsprechen.
Für die Addition und Multiplikation verwenden wir Assoziativität, Distributivität und Kommutativität, damit ist
\begin{equation*}
\begin{split} z_1+z_2 = (a_1+a_2) + (b_1+b_2)\i,\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} z_1 ~z_2 = a_1 a_2 + a_1 b_2 \i + a_2 b_1 \i + b_1 b_2 \i^2 = (a_1 a_2 - b_1 b_2) + ( a_1 b_2   + a_2 b_1) \i .\end{split}
\end{equation*}
Dies führt auf die Definition der komplexen Zahlen als \(\C = \R \times \R\) mit den Rechenregeln
\begin{equation*}
\begin{split} (a_1,b_1) +(a_2,b_2) := (a_1+a_2,b_1+b_2)\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} (a_1,b_1) ~(a_2,b_2) := (a_1 a_2 - b_1 b_2,a_1 b_2   + a_2 b_1).\end{split}
\end{equation*}
Damit erhalten wir:
\label{grundlagen/zahlensysteme:theorem-29}
\begin{sphinxadmonition}{note}{Theorem 2.9}



Die komplexen Zahlen \(\C\) sind ein Körper
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Zunächst sehen wir sofort, dass \((\C,+)\) eine abel’sche Gruppe mit neutralem Element \((0,0)\) und zu \((a,b)\) inversem Element \((-a,-b)\) bildet. Darüber hinaus sehen wir, das \((\C,\cdot)\) eine abel’sche Gruppe mit Einselement \((1,0)\) und zu \((a,b)\) inversem Element \((\frac{a}{a^2+b^2},  \frac{-b}{a^2+b^2}) \) bildet. Das Assoziativ\sphinxhyphen{} und Distributivgesetz kann man direkt mit den entsprechenden Gesetzen für die Multiplikation in \(\R\) nachrechnen
\end{sphinxadmonition}

Mit der Abbildung \(x \in \R \mapsto (x,0) \in \C\) haben wir eine kanonische Einbettung der reellen Zahlen in \(\C\). Wir schreiben dann allgemein wieder \(z=a+b\i\) und sagen \(z \in\R\) wenn der Imaginärteil \(b=0\) ist.  Die imaginäre Zahl \(\i\) können wir also mit \((0,1)\) identifizieren.  Wir schreiben auch \(a=\)Re\((z)\) und \(b=\)Im\((z)\). Wir definieren den Betrag einer komplexen Zahl als
\begin{equation*}
\begin{split} |z| := \sqrt{a^2+b^2}\end{split}
\end{equation*}
und die konjugierte Zahl als
\begin{equation*}
\begin{split} \overline{z} := a - b \i.\end{split}
\end{equation*}
Als Beispiel betrachen wir \(z= 4+3\i\), dann ist \(|z|=5\) und \(\overline{z}=4-3\i\).

Den folgenden Satz geben wir ohne seinen (einfachen) Beweis an:
\label{grundlagen/zahlensysteme:theorem-30}
\begin{sphinxadmonition}{note}{Theorem 2.10}



Sei \(z \in \C \), dann gilt:
\begin{equation*}
\begin{split}  |z| \geq 0\quad \text{ und } |z|=\sqrt{z \overline{z}},\end{split}
\end{equation*}
darüber hinaus ist \(|z|=0\) genau dann wenn \(z=0\) gilt.Sind \(x,y \in \C\), dann gilt
\begin{equation*}
\begin{split} |xy|=|x|~|y|,\qquad |x+y| \leq |x|+|y| .\end{split}
\end{equation*}\end{sphinxadmonition}

Ist \(y \in \C\) und \(n\in \N\setminus\{0,1\}\), dann heisst \(x \in \C\) die \(n\)\sphinxhyphen{}te Wurzel von \(y\), wenn \(y=x^n\) gilt.Eine Wurzel ist eine Nullstelle des Polynoms \(p:\C \rightarrow \C, x \mapsto x^n-y\). Eine solche Wurzel existiert immer in \(\C\), allgemeiner gilt der \sphinxstyleemphasis{Hauptsatz der Algebra} (hier ohne Beweis):
\label{grundlagen/zahlensysteme:theorem-31}
\begin{sphinxadmonition}{note}{Theorem 2.11}



Jedes Polynom```\{math\}
p: \textbackslash{}C \textbackslash{}mapsto \textbackslash{}C, x \textbackslash{}mapsto \textbackslash{}sum\_\{j=0\}\textasciicircum{}n a\_j x\textasciicircum{}j

\begin{sphinxVerbatim}[commandchars=\\\{\}]

mit \PYGZdl{}n \PYGZbs{}in \PYGZbs{}N\PYGZbs{}setminus\PYGZbs{}\PYGZob{}0\PYGZbs{}\PYGZcb{}\PYGZdl{}, \PYGZdl{}a\PYGZus{}j \PYGZbs{}in \PYGZbs{}C\PYGZdl{}, \PYGZdl{}a\PYGZus{}n \PYGZbs{}neq 0\PYGZdl{} hat eine komplexe Nullstelle, d.h. es existiert ein \PYGZdl{}\PYGZbs{}overline\PYGZob{}x\PYGZcb{} \PYGZbs{}in \PYGZbs{}C\PYGZdl{} mit \PYGZdl{}p(\PYGZbs{}overline\PYGZob{}x\PYGZcb{})=0\PYGZdl{}
\end{sphinxVerbatim}
\end{sphinxadmonition}
\label{grundlagen/zahlensysteme:example-32}
\begin{sphinxadmonition}{note}{Example 2.11}



Das quadratische Polynom \(p(x) =x^2+1\) hat die Nullstellen \(\i\) und \(-\i\)
\end{sphinxadmonition}
\label{grundlagen/zahlensysteme:example-33}
\begin{sphinxadmonition}{note}{Example 2.12}



Das kubische Polynom \(p(x) =x^3+1\) hat die Nullstellen \(-1\), \(\frac{-1+\sqrt{3}\i}2\) und \(\frac{-1-\sqrt{3}\i}2\)
\end{sphinxadmonition}

In den Beispielen sehen wir, dass zu jeder Nullstelle auch die konjugierte Zahl Nullstelle ist. Dies gilt allgemein für komplexe Polynome: ist \(z \in \C\) Nullstelle, dann ist auch \(\overline{z}\) eine Nullstelle.


\chapter{Vektorräume und lineare Abbildungen}
\label{\detokenize{vektorraeume/vektorraeume:vektorraume-und-lineare-abbildungen}}\label{\detokenize{vektorraeume/vektorraeume::doc}}
Die komplexen Zahlen aus dem letzten Abschnitt sind ein einfaches Beispiel eines Vektorraums, nun wollen wir Vektorräume etwas genauer betrachten, die allgemein folgendermaßen definiert sind:
\label{vektorraeume/vektorraeume:definition-0}
\begin{sphinxadmonition}{note}{Definition 3.1}



Sei \(V\) eine Menge und \(\K\) ein Körper. Gegeben seien die Addition \(\oplus: V \times V \rightarrow V\) und die Skalarmultiplikation \(\odot: \K \times V \rightarrow V\). \((V,\oplus,\odot)\) heisst Vektorraum, wenn für alle \(u,v \in V\) und alle \(\alpha, \beta \in \K\) die folgenden Eigenschaften gelten:
\begin{itemize}
\item {} 
\((V,\oplus)\) ist eine abel’sche Gruppe.

\item {} 
\(\alpha \odot (u \oplus v) = \alpha \odot(v \oplus u) = (\alpha \odot u) \oplus (\alpha \odot v)\)

\item {} 
\((\alpha + \beta) \odot v = (\alpha \odot v) \oplus (\beta \odot v)\)

\item {} 
\((\alpha \beta)\odot v = \alpha \odot (\beta \odot v)\)

\item {} 
\(1 \odot v = v\), wobei \(1\) das Einselement der Multiplikation in \(\K\) ist.

\end{itemize}
\end{sphinxadmonition}

Wir werden im Folgenden einfach \(u +v\) statt \(u \oplus v\) und \(\alpha v\) statt \(\alpha \odot v\) schreiben.
\label{vektorraeume/vektorraeume:example-1}
\begin{sphinxadmonition}{note}{Example 3.1}



\(\R^n\) ist ein Vektorraum über \(\R\), allgemein ist \(\K^n\) ein Vektorraum über \(\K\).
\end{sphinxadmonition}
\label{vektorraeume/vektorraeume:example-2}
\begin{sphinxadmonition}{note}{Example 3.2}



\(\R^\N =  \{(x_n):\N \rightarrow \R\}\) ist Vektorraum über \(\R\).
\end{sphinxadmonition}
\label{vektorraeume/vektorraeume:example-3}
\begin{sphinxadmonition}{note}{Example 3.3}



\(\ell^\infty(\N) = \{(x_n) \in \R^\N~|~\sup_n |x_n| < \infty\}\) ist ein Vektorraum über \(\R\)
\end{sphinxadmonition}
\label{vektorraeume/vektorraeume:example-4}
\begin{sphinxadmonition}{note}{Example 3.4}



\(\C = \{ a + b \i ~|~ a,b \in \R\}\) ist ein Vektorraum über \(\R\)
\end{sphinxadmonition}

Im Fall der komplexen Zahlen haben wir den Vektorraum über eine \sphinxstyleemphasis{\textbackslash{}2} (also Vielfache und Summen) gewisser Elemente, nämlich \(1\) und \(\i\), definiert. Dies ist auch in den anderen Vektorräumen der Fall, z.B. können wir in \(\R^n\) alles als Linearkombination der Einheitsvektoren
\begin{equation*}
\begin{split} e_i = (\delta_{ij})_{j=1}^n\end{split}
\end{equation*}
schreiben, wobei \(\delta_{ij}\) das Kronecker\sphinxhyphen{}Delta bezeichnet, definiert als \(\delta_{ii}=1\) und \(\delta_{ij}=0\) sonst. Diese Elemente sind aber nicht eindeutig, z.B. können wir im \(\R^2\) auch alles als Linearkombination von \((1,1)\) und \((1,-1)\) schreiben, es gilt
\begin{equation*}
\begin{split} (a,b) = \lambda_1 (1,1) + \lambda_2 (1,-1)\end{split}
\end{equation*}
mit \(\lambda_1 = \frac{a+b}2\), \(\lambda_2 = \frac{a-b}2\).
\label{vektorraeume/vektorraeume:definition-5}
\begin{sphinxadmonition}{note}{Definition 3.2}



Seien \(v_1,\ldots,v_n \in V\).
\begin{itemize}
\item {} 
Seien \(\lambda_1,\ldots,\lambda_b \in \K\), dann heisst

\end{itemize}
\begin{equation*}
\begin{split} v = \sum_{i=1}^n \lambda_i v_i\end{split}
\end{equation*}
Linearkombination der \(v_i\).
\begin{itemize}
\item {} 
Die Elemente \(v_1,\ldots,v_n\) ( bzw. \(\{v_1,\ldots,v_n\}\)) die Menge heissen linear unabhängig, wenn aus\(\sum_{i=1}^n \lambda_i v_i =0\) folgt, dass \(\lambda_1 = \lambda_2 \ldots = \lambda_n = 0\).

\item {} 
Eine Menge \(W \subset V\) heisst linear unabhängig für alle \(n \in \N \setminus \{0\}\), \(n \leq |W|\) jede Teilmenge \(\{v_1,\ldots,v_n\}\) linear unabhängig ist. Andernfalls nennen wir \(W\) linear abhängig.

\item {} 
Sei \(W \subset V\), dann ist die lineare Hülle

\end{itemize}
\begin{equation*}
\begin{split} \text{lin}(W) = \{ \sum_{i=1}^n \lambda_i v_i~|~ n \in \N, \lambda_i \in \K, v_i \in W\}\end{split}
\end{equation*}
gegeben als die Menge aller Linearkombinationen von Elementen aus \(W\).
\begin{itemize}
\item {} 
\(W\) heisst Erzeugendensystem, wenn lin\((W)=V\).

\item {} 
\(B \subset V\) heisst Basis von \(V\), wenn \(B\) Erzeugendensystem und linear unabhängig ist.

\item {} 
Ist \(B\) Basis von \(V\) und \(n=|B| < \infty\), dann heisst \(V\) endlichdimensionaler Vektorraum und \(n\) die Dimension von \(V\).

\item {} 
Ist \(W \subset V\) selbst ein Vektorraum, d.h. abgeschlossen bezüglich Addition und Skalarmultiplikation, dann nennen wir \(W\) Unterraum von \(V\).

\end{itemize}
\end{sphinxadmonition}
\label{vektorraeume/vektorraeume:example-6}
\begin{sphinxadmonition}{note}{Example 3.5}



Wir betrachten wieder den \(\R^2\). \(B=\{(1,0),(0,1)\}\) ist eine Basis, \(W= \{(1,0),(0,1),(1,1)\}\) ist ein Erzeugendensystem, aber nicht linear unabhängig, da
\begin{equation*}
\begin{split} (1,0) + (0,1) - (1,1) = 0.\end{split}
\end{equation*}\end{sphinxadmonition}
\label{vektorraeume/vektorraeume:example-7}
\begin{sphinxadmonition}{note}{Example 3.6}



Für \(V=\R^3\) ist \(W=\{(1,0,0),(0,1,0)\}\)  linear unabhängig, aber kein Erzeugendensystem, da wir kein \(\lambda_1,\lambda_2 \in \R\) finden können mit
\begin{equation*}
\begin{split}(0,0,1) =  \lambda_1 (1,0,0)+ \lambda_2 (0,1,0) = (\lambda_1,\lambda_2,0).\end{split}
\end{equation*}
Tatsächlich entspricht lin\((V)\) dem \(\R^2\) mit einer zusätzlichen Null in der dritten Variable.
\end{sphinxadmonition}
\label{vektorraeume/vektorraeume:example-8}
\begin{sphinxadmonition}{note}{Example 3.7}



Sei \(V=\R^N\), \(W=\{(\delta_{in})_n ~|~i \in \N\}\). Dann ist \(W\) linear unabhängig, aber keine Basis. Es gilt
\begin{equation*}
\begin{split} \text{lin}(W) = \{ (x_n) \in \R^\N~|~ \exists n_0 \in \N \forall n \geq n_0: x_n =0 \},\end{split}
\end{equation*}
die lineare Hülle entspricht also den Folgen, die nach endlich vielen Indizes null werden.
\end{sphinxadmonition}

Wir machen einige Beobachtungen:
\begin{itemize}
\item {} 
Ist \(W\) linear unabhängig, so ist natürlich auch jede nichtleere Teilmenge von \(W\) linear unabhängig. Ist \(W\) ein Erzeugendensystem, dann ist auch jedes \(W'\) mit \(W \subset W'\) Erzeugendensystem.

\item {} 
Eine Menge, die \(v=0\) enthält ist nie linear unabhängig, da \(\lambda   v  = 0\) für alle \(\lambda \in \K\).

\end{itemize}

Linearkombinationen führen in natürlicher Weise auf lineare Gleichungssysteme für die Koeffizienten \(\lambda_i\), dies sehen wir noch mal am Beispiel
\begin{equation*}
\begin{split} (a,b) = \lambda_1 (1,1) + \lambda_2 (1,-1),\end{split}
\end{equation*}
das wir auch  als Gleichungssystem
\begin{equation*}
\begin{split} \lambda_1 + \lambda_2 = a, \qquad \lambda_1 - \lambda_2 = b\end{split}
\end{equation*}
für \(\lambda_1\), \(\lambda_2\) schreiben können. Die Möglichkeit \(v\) als Linearkombination von \(v_1, \ldots,v_n\) darstellen zu können, ist dann die Frage der Lösbarkeit eines linearen Gleichungssystems für \(n\) unbekannte Koeffizienten, die wir noch intensiv diskutieren werden. Der Einfachheit halber werden wir uns dazu auf \(\K=\R\) beschränken.
\label{vektorraeume/vektorraeume:theorem-9}
\begin{sphinxadmonition}{note}{Theorem 3.1}



Sei \(W \subset V\), dann ist lin\((W)\) ein Teilraum von \(V\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Da die Gesetze der Addition und Multiplikation auch in lin\((W) \subset V\) gelten, müssen wir nur nachprüfen, dass \(+:\)lin\((W) \times\) lin\((W) \rightarrow \)lin\((W)\) und \(\cdot:\R \times\) lin\((W) \rightarrow\) lin\((W)\) gilt.Ist \(v \in \) lin\((W)\), dann gilt
\begin{equation*}
\begin{split} v = \sum_{i=1}^n \lambda_i v_i, \qquad \lambda_i \in \R, v_i \in W\end{split}
\end{equation*}
und
damit auch
\begin{equation*}
\begin{split} \alpha v = \sum_{i=1}^n (\alpha \lambda_i) v_i \quad \in \text{ lin}(W).\end{split}
\end{equation*}
Ist darüber hinaus \(w \in \) lin\((W)\), dann gilt
\begin{equation*}
\begin{split} v = \sum_{i=1}^n \mu_i w_i, \qquad \mu_i \in \R, w_i \in W\end{split}
\end{equation*}
und
damit
\begin{equation*}
\begin{split} v+ w = \sum_{i=1}^{n+m} \lambda_i v_i \qquad \lambda_i \in \R, v_i \in W,\end{split}
\end{equation*}
wobei wir \(\lambda_{n+i} = \mu_i\) und \(v_{n+i} = \mu_i\) setzen. Also ist auch \(v+w \in \) lin\((W)\).
\end{sphinxadmonition}
\label{vektorraeume/vektorraeume:theorem-10}
\begin{sphinxadmonition}{note}{Theorem 3.2}



Sei \(V\) ein Vektorraum und \(B \subset V\) eine Basis mit \(|B|=n\). Dann gilt für jede linear unabhängig Menge \(W\) auch \(|W| \leq n\) und für jedes Erzeugendensystem \(E\) auch \(|E| \geq n\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(W\) eine  Menge und nehmen wir an es gibt \(n+1\) linear unabhängige Elemente \(w_1, \ldots w_{n+1}\) in \(W\). Dann gibt es für jedes \(w_i\) eine Darstellung in der Basis \(B=\{b_1,\ldots,b_n\}\), d.h.
\begin{equation*}
\begin{split} w_i =  \sum_{j=1}^n \lambda_{ij} b_j.\end{split}
\end{equation*}
Wir beginnen mit \(i=1\). Da nicht alle \(\lambda_{1j} =0 \) sein können, nehmen wir ohne Beschränkung der Allgemeinheit an, dass \(\lambda_{11} \neq 0\) gilt. Also folgt
\begin{equation*}
\begin{split} - \lambda_{11} w_i + \lambda_{i1} w_1 = \sum_{j=2}^n \lambda_{ij}^{(2)}  b_j, \qquad i=2,\ldots,n+1\end{split}
\end{equation*}
mit neuen Koeffizienten \(\lambda_{ij}^{(2)}\). Da nicht alle \(\lambda_{2j}^{(2)} \) gleich Null sein können, nehmen wir wieder an, \(\lambda_{22}^{(2)} \neq 0\). Mit analogem Vorgehen erhalten wir eine Darstellung der Form
\begin{equation*}
\begin{split} \alpha_{1i} w_1 + \alpha_{2i} w_2 + \beta_i w_i = \sum_{j=3}^n \lambda_{ij}^{(3)}  b_j, \qquad i=3,\ldots,n+1.\end{split}
\end{equation*}
Wenden wir dieses Argument wiederholt an, so gilt nach \(n\) Schritten
\begin{equation*}
\begin{split} \alpha_{1,n+1} w_1 + \alpha_{2,n+1} w_2 + \ldots \alpha_{n,n+1} w_n + \beta_{n+1} w_{n+1} = 0\end{split}
\end{equation*}
und damit sind die \(w_i\) linear abhängig.
Ist umgekehrt \(v_1, \ldots, v_m\) ein Erzeugendensystem mit \(m<n\), so können wir \(b_1,\ldots,b_{m+1}\) wie oben durch die \(v_i\) ausdrücken und analog folgern, dass die Menge \(\{b_1,\ldots,b_{m+1}\}\) linear abhängig ist. Dies widerspricht aber der Basis\sphinxhyphen{}Eigenschaft.
\end{sphinxadmonition}

Wir sehen insbesondere, dass jede Basis eines Vektorraums die gleiche Anzahl an Elementen haben muss, sobald eine endliche Basis existiert. Wir sprechen dann von der Dimension eines Vektorraums, die wir gleich der Anzahl der Basiselemente setzen. Ist \(|B|\) nicht endlich, so sprechen wir von einem unendlich\sphinxhyphen{}dimensionalen Vektorraum. In einem Raum der Dimension \(n\) können (und werden) wir immer jedes Element als Linearkombination aller \(n\) Basiselemente schreiben und ggf. Koeffizienten gleich Null setzen.
Im Allgemeinen sehen wir auch, dass die Entwicklung von \(V\) in einer Basis, d.h. eine Linearkombination aus Basiselementen, eindeutig ist:
\label{vektorraeume/vektorraeume:theorem-11}
\begin{sphinxadmonition}{note}{Theorem 3.3}



Sei \(V\) ein Vektorraum mit Basis \(B\) und \(v \neq \in V\). Dann existiert genau ein \(n \in \N\) und eine Teilmenge \(\{b_i\}_{i=1,\ldots,n} \subset V\) sowie Koeffizienten \(\lambda_i \in \R\) mit
\begin{equation*}
\begin{split} v =  \sum_{i=1}^n \lambda_i b_i .\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei
\begin{equation*}
\begin{split} v = \sum_{i=1}^n \lambda_i b_i = \sum_{j=1}^m \lambda_j' b_j'\end{split}
\end{equation*}
mit \(b_j' \in B\). Dann gilt mit \(\lambda_{n+i} = -\lambda_i'\) und \(b_{n+i}=-b_i'\)
\begin{equation*}
\begin{split} \sum_{i=1}^{n+m} \lambda_i b_i = 0.\end{split}
\end{equation*}
Wegen der linearen Unabhängigkeit der Basis folgt dann aber \(v=0\).
\end{sphinxadmonition}

Wir definieren nun noch die Summe zweier Vektorräume \(V\) und \(W\), die beide Unterräume eines größeren Raums \(U\) sind, als
\begin{equation*}
\begin{split} V+W = \{ v + w ~|~ v \in V, w \in W \}.\end{split}
\end{equation*}\label{vektorraeume/vektorraeume:example-12}
\begin{sphinxadmonition}{note}{Example 3.8}



Sei
\begin{equation*}
\begin{split} V= \{(x,0)~|~x \in  \R \}, \qquad W= \{(0,y)~|~Y \in  \R \},\end{split}
\end{equation*}
dann gilt  \(V+W = \R^2\).
\end{sphinxadmonition}

Analog definieren wir \(\sum_{i=1}^m V_i\) für mehrere Vektorräume   \(V_i\).Eine Summe von Vektorräumen heisst \sphinxstyleemphasis{\textbackslash{}2}, wenn für jedes \(w \in \sum_{i=1}^m V_i\) eine eindeutige Zerlegung\(w=\sum_{i=1}^n v_i\), \(v_i \in V_i\) existiert. Wir schreiben dann für direkte Summe zweier Vektorräume \(V\) und \(W\) auch \(V \oplus W\).
\label{vektorraeume/vektorraeume:lemma-13}
\begin{sphinxadmonition}{note}{Lemma 3.1}



Seien \(V\) und \(W\) Vektorräume mit Summe \(V+W\). Die Summe ist direkt genau dann, wenn \(V \cap W = \{0\}\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Wir zeigen die äquivalente Aussage \(V \cap W \neq \{0\}\) genau dann, wenn die Summe \(V+W\) nicht direkt ist. Sei \(w \neq 0, w \in V \cap W\). Dann ist wegen der Vektorraumeigenschaft auch \(-w \in W\). Damit hat
\begin{equation*}
\begin{split} 0  = 0 + 0 = w + (-w) \in V + W\end{split}
\end{equation*}
eine nichteindeutige Darstellung als Summe. Sei umgekehrt die Summe nicht direkt, dann gibt es \(v_1,v_2 \in V\), \(v_1 \neq v_2\) and \(w_1, w_2 \in W\), \(w_1 \neq w_2\) mit
\begin{equation*}
\begin{split} v_1 + w_1 = v_2 + w_2.\end{split}
\end{equation*}
Nun definieren wir \(v = v_1-v_2 = w_2 -w_1\) und sehen sofort \(v \in V\cap W \setminus\{0\}\).
\end{sphinxadmonition}


\section{Lineare Abbildungen}
\label{\detokenize{vektorraeume/LineareAbb:lineare-abbildungen}}\label{\detokenize{vektorraeume/LineareAbb::doc}}
Wir können nun Abbildungen zwischen zwei Vetorräumen \(V_1\) und \(V_2\) betrachten, die einfachsten sind dabei die linearen, die auch eine wichtige Klasse bilden.
\label{vektorraeume/LineareAbb:definition-0}
\begin{sphinxadmonition}{note}{Definition 3.3}



Wir bezeichnen eine Abbildung \(L: V_1 \rightarrow V_2\) als linear, wenn die Addition und Skalarmultiplikation unter \(L\) erhalten bleibt, d.h.
\begin{itemize}
\item {} 
\(\forall v,w \in V_1: L(v+w) = L(v) + L(w).\)

\item {} 
\(\forall v \in V_1, \alpha \in \R: L(\alpha v) = \alpha L(v)\)

\end{itemize}
\end{sphinxadmonition}

Wir sehen einfach durch einen induktiven Beweis, dass für alle \(n \in \N\), \(\lambda_i \in \R\) und \(v_i \in V_1\) dann gilt
\begin{equation*}
\begin{split} L(\sum_{i=1}^n \lambda_i v_i) = \sum_{i=1}^n \lambda_i L(v_i) .\end{split}
\end{equation*}
Daraus sehen wir auch, dass es genügt eine lineare Abbildung auf einer Basis \(B\) von \(V_1\) zu definieren, denn ein beliebiges \(v \in V_1\) können wir dann als \(v= \sum_{i=1}^n \lambda_i b_i\) schreiben und daraus folgt
\begin{equation*}
\begin{split} L(v) = \sum_{i=1}^n \lambda_i L(b_i).\end{split}
\end{equation*}
Sind umgekehrt \(L(b_i)\) festgelegt, erhalten wir daraus immer mit der obige Setzung eindeutig eine lineare Abbildung.
\label{vektorraeume/LineareAbb:example-1}
\begin{sphinxadmonition}{note}{Example 3.9}



\(L: \R \rightarrow \R\) erfüllt \(L(x) = x L(1)\) für alle \(x \in \R\), also ist die lineare Abbildung durch den Wert am einzigen Basiselement \(b_1=1\) festgelegt (diesen nennen wir Steigung).
\end{sphinxadmonition}
\label{vektorraeume/LineareAbb:example-2}
\begin{sphinxadmonition}{note}{Example 3.10}



\(L: \R^2 \rightarrow \R, (x_1,x_2) \mapsto x_1 +x_2\) ist festgelegt durch \(L((1,0)) = 1\) und \(L((0,1))=1\).
\end{sphinxadmonition}
\label{vektorraeume/LineareAbb:example-3}
\begin{sphinxadmonition}{note}{Example 3.11}



\(L: \R^2 \rightarrow \R^2, (x_1,x_2) \mapsto (2x_1-x_2,2x_2+x_1)\) ist festgelegt durch \(L((1,0)) = (2,1)\) und \(L((0,1))=(-1,2)\).
\end{sphinxadmonition}

Wir sehen in den Beispielen, dass wir eine lineare Abbildung zwischen einem \(n\)\sphinxhyphen{}dimensionalen Vektorraum und einem \(m\)\sphinxhyphen{}dimensionalen Vektorraum durch \(nm\) reelle Zahlen festlegen können, indem wir die \(L(b_i)\), \(i=1,\ldots,n\) in einer Basis von \(V_2\) entwickeln.

Wir wollen nun die Injektivität und Surjektivität linearer Abbildungen genauer betrachten. Dies ist direkt verwandt mit der Eindeutigkeit und Lösbarkeit linearer Gleichungen: \(L(v) = w\) hat für jedes \(w\) eine Lösung \(v\), wenn \(L\) surjektiv ist. Die Lösung ist eindeutig, wenn \(L\) injektiv ist. Eine bijektive lineare Abbildung nennen wir Isomorphismus.
\label{vektorraeume/LineareAbb:lemma-4}
\begin{sphinxadmonition}{note}{Lemma 3.2}



Ein linearer Operator \(L\) ist injektiv genau dann, wenn aus \(L(v) = 0\) folgt \(v=0\).
\end{sphinxadmonition}
\label{vektorraeume/LineareAbb:theorem-5}
\begin{sphinxadmonition}{note}{Theorem 3.4}



Sei dim\((V) =n\), dann existiert ein Isomorphismus von \(V\) nach \(\R^n\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Wir definieren für die Basiselemente \(L(b_i) = e_i = (\delta_{ij})_{j=1,\ldots,n}\) und entsprechend
\begin{equation*}
\begin{split} L(v) = L(\sum_{i=1}^n \lambda_i b_i) = \sum_{i=1}^n \lambda_i e_i = (\lambda_1, \ldots, \lambda_n) \in \R^n .\end{split}
\end{equation*}
L ist injektiv, da aus \(L(v) = 0\) folgt \(\lambda_1=\ldots,\lambda_n = 0\) und damit \(v=0\). \(L\) ist surjektiv, da jedes
\begin{equation*}
\begin{split} (\lambda_1, \ldots, \lambda_n) \in \R^n \end{split}
\end{equation*}
gleich \(L(v)\) mit \(v = \sum_{i=1}^n \lambda_i b_i\) ist.
\end{sphinxadmonition}

Damit können wir im Prinzip jeden endlichdimensionalen Vektorraum mit \(\R^n\) identifizieren, indem wir einfach die Koeffizienten in der Basisentwicklung betrachten.
\label{vektorraeume/LineareAbb:theorem-6}
\begin{sphinxadmonition}{note}{Theorem 3.5}



Sei \(L: V_1 \rightarrow V_2\) eine lineare Abbildung zwischen endlichdimensionalen Vektorräumen. Dann gilt:
\begin{itemize}
\item {} 
Ist \(L\) injektiv, so folgt dim\((V_2) \geq \) dim\((V_1)\).

\item {} 
Ist \(L\) surjektiv, so folgt dim\((V_2) \leq \) dim\((V_1)\).

\end{itemize}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei dim\((V_1)=n\) und \(\{b_i\}_{i=1,\ldots,n}\) eine Basis für \(V_1\). Man sieht sofort, dass \(\{L(b_i)\}_{i=1,\ldots,n}\) ein Erzeugendensystem für das Bild von \(L\) ist. Ist \(L\) surjektiv, dann wissen wir auch dim\((V_2) \leq n\) gilt. Ist \(L\) injektiv, dann folgt aus
\begin{equation*}
\begin{split} 0 = \sum_{i=1}^n \lambda_i L(b_i) = L( \sum_{i=1}^n \lambda_i b_i)\end{split}
\end{equation*}
auch\(\sum_{i=1}^n \lambda_i b_i = 0\) und damit \(\lambda_1=\ldots=\lambda_n = 0\) und damit ist \(\{L(b_i)\}_{i=1,\ldots,n}\) ein linear unabhängiges System. Daraus folgt dim\((V_2) \geq n\).
\end{sphinxadmonition}

\textbackslash{}begin\{cor\}
Sei \(L: \R^n \rightarrow \R^m\). Ist \(m < n\), dann ist \(L\) nicht injektiv. Ist \(m> n\), dann ist \(L\) nicht surjektiv.\textbackslash{}end\{cor\}
Wir sehen dies auch in den obigen Beispielen von \(\R^2\) nach  \(\R\) (nicht injektiv, aber surjektiv) bzw. von \(\R^2\) nach \(\R^2\) (bijektiv).

Eine lineare Abbildung von \(\R^n\) nach \(\R^m\) können wir durch eine Matrix \(A \in \R^{m \times n}\) darstellen, wir schreiben
\begin{equation*}
\begin{split} A = \left( \begin{array}{ccc} a_{11} &\ldots& a_{1n} \\ a_{21} &\ldots &a_ {2n} \\ \vdots &\ddots &\vdots \\ a_{m1} &\ldots& a_{mn} \end{array} \right)\end{split}
\end{equation*}
wobei
\begin{equation*}
\begin{split} \left( \begin{array}{c} a_{1i} \\ a_{2i}  \\ \vdots \\ a_{mi}  \end{array} \right) = L(e_i)\end{split}
\end{equation*}
ist. An dieser Stelle müssen wir das erste Mal darauf achten, ob wir Zeilen\sphinxhyphen{} oder Spaltenvektoren schreiben.Ein Zeilenvektor ist dann eine Matrix in \(\R^{1 \times n}\), ein Spaltenvektor in \(\R^{m \times 1}\).Solange wir keine Matrizen verwenden ist die Unterscheidung unerheblich, aber etwa bei der Konstruktion von \(A\) oder bei der Multiplikation einer Matrix mit einem Vektor wird dies wichtig. Letztere können wir definieren, in dem wir wieder die Matrix \(A\) mit dem linearen Operator \(L\) identifizieren. Ist \(x\) ein Spaltenvektor, dann schreiben wir
\begin{equation*}
\begin{split} A x := L(x) = \left( \begin{array}{c} \sum_{i=1}^n a_{1i} x_i \\ \sum_{i=1}^n a_{2i} x_i \\ \vdots \\ \sum_{i=1}^n a_{mi} x_i  \end{array} \right) .\end{split}
\end{equation*}
Damit berechnen wir also immer das Skalarprodukt einer Zeile von \(A\) (ein Zeilenvektor der Länge \(n\)) mit dem Vektor \(x\) (ein Spaltenvektor der Länge \(n\). Wir werden sehen, dass dies auch bei allgemeiner Matrixmultiplikation der Fall ist. Um diese zu definieren können wir einfach die Hintereinanderausführung linearer Operatoren betrachten. Sind \(L_1: \R^n \rightarrow \R^m\) und  \(L_2: \R^m \rightarrow \R^k\) dargestellt durch Matrizen \(A \in \R^{m \times n}\) bzw. \(B \in \R^{k \times n}\), dann definieren wir das Matrixprodukt \(C = BA \in \R^{n \times k}\) als Darstellung der Abbildung \(x \mapsto L_2(L_1(x))\). Damit ist insbesondere
\begin{equation*}
\begin{split} C e_j = BAe_j = B (\sum_{k=1}^n a_{ik} \delta_{kj})_{i=1,\ldots,m} = B (a_{ij})_{i=1,\ldots,m} = (\sum_{p=1}^m b_{ip} a_{pj})_{i=1,\ldots,k}.\end{split}
\end{equation*}
Insgesamt gilt dann
\begin{equation*}
\begin{split} C = BA =  (\sum_{p=1}^m b_{ip} a_{pj})_{i=1,\ldots,k; j=1,\ldots,m} .\end{split}
\end{equation*}

\section{Lineare Gleichungssysteme}
\label{\detokenize{vektorraeume/LGS:lineare-gleichungssysteme}}\label{\detokenize{vektorraeume/LGS::doc}}
Im Folgenden werden wir uns mit linearen Gleichungssystemen der Form
\begin{equation*}
\begin{split}\begin{matrix}
a_{11} x_1 &+& a_{12}x_2 &+& \ldots &+& a_{1n}x_n &=& y_1 \\
a_{21} x_1 &+& a_{22}x_2 &+& \ldots &+& a_{2n}x_n &=& y_2 \\
\vdots &&  \vdots && \ddots && \vdots && \vdots \\
a_{m1} x_1 &+& a_{m2}x_2 &+& \ldots &+& a_{mn}x_n &=& y_m
\end{matrix}\end{split}
\end{equation*}
mit \(m,n \in \N\), \(a_{ij} \in \R\) und \(y_i \in \R\). Die Unbekannten sind dabei \(x_i \in \R\). Schreiben wir die \(x_i\) und \(y_i\) jeweils wieder in einen Spaltenvektor \(x\) bzw. \(y\) und die \(a_{ij}\) in eine Matrix \(A \in \R^{m \times n}\), so erhalten wir die kürzere Matrixform
\begin{equation*}
\begin{split} A x = y .\end{split}
\end{equation*}
Ist \(m=n=1\), so können wir einfach durch \(A\) dividieren um die Gleichung zu lösen, für größere \(m\) und \(n\) benötigen wir eine Verallgemeinerung dieses Vorgangs.
Wir wissen schon, dass die Lösung nur dann eindeutig ist, wenn das homogene System \(A x = 0\) nur die eindeutige Lösung \(x=0\) hat. Ist dies nicht der Fall, dann nennen wir
\begin{equation*}
\begin{split} {\cal N}(A) = \{ x \in \R^n ~|~ Ax = 0 \}\end{split}
\end{equation*}
den Nullraum der Matrix \(A\) (analog definieren wir den Nullraum einer linearen Abbildung). Wegen der Linearität prüft man leicht nach, dass \({\cal N}(A)\) ein Unterraum des \(\R^n\) ist. Wir nennen
\begin{equation*}
\begin{split} \text{Rg}(A) := n - \text{dim}({\cal N}(A))\end{split}
\end{equation*}
den Rang der Matrix \(A\). Dazu definieren wir den Range (oder auch das Bild) von \(A\) als
\begin{equation*}
\begin{split} {\cal R}(A) = \{ y \in \R^m~|~\exists x \in \R^N: y = Ax\}.\end{split}
\end{equation*}\label{vektorraeume/LGS:example-0}
\begin{sphinxadmonition}{note}{Example 3.12}



Die Matrix
\begin{equation*}
\begin{split} A = \left(\begin{matrix} 1 & 2 & 1 \\ 1 & 0 & 0  \\ 2 & 2 & 1  \end{matrix} \right)\end{split}
\end{equation*}
hat den Nullraum
\begin{equation*}
\begin{split} {\cal N}(A) = \{ x \in \R^3 ~|~x_1=0, x_3= -x_2\} =\{ \left(\begin{matrix} 0 \\ t  \\ -t  \end{matrix} \right) ~|~ t \in \R\}\end{split}
\end{equation*}
und den Range
\begin{equation*}
\begin{split} {\cal R}(A) = \{ y \in \R^3~|~y_3 = y_1 + y_2\}=\{ \left(\begin{matrix} s \\ t  \\ s+t  \end{matrix} \right) ~|~ s,t \in \R\}\end{split}
\end{equation*}
Hier ist der Rang zwei, dies ist gleich der Dimension von \({\cal R}(A)\).
\end{sphinxadmonition}
\label{vektorraeume/LGS:lemma-1}
\begin{sphinxadmonition}{note}{Lemma 3.3}



Sei dim\((V)=n\) und für \(k < n\) sei \(\{b_1,\ldots,b_k\}\) eine linear unabhängige Menge. Dann existieren \(b_{k+1}, \ldots, b_n\), sodass \(\{b_1,\ldots,b_n\}\) eine Basis von \(V\) ist.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Wir wählen nacheiander \(b_{j+1} \in \) lin\((\{b_1,\ldots,b_j\})\) für \(j=k,\ldots,n-1\) und sehen dann\( \{b_1,\ldots,b_j,b_{j+1} \}\) ist linear unabhängig. Für \(j=n-1\) sind wir dann bei einer Basis von \(V\) angekommen.
\end{sphinxadmonition}
\label{vektorraeume/LGS:lemma-2}
\begin{sphinxadmonition}{note}{Lemma 3.4}



Sei \(A \in \R^{m \times n}\). Dann gilt Rg\((A)=\)dim \(({\cal R}(A))\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(k=\)dim\(({\cal N}(A))\) und \(\{b_1,\ldots,b_k\}\) eine Basis von \({\cal N}(A)\). Dann können wir nach dem obigen Lemma zu einer Basis \(\{b_1,\ldots,b_n\}\) erweitern. Nun sehen wir, dass \(\{Ab_{k+1},\ldots,Ab_n\}\) ein Erzeugendensystem für \({\cal R}(A)\). Andererseits sehen wir auch, dass \(\{Ab_{k+1},\ldots,Ab_n\}\) linear unabhängig ist. Nehmen wir an es gibt \(\lambda_i \in \R\) mit
\begin{equation*}
\begin{split} 0 = \sum_{i=k+1}^n \lambda_i A b_i = A(\sum_{i=k+1}^n \lambda_i b_i).\end{split}
\end{equation*}
Da per Konstruktion \(\sum_{i=k+1}^n \lambda_i b_i\) nur dann im Nullraum von A liegt, falls \(\sum_{i=k+1}^n \lambda_i b_i = 0\) ist, folgt \(\lambda_i=0\)  wegen der linearen Unabhängigkeit der \(b_i\).
Damit ist \(n-k=\)Rg\((A)=\)dim\(({\cal R}(A))\).
\end{sphinxadmonition}

Wir sehen, dass der Rang angibt wie groß der Raum möglichen \(y\) ist, für die \(Ax = y\) lösbar ist. Andererseits gibt uns der Rang auch Information über die mögliche Anzahl der Lösungen, wenn das Problem lösbar ist. Grundlage dafür ist folgendes Resultat:
\label{vektorraeume/LGS:lemma-3}
\begin{sphinxadmonition}{note}{Lemma 3.5}



Sei \(x_0 \in \R^n\) eine Lösung von \(A x = y\). Dann ist die Menge aller Lösungen gegeben durch
\begin{equation*}
\begin{split} x_0 + {\cal N}(A) = \{ x \in \R^n ~|~x=x_0+z, z \in {\cal N}(A)\}.\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(x_0\) eine Lösung, dann gilt für \(z \in {\cal N}(A)\) auch
\begin{equation*}
\begin{split} A(x_0+z) = Ax_0  + A_z =y,\end{split}
\end{equation*}
also ist \(x_0 + z\) Lösung. Ist umgekehrt \(x\) eine weitere Lösung neben \(x_0\), dann gilt
\begin{equation*}
\begin{split} A(x-x_0)  = Ax - Ax_0= y - y=0,\end{split}
\end{equation*}
also ist \(x \in x_0 + {\cal N}(A)\).
\end{sphinxadmonition}

Allgemein nennen wir für einen Teilraum \(U \subset V\) und \(x \in V\) die Menge  \(M= x + U\) eine \sphinxstyleemphasis{lineare Mannigfaltigkeit}.


\subsection{Transponierte Matrizen und Skalarprodukte}
\label{\detokenize{vektorraeume/LGS:transponierte-matrizen-und-skalarprodukte}}
Zu einer Matrix \(A=(A_{ij})_{i=1,\ldots,m;j=1,\ldots,n} \in \R^{m \times n}\) definieren wir die transponierte Matrix \(A^T
\in \R^{n \times m} \) als
\begin{equation*}
\begin{split} A^T =  (A_{ji})_{i=1,\ldots,n;j=1,\ldots,m}.\end{split}
\end{equation*}
Die transponierte Matrix entsteht durch Wechseln zwischen Zeilen und Spalten, die Zeilen von \(A\) sind die Spalten von \(A^T\) und umgekehrt. Wir sehen leicht, dass \((A^T)^T = A \) gilt und darüber hinaus rechnen wir
\begin{equation*}
\begin{split} (AB)^T = (\sum_{p } A_{jp} B_{pi} ) = (\sum_{p } B_{pi} A_{jp}  ) = B^T A^T.\end{split}
\end{equation*}
Eine Matrix heisst symmetrisch, wenn \(A=A^T\) gilt.

Wir beachten, dass wir \(A^T A \in \R^{n \times n}\) und \(AA^T \in \R^{m \times m}\) berechnen können, während \(A A \) für \(m \neq n\) nicht definiert ist. Deshalb sind die Ausdrücke \(AA^T\) und \(A^T A\)  geeignete Verallgemeinerung des Quadrats auf eine Matrix. Im Fall von Vektoren \(x \in \R^{n \times 1}\) können wir
\begin{equation*}
\begin{split} x^T x = \sum_{i=1}^n x_i^2 \in \R\end{split}
\end{equation*}
berechnen und sehen \(x^T x > 0\) für \(x \neq 0.\)  Als Verallgemeinerung des Betrags definieren wir die Euklidische Norm
\begin{equation*}
\begin{split} \Vert x \Vert := \sqrt{x^T x}.\end{split}
\end{equation*}
Allgemeiner können wir für zwei Vektoren \(x,y \in \R^{n \times n}\) das \sphinxstyleemphasis{Skalarprodukt}
\begin{equation*}
\begin{split} x \cdot y := x^T y = y^T x =  \sum_{i=1}^n x_i y_i\end{split}
\end{equation*}
definieren. Das Skalarprodukt erlaubt eine Verallgemeinerung von Orthogonalität, wir sagen \(x\) und \(y\) sind orthogonal, wenn\( x^T y  = 0.\) Dies können wir auch für Basen definieren:
\label{vektorraeume/LGS:definition-4}
\begin{sphinxadmonition}{note}{Definition 3.4}



Eine Basis \(\{b_1,\ldots,b_n\}\) der \(\R^n\) heisst Orthogonalbasis, falls \(b_i^T b_j = 0\) für \(i \neq j\) gilt. Gilt zusätzlich \(\Vert b_i \Vert=1\) für \(i=1,\ldots,n\), dann sprechen wir von einer Orthonormalbasis. Analog definieren wir eine Orthogonalbasis oder Orthonormalbasis eines beliebigen Unterraums \(U \subset \R^n\).
\end{sphinxadmonition}

Wir können aus einer beliebigen Basis \(\{b_1,b_2, \ldots,b_n\}\) eine Orthonormalbasis konstruieren, mit dem sogenannten \sphinxstyleemphasis{Gram\sphinxhyphen{}Schmidt Verfahren}. Dabei beginnen wir mit \(b_1\) und normieren es einfach zu \(\tilde b_1 = \frac{b_1}{\Vert b_1\Vert}\). Wir beachten, dass \(\Vert b_1\Vert \neq 0\) gilt, da sonst \(b_1=0\) wäre und die Annahme einer Basis falsch wäre. \(\tilde b_2\) konstruieren wir als Linearkombination \(\tilde b_2 = \alpha \tilde b_1 + \beta b_2\) mit \(\alpha, \beta \in \R\), die wir so bestimmen, dass
\begin{align*}
0 &=  \tilde b_1^T \tilde b_2 = \alpha \tilde b_1^T \tilde b_1 + \beta \tilde b_1^T b_2\\
1 &=  \tilde b_2^T \tilde b_2 = \alpha^2 \tilde b_1^T \tilde b_1 + 2 \alpha \beta \beta \tilde b_1^T b_2 + \beta^2 b_2^T b_2.
\end{align*}
Wegen \(1 = \tilde b_1^T \tilde b_1 = \Vert \tilde b_1 \Vert^2\) folgt aus der ersten Gleichung
\begin{equation*}
\begin{split} \alpha = - \beta \tilde b_1^T b_2.\end{split}
\end{equation*}
Setzen wir dies in die zweite Gleichung ein, so folgt
\begin{equation*}
\begin{split} 1 = \beta^2 ( b_2^T b_2 - (\tilde b_1^T b_2)^2).\end{split}
\end{equation*}
Dies Gleichung hat einer reelle Lösung \(\beta\), da wegen der Cauchy\sphinxhyphen{}Schwarz Ungleichung (siehe Vorkurs) gilt
\begin{equation*}
\begin{split} (\tilde b_1^T b_2)^2 \leq \Vert \tilde b_1 \Vert^2 \Vert b_2 \Vert^2 =  b_2^T b_2 ,\end{split}
\end{equation*}
mit Gleichheit wenn \(\tilde b_1\) und \(b_2\) linear abhängig sind. Dies ist wegen der Basiseigenschaft ausgeschlossen, also ist
\begin{equation*}
\begin{split} \beta = \frac{1}{\sqrt{b_2^T b_2 - (\tilde b_1^T b_2)^2}} \end{split}
\end{equation*}
eine positive reelle Zahl.Nun gehen wir schrittweise weiter so vor und konstruieren \(\tilde b_{j+1}\) als Linearkombination von \(\tilde b_1, \ldots, \tilde b_j\) und \(b_{j+1}\), aus den Gleichungen
\begin{equation*}
\begin{split} \tilde b_i^T \tilde b_{j+1} = 0, \qquad i=1,\ldots,j\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} \tilde b_{j+1}^T \tilde b_{j+1} = 1.\end{split}
\end{equation*}
Das Vorgehen ist dabei völlig analog zur Bestimmung von \(\tilde b_2\), aus den ersten \(j\) Gleichungen können wir die ersten \(j\) Koeffizienten durch den Koeffizienten von \(\tilde b_{j+1}\) ausdrücken und diesen wieder aus der letzten Gleichung bestimmen.
Wir bemerken, dass wir im Fall einer Orthonormalbasis \(\{b_1,\ldots,b_n\}\) die Koeffizienten durch Skalarprodukte ausdrücken können. Ist
\begin{equation*}
\begin{split} x = \sum_{i=1}^n \lambda_i b_i,\end{split}
\end{equation*}
dann gilt
\begin{equation*}
\begin{split} b_j^T x = \sum_{i=1}^n \lambda_i b_j^T b_i = \lambda_j.\end{split}
\end{equation*}
Also erhalten wir
\begin{equation*}
\begin{split} x= \sum_{i=1}^n (b_i^T x) b_i.\end{split}
\end{equation*}
Wir sehen dann auch nochmal eine allgemeine Version des Satz von Pythagoras, es gilt
\begin{equation*}
\begin{split} \Vert x \Vert^2 = \sum_{i=1}^n (b_i^T x)^2.\end{split}
\end{equation*}
Nun wenden wir uns noch ein wenig dem Range und dem Nullraum von Operatoren zu. Insbesondere betrachten wir \({\cal R}(A)\) und \({\cal N}(A^T)\). Wir werden sehen, dass diese Räume orthogonal sind. Wir sprechen von zwei orthogonalen Unterräumen \(U\) und \(V\), wenn \(u^T v = 0\) für alle \(u \in U\) und \(v \in V\) gilt. Damit gilt insbesondere \(U \cap V = \{0\}\).
\label{vektorraeume/LGS:theorem-5}
\begin{sphinxadmonition}{note}{Theorem 3.6}



Es gilt
\begin{equation*}
\begin{split} \R^m  = {\cal R}(A) \oplus {\cal N}(A^T), \qquad \R^n = {\cal R}(A^T) \oplus {\cal N}(A ),\end{split}
\end{equation*}
dazu sind die beiden Teilräume jeweils orthogonal.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Wir zeigen zunächst die Orthogonalität. Ist \(y=Ax \in {\cal R}(A)\) und \(z \in {\cal N}(A^T)\), dann gilt
\begin{equation*}
\begin{split} z^T y = z^T A x = (A^T z)^T x = 0.\end{split}
\end{equation*}
Damit folgt insbesondere, dass die Summe aus den beiden Unterräumen direkt ist und wir müssen nur noch nachweisen, dass sie auch gleich dem \(\R^m\) ist. Sei zunächst \(\{b_1,\ldots,b_k\}\) eine Basis von \({\cal R}(A)\) und \(\{b_{k+1},\ldots,b_m\}\) eine Basiserweiterung. Dann können wir mit dem Gram\sphinxhyphen{}Schmidt Verfahren eine Orthonormalbasis  \(\{\tilde b_1,\ldots, \tilde b_m\}\)
konstruieren. Da \(\{\tilde b_1, \ldots, \tilde b_k\}\) aus Linearkombinationen von \(\{b_1, \ldots b_k\}\) entsteht, ist es eine Orthonormalbasis von \({\cal R}(A)\). Für \(j > k\) gilt nun \(\tilde b_i^T \tilde b_j = 0\) für \(j \leq k\) und damit auch
\begin{equation*}
\begin{split} y^T \tilde b_j = (Ax)^T \tilde b_j = x^T (A^T \tilde b_j) = 0\end{split}
\end{equation*}
für alle \(x \in \R^n\) und damit \(y \in {\cal R}(A)\). Insbesondere folgt daraus \(A^T \tilde b_j = 0\) für \(j > k\). Damit ist \(\tilde b_j \in {\cal N}(A^T)\). Wegen \(\R^m  = \)lin\((\{\tilde b_i\}_{i=1,\ldots,m})\) folgt dann direkt
\begin{equation*}
\begin{split} \R^m  = {\cal R}(A) \oplus {\cal N}(A^T).\end{split}
\end{equation*}
Die zweite Identität folgt direkt aus der ersten wegen \((A^T)^T = A\).
\end{sphinxadmonition}

Neben dem Skalarprodukt können wir auch das äußere Produkt \(x \otimes y = x y^T \in \R^{n \times n}\) berechnen. Für die Matrix \(A=x y^T\) gilt \(A z = x(y^T z)\), d.h. alle Elemente von \({\mathcal R}(A)\) sind skalare Vielfache von \(x\). Ist \(x \neq 0\) ist deshalb Rg\((A) = \)dim\(({\mathcal R}(A))\).


\subsection{Lösbarkeit}
\label{\detokenize{vektorraeume/LGS:losbarkeit}}
Basierend auf Skalarprodukten und der transponierten Matrix können wir auch eine Lösbarkeitsbedingung für das lineare Gleichungssystem \(A x = y\) herleiten. Es gilt
\begin{equation*}
\begin{split} z^T y = z^T A x = (A^T z)^T x.\end{split}
\end{equation*}
Ist nun \(z \in {\cal N}(A^T)\), so muss \(z^T y = 0\) sein. Dies ist aber nicht nur notwendige, sondern auch hinreichende Bedingung.
\label{vektorraeume/LGS:theorem-6}
\begin{sphinxadmonition}{note}{Theorem 3.7}



\(Ax=y\) ist lösbar, genau dann wenn \(z^T y = 0\) für alle \(z \in {\cal N}(A^T)\) gilt.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Die eine Richtung haben wir schon oben gesehen. Nun nehmen wir an, \(z^T y = 0\) für alle \(z \in {\cal N}(A^T)\) gilt.
\end{sphinxadmonition}
\label{vektorraeume/LGS:example-7}
\begin{sphinxadmonition}{note}{Example 3.13}



Sei
\begin{equation*}
\begin{split}A = \left(  \begin{matrix} 2 & 1 \\ 1  & 0 \\ -1 & 1\end{matrix} \right), \qquad A^T = \left(  \begin{matrix}
2 & 1 & -1 \\ 1 & 0 & 1 \end{matrix} \right).\end{split}
\end{equation*}
Dann gilt
\begin{equation*}
\begin{split} {\cal N}(A^T) = t \begin{pmatrix} 1 \\ - 3 \\ -1\end{pmatrix},\end{split}
\end{equation*}
also ist \(Ax =y\) lösbar, wenn \(y_1 - 3y_2 -y_3 = 0\) gilt.
\end{sphinxadmonition}

Wir haben nun die Lösbarkeit und die Lösungsmenge untersucht, als nächsten Schritt wollen wir nun konkreter die Berechnung einzelner Lösungen untersuchen. Dazu können wir das Problem auf eines mit quadratischer Matrix zurückführen: Ist \(m > n\), dann haben wir ein überbestimmtes Problem. Statt \(Ax = y\) können wir
\begin{equation*}
\begin{split}A^TA x = A^T y\end{split}
\end{equation*}
lösen. Ist \({\cal N}(A^T) =\{0\}\), dann ist eine Lösung von \(A^T A x=A^Ty\) auch eine Lösung von \(Ax =b\). Andernfalls ist das Problem nur lösbar, wenn \(y\) orthogonal zu \({\cal N}(A^T)\) ist und damit ist wiederum jede Lösung von \(A^T A x=A^Ty\) auch eine Lösung von \(Ax =y\).

Ist \(m < n\), dann ist das Problem unterbestimmt und der Nullraum von \(A\) in jedem Fall nichttrivial. Hier können wir eine Lösung der Form \(x=A^T z\) suchen, also das Problem \(AA^T z = y\) lösen.


\subsection{Quadratische Matrizen und Inverse}
\label{\detokenize{vektorraeume/LGS:quadratische-matrizen-und-inverse}}
Im Folgenden betrachten wir nun Systeme \(Ax = y\) mit \(A \in \R^{n \times n}\). In diesem Fall können wir bestenfalls das System für alle \(y \in \R^n\) eindeutig lösen, d.h. die Abbildung \(x \mapsto Ax\) ist bijektiv.
\label{vektorraeume/LGS:definition-8}
\begin{sphinxadmonition}{note}{Definition 3.5}



Eine Matrix \(A \in \R^{n \times n}\) heisst regulär, wenn für alle \(y \in \R^n\) eine eindeutige Lösung des linearen Systems \(Ax=y\) existiert. Andernfalls heisst \(A\) singulär.
\end{sphinxadmonition}

Ist \(A\) regulär, dann können wir insbesondere eindeutige Lösungen \(b_i\) zu \(y=e_i\) finden, d.h. \(A b_i = e_i\). Schreiben wir die Spaltenvektoren \(b_i\) in eine Matrix \(B \in \R^{n \times n}\) und analog die Spaltenvektoren \(e_i\) in die Einheitsmatrix \(I\), dann gilt \(AB=I\), d.h. \(B\) ist die inverse Matrix \(B=A^{-1}\). Also ist eine reguläre Matrix immer invertierbar im Sinne der Matrixmultiplikation und umgekehrt, denn für invertierbare Matrizen ist \(x=A^{-1}y\) die eindeutige Lösung.Wir sehen, dass die invertierbaren Matrizen im \(\R^{n \times n}\) eine Gruppe bilden mit \(I\) als neutralem Element, diese wird allgemeine lineare Gruppe (englisch general linear group), GL\((n)\) genannt.
\label{vektorraeume/LGS:example-9}
\begin{sphinxadmonition}{note}{Example 3.14}



Sei
\begin{equation*}
\begin{split} A = \left( \begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22}  \end{matrix} \right).\end{split}
\end{equation*}
Wir berechnen \(b_1\) als Lösung von \(Ax = e_1\), d.h.
\begin{equation*}
\begin{split} a_{11} b_{11} + a_{12} b_{21} = 1, \quad a_{21} b_{11} + a_{22} b_{21} = 0.\end{split}
\end{equation*}
Wir multiplizieren die erste Gleichung mit \(a_{22}\) und die zweite mit \(-a_{12}\) und addieren die beiden, dann erhalten wir
\begin{equation*}
\begin{split} (a_{11} a_{22} - a_{12} a_{21}) b_{11} = a_{22},\end{split}
\end{equation*}
also, falls der Nenner ungleich Null ist
\begin{equation*}
\begin{split} b_{11} = \frac{a_{22}}{a_{11} a_{22} - a_{12} a_{21}}, \quad b_{21} = \frac{-a_{12}}{a_{11} a_{22} - a_{12} a_{21}}.\end{split}
\end{equation*}
Analog können wir die zweite Spalte \(b_2\) der inversen Matrix als
\begin{equation*}
\begin{split} b_{22} = \frac{a_{11}}{a_{11} a_{22} - a_{12} a_{21}}, \quad b_{12} = \frac{-a_{21}}{a_{11} a_{22} - a_{12} a_{21}} \end{split}
\end{equation*}
berechnen. Damit erhalten wir
\begin{equation*}
\begin{split} A^{-1} = \frac{1}{a_{11} a_{22} - a_{12} a_{21}} \left( \begin{matrix} a_{22} & -a_{12} \\ - a_{21} & a_{11}  \end{matrix} \right).\end{split}
\end{equation*}
Die Inverse existiert unter der Bedingung, dass \(a_{11} a_{22} - a_{12} a_{21} \neq 0\).
\end{sphinxadmonition}


\subsection{Determinanten}
\label{\detokenize{vektorraeume/LGS:determinanten}}
Im letzen Beispiel haben wir gesehen, dass wir die Invertierbarkeit einer \(2 \times 2\) Matrix durch eine einzige Zahl, die sogenannte Determinante
\begin{equation*}
\begin{split} \text{det}(A) = a_{11} a_{22} - a_{12} a_{21}\end{split}
\end{equation*}
charakterisieren können. Wir werden sehen, das eine analoge Eigenschaft für \(n \times n\) Matrizen gilt. Wie im Fall von \(n=2\) werden wir Determinanten als Summe über Produkte definieren, in denen wir aus jeder Zeile (bzw. Spalte) genau ein Element verwenden. Dann müssen wir nur noch die jeweiligen Vorzeichen klären. Um dies strukturiert zu machen führen wir das Konzept der Permutation ein:
\label{vektorraeume/LGS:definition-10}
\begin{sphinxadmonition}{note}{Definition 3.6}



Eine \sphinxstyleemphasis{Permutation} ist eine bijektive Abbildung \(\pi: \{1,\ldots,n\} \rightarrow \{1,\ldots,n\} \). Die Menge der Permutationen zur Mächtigkeit \(n\) nennen wir \(\Pi_n\).
\end{sphinxadmonition}

Wir unterscheiden monotone Teile der Permutation (\(i \leq j\) und \(\pi(i) \leq \pi(j)\)), sowie nichtmonotone (\(i < j\) und \(\pi(i) > \pi(j)\)), letztere nennt man \sphinxstyleemphasis{Fehlstände} oder Inversionen
\begin{equation*}
\begin{split} \text{inv}(\pi): = \{(i,j)~|~i<j, \pi(i) > \pi(j) \}.\end{split}
\end{equation*}
Basierend darauf definieren wir das Vorzeichen einer Permutation als
\begin{equation*}
\begin{split} \text{sign}(\pi) =(-1)^{|\text{inv}(\pi)|}.\end{split}
\end{equation*}\label{vektorraeume/LGS:example-11}
\begin{sphinxadmonition}{note}{Example 3.15}



Wir betrachten nochmal die Determinante einer \(2 \times 2\) Matrix. Dort gibt es zwei Permutationen in \(\Pi_2\), nämlich
\begin{equation*}
\begin{split} \pi_1(1)=1, \pi_1(2)=2,\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} \pi_2(1)=2, \pi_2(2)=1.\end{split}
\end{equation*}
\(\pi_1\) hat keine Fehlstände, inv\((\pi_1)=\emptyset\), also sign\((\pi_1)=1\). \(\pi_2\) hat einen Fehlstand, inv\((\pi_2)={(1,2)}\),
also  sign\((\pi_2)=-1\). Die Determinante können wir dann als
\begin{equation*}
\begin{split} \text{det}(A) =  \text{sign}(\pi_1) a_{1\pi_1(1)} a_{2\pi_1(2)} + \text{sign}(\pi_2) a_{1\pi_2(1)} a_{2\pi_2(2)} \end{split}
\end{equation*}
schreiben.
\end{sphinxadmonition}

Die Einsicht des obigen Beispiels ist die Grundlage der Definition der Determinante einer \(n \times n\) Matrix:
\label{vektorraeume/LGS:definition-12}
\begin{sphinxadmonition}{note}{Definition 3.7}



Sei \(A \in \R^{n \times n}\), dann ist die Determinante von \(A\) definiert als
\begin{equation*}
\begin{split} \text{det}(A) = \sum_{\pi \in \Pi_n} \text{ sign}(\pi) \prod_{j=1}^n a_{j \pi(j)}.\end{split}
\end{equation*}\end{sphinxadmonition}

Wir sehen, dass durch die Eigenschaft der Permutationen in jedem Produkt genau ein Element aus jeder Zeile bzw. Spalte steht.
\label{vektorraeume/LGS:example-13}
\begin{sphinxadmonition}{note}{Example 3.16}



Für die Einheitsmatrix \(I \in \R^{n \times n}\) gilt det\((I)=1\).
\end{sphinxadmonition}
\label{vektorraeume/LGS:example-14}
\begin{sphinxadmonition}{note}{Example 3.17}



Für eine Dreiecksmatrix  \(A \in \R^{n \times n}\) (\(A_{ij} = 0\) für \(i< j\) oder \(A_{ij} = 0\) für \(i> j\)) gilt det\((A)=\prod_{i=1}^n A_{ii}\).
\end{sphinxadmonition}

Eine besonders wichtige Eigenschaft ist der sogenannte Determinantenproduktsatz, der auf einer einfachen Eigenschaft von Permutationen beruht, die wir hier nicht beweisen:
\label{vektorraeume/LGS:lemma-15}
\begin{sphinxadmonition}{note}{Lemma 3.6}



Seien \(\pi_1, \pi_2 \in \Pi_n\), dann gilt
\begin{equation*}
\begin{split}  \text{sign}(\pi_1 \circ \pi_2) = \text{ sign}(\pi_1) \text{ sign}(\pi_2).\end{split}
\end{equation*}\end{sphinxadmonition}

Wir wollen nun die Determinante eines Produkts \(A B\) berechnen, dazu betrachten wir zunächst wieder das Beispiel \(n=2\)
\label{vektorraeume/LGS:example-16}
\begin{sphinxadmonition}{note}{Example 3.18}



Wir berechnen det\((AB)\) für
\begin{equation*}
\begin{split} A = \left( \begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22}  \end{matrix} \right),  B = \left( \begin{matrix} b_{11} & b_{12} \\ b_{21} & b_{22}  \end{matrix} \right).\end{split}
\end{equation*}
Dann ist
\begin{equation*}
\begin{split} C = A B = \left( \begin{matrix} a_{11} b_{11} + a_{12} b_{21}   & a_{11} b_{12} + a_{12} b_{22} \\ a_{21} b_{11} + a_{22} b_{21}    & a_{21} b_{12} + a_{22} b_{22}   \end{matrix} \right)\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} \text{det}(C) = (a_{11} b_{11} + a_{12} b_{21}) (a_{21} b_{12} + a_{22} b_{22}) - (a_{11} b_{12} + a_{12} b_{22}) (a_{21} b_{11} + a_{22} b_{21}).\end{split}
\end{equation*}
Nach ausmultiplizieren erhalten wir
\begin{align*}
  \text{det}(C) =& (a_{11} b_{11} a_{21} b_{12}  +  a_{11} b_{11} a_{22} b_{22} + a_{12} b_{21} a_{21} b_{12} +  a_{12} b_{21}a_{22} b_{22}) - \\& (a_{11} b_{12} a_{21} b_{11} + a_{11} b_{12}  a_{22} b_{21} + a_{12} b_{22} a_{21} b_{11} + a_{12} b_{22} a_{22} b_{21}) \\ =& a_{11} a_{22} b_{11}  b_{22} - a_{12} a_{21}   b_{11} b_{22}   -  a_{11} a_{22}  b_{12}  b_{21} +  a_{12}  a_{21} b_{12} b_{21}  +\\ &    a_{11} a_{21} b_{11}  b_{12} - a_{11} a_{21} b_{11}  b_{12}   +  a_{12} a_{22} b_{21} b_{22}  -  a_{12} a_{22} b_{21} b_{22}  \\=& (a_{11} a_{22} - a_{12} a_{21} )(b_{11}  b_{22}  - b_{12}  b_{21} ).
\end{align*}
Wir sehen also det\((C)\)= det\((A)\) det\((B)\).
\end{sphinxadmonition}
\label{vektorraeume/LGS:theorem-17}
\begin{sphinxadmonition}{note}{Theorem 3.8}



Seien \(A,B \in \R^{n \times n}\). Dann gilt
\begin{equation*}
\begin{split} \text{det}(AB) = \text{ det}(A) \text{ det}(B).\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(C=AB\), dann gilt
\begin{align*}
\text{det}(C) &= \sum_{\pi \in \Pi_n} \text{ sign}(\pi) \prod_{i=1}^n C_{i\pi(i)} \\
&= \sum_{\pi \in \Pi_n} \text{ sign}(\pi) \prod_{i=1}^n ( \sum_{j_i=1}^n a_{ij_i} b_{j_i\pi(i)})  .
%&= \sum_{\pi \in \Pi_n} \text{ sign}(\pi) \sum_{j=1}^n \prod_{i_j=1}^n (  a_{i_j j} b_{j \pi(i_j)}) \\
%&= \sum_{j=1}^n \sum_{\pi \in \Pi_n} \text{ sign}(\pi)  \prod_{i_j=1}^n (  a_{i_j j} b_{j \pi(i_j)}).
\end{align*}
Nun wollen wir die letzte Summe und das Produkt vertauschen, d.h. ein allgemeines Distributivgesetz anwenden.Dazu definieren wir die Indexmenge \( I_n = \{1,\ldots,n\}^n\), die als mögliche Einträge für die Vektoren \(J=(j_1,\ldots,j_n)\) vorkommen. Nun sehen wir, dass
\begin{equation*}
\begin{split} \prod_{i=1}^n ( \sum_{j_i=1}^n a_{ij_i} b_{j_i\pi(i)}) = \sum_{J \in I_n} \prod_{i=1}^n  a_{ij_i} b_{j_i\pi(i)}\end{split}
\end{equation*}
gilt, was wir bei der Determinantenberechnung verwenden können:
\begin{align*} \text{det}(C) &= \sum_{\pi \in \Pi_n} \text{ sign}(\pi) \sum_{J \in I_n} \prod_{i=1}^n  a_{ij_i} b_{j_i\pi(i)}  \\ &= \sum_{J \in I_n}  \sum_{\pi \in \Pi_n} \text{ sign}(\pi)\prod_{i=1}^n  a_{ij_i} b_{j_i\pi(i)}\end{align*}\begin{equation*}
\begin{split} \prod_{i=1}^n  a_{ij_i} b_{j_i\pi(i)} = a_{kj_k} b_{j_k\pi(k)} a_{\ell j_k} b_{j_k\pi(\ell)} \prod_{i=1, i \neq k, i \neq \ell}^n  a_{ij_i} b_{j_i\pi(i)}. \end{split}
\end{equation*}
Sei nun \(\tilde \pi\) die Permutation mit \(\tilde \pi(\ell)=k, \tilde \pi(k) = \ell\) und \(\tilde \pi(i)=\pi(i)\) sonst. Dann hat \(\tilde \pi\) einen zusätzlichen Fehlstand zu \(\pi\), d.h. sign\((\tilde \pi)= - \) sign\((\pi)\). Darüber hinaus gilt
\begin{equation*}
\begin{split} \prod_{i=1}^n  a_{ij_i} b_{j_i\tilde \pi(i)} = a_{kj_k} b_{j_k\pi(\ell)} a_{\ell j_k} b_{j_k\pi(k)} \prod_{i=1, i \neq k, i \neq \ell}^n  a_{ij_i} b_{j_i\pi(i)} = \prod_{i=1}^n  a_{ij_i} b_{j_i\pi(i)} . \end{split}
\end{equation*}
Damit sind heben sich die beiden Terme mit \(\pi\) und \(\tilde \pi\) in der Summe über alle Permutationen weg und wir folgern für solche \(J\)
\begin{equation*}
\begin{split} \sum_{\pi \in \Pi_n} \text{ sign}(\pi)\prod_{i=1}^n  a_{ij_i} b_{j_i\pi(i)}  = 0.\end{split}
\end{equation*}
Damit bleiben nur jene \(J\) übrig, in denen jeder Index in \(\{1,\ldots,n\}\) genau einmal vorkommt. Diese entsprechen genau den Permutationen in \(\Pi_n\). Damit erhalten wir
\begin{equation*}
\begin{split} \text{det}(C) =  \sum_{\sigma \in \Pi_n} \sum_{\pi \in \Pi_n} \text{ sign}(\pi)\prod_{i=1}^n  a_{i\sigma(i)} b_{\sigma(i) \pi(i)} =  \sum_{\sigma \in \Pi_n} \sum_{\pi \in \Pi_n} \text{ sign}(\pi)\prod_{i=1}^n  a_{i\sigma(i)} \prod_{j=1}^n  b_{j \pi(\sigma^{-1}(j))},\end{split}
\end{equation*}
wobei \(\sigma^{-1}\) die Umkehrung von \(\sigma\) und damit wieder eine Permutation ist. Nun sehen wir leicht, dass
\begin{equation*}
\begin{split} \{ \pi \circ \sigma^{-1}~|~\pi \in \Pi_n\} = \Pi_n\end{split}
\end{equation*}
gilt und wegen des obigen Lemmas zum Vorzeichen der Hintereinanderausführung gilt
\begin{equation*}
\begin{split} \text{sign}(\pi \circ \sigma^{-1}) = \text{ sign}(\pi) \text{ sign}(\sigma^{-1}) = \text{ sign}(\pi) \text{ sign}(\sigma ) \end{split}
\end{equation*}
bzw.
\begin{equation*}
\begin{split}  \text{sign}(\pi ) = \text{ sign}(\sigma)  \text{sign}(\pi \circ \sigma^{-1}).\end{split}
\end{equation*}
Also folgt
\begin{align*} \text{det}(C) &=  \sum_{\sigma \in \Pi_n} \sum_{\pi' \in \Pi_n} \text{ sign}(\pi') \text{ sign}(\sigma) \prod_{i=1}^n  a_{i\sigma(i)} \prod_{j=1}^n  b_{j \pi'(j)} \\  &= \sum_{\sigma \in \Pi_n} \sum_{\pi' \in \Pi_n} \text{ sign}(\pi') \text{ sign}(\sigma) \prod_{i=1}^n  a_{i\sigma(i)} \prod_{j=1}^n  b_{j \pi'(j)}  \\  &= \sum_{\sigma \in \Pi_n} \text{ sign}(\sigma) \prod_{i=1}^n  a_{i\sigma(i)} \sum_{\pi' \in \Pi_n} \text{ sign}(\pi')   \prod_{j=1}^n  b_{j \pi'(j)}  = \text{ det}(A) \text{ det}(B). \end{align*}\end{sphinxadmonition}

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Gabriel Cramer}

\sphinxhref{https://de.wikipedia.org/wiki/Gabriel\_Cramer}{Gabriel Cramer} (* 31. Juli 1704 in Genf; † 4. Januar 1752 in Bagnols\sphinxhyphen{}sur\sphinxhyphen{}Cèze, Frankreich) war ein Genfer Mathematiker.
\end{sphinxShadowBox}

Aus dem Determinantenproduktsatz können wir die Cramer’sche Regel, eine Formel für die Lösung des Gleichungssystems \(Ax=y\) herleiten, aus der wir auch die Lösbarkeit sehen, wenn die Determinante nicht verschwindet. Sei dazu \(X_i \in \R^{n \times n}\) die Matrix, die aus der Einheitsmatrix entsteht, wenn man die \(i\)\sphinxhyphen{}te Spalte durch \(x\) ersetzt. Dann gilt \(A X_i = A_i\), wobei \(A_i \in \R^{n \times n}\) die Matrix ist, die aus \(A\) entsteht, wenn man die \(i\)\sphinxhyphen{}te Spalte durch \(y=Ax\) ersetzt.  Nach dem Determinantenproduktsatz gilt dann
\begin{equation*}
\begin{split} \det(A) \det(X_i) = \det(A_i).\end{split}
\end{equation*}
Wir sehen sofort \(\det(X_i)=x_i\), also ist das System für jeden Eintrag von \(x\) lösbar, wenn \(\det(A)\neq 0\) und wir erhalten
\begin{equation*}
\begin{split} x_i = \frac{\det(A_i)}{\det(A)},\end{split}
\end{equation*}
die sogenannte Cramer’sche Regel.
\label{vektorraeume/LGS:theorem-18}
\begin{sphinxadmonition}{note}{Theorem 3.9}



\(A \in \R^{n \times n}\) ist regulär genau dann wenn \(\det(A)\neq 0\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Die eine Richtung sehen wir sofort aus der Cramer’schen Regel. Ist umgekehrt \(A\) regulär, dann folgt aus dem Determinantenproduktsatz
\begin{equation*}
\begin{split} \det(A) \det(A^{-1}) = \det(I) = 1,\end{split}
\end{equation*}
damit kann \(\det(A)\) nicht verschwinden. \(\square\)
\end{sphinxadmonition}

Die Cramer’sche Regel ist theoretisch sehr angenehm, aber ungeeignet für die praktische Anwendung bei großen linearen Gleichungssystemen, da die Berechnung der Determinanten sehr aufwändig ist. Es gibt \(n!\) verschiedene Permutationen, für jede davon ist ein Produkt aus \(n \) Faktoren zu berechnen. Deshalb betrachten wir alternative Verfahren und überlegen zuerst für welche Matrizen das System \(Ax=y\) einfach zu lösen ist.
\label{vektorraeume/LGS:definition-19}
\begin{sphinxadmonition}{note}{Definition 3.8}


\begin{itemize}
\item {} 
\(D \in \R^{n \times n}\) heisst Diagonalmatrix, wenn \(D_{ij} =0 \) für \(i \neq j\) gilt.

\item {} 
\(L \in \R^{n \times n}\) heisst linke untere Dreiecksmatrix, wenn \(L_{ij} =0 \) für \(i < j\) gilt.

\item {} 
\(R \in \R^{n \times n}\) heisst rechte obere Dreiecksmatrix, wenn \(R_{ij} =0 \) für \(i > j\) gilt.

\end{itemize}
\end{sphinxadmonition}

Im Fall einer Diagonalmatrix ist die Lösung von \(Dx = y\) direkt durch
\begin{equation*}
\begin{split} x_i = \frac{y_i}{D_{ii}}\end{split}
\end{equation*}
gegeben. Im Fall einer linken unteren Dreiecksmatrix berechnen wir die Einträge nacheinander (sogenanntes Vorwärtseinsetzen)
\begin{align*}
x_1 &= \frac{y_1}{L_{11}} \\
x_2 &= \frac{y_2-L_{21} x_1}{L_{22}} \\
x_3 &= \frac{y_3-L_{31} x_1-L_{32} x_2}{L_{33}} \\
\ldots
\end{align*}
Im Fall einer rechten oberen Dreiecksmatrix gehen wir umgekehrt vor und beginnen beim letzten Eintrag (sogenanntes Rückwärtseinsetzen)
\begin{align*}
x_n &= \frac{y_n}{R_{nn}} \\
x_{n-1} &= \frac{y_{n-1}-R_{n-1,n} x_n}{R_{n-1,n-1}} \\
x_{n-2} &= \frac{y_{n-2}-R_{n-2,n} x_n-R_{n-2,n-1} x_{n-1}}{R_{n-2,n-2}} \\
\ldots
\end{align*}
Dasselbe Vorgehen ist auch möglich, wenn wir eine Matrix \(A\) haben, die sich durch Vertauschen von Zeilen und Spalten in eine linkere und rechte obere Dreiecksmatrix überführen lässt. Zeilenvertauschung bedeutet, dass wir Gleichungen umnummerieren, Spaltenvertauschung, dass wir Variablen umnummerieren.
Wie können wir nun ein allgemeines System \(Ax = y\) mit \(A \in \R^{n \times n}\) lösen ? Die grundlegende Idee ist es, das System auf eine Dreiecksform zu bringen und dann Vorwärts\sphinxhyphen{} oder Rückwärtseinsetzen zu benutzen. Dies erreicht das sogenannte Gauss\sphinxhyphen{}Verfahren. Hier beginnen wir mit der ersten Gleichung
\begin{equation*}
\begin{split} a_{11}x_1 + a_{12} x_2 + \ldots + a_{1n} x_n = y_n.\end{split}
\end{equation*}
Ist \(a_{11}=0\) so vertauschen wir die erste Gleichung mit einer anderen Gleichung \(k \in \{2,\ldots,n\}\), sodass \(a_{k1} \neq 0\) gilt. Ist \(a_{k1}=0\) für alle \(k\), so sehen wir sofort dass \(\det(A)=0\) und damit die Gleichung nicht eindeutig lösbar (wenn überhaupt) ist.  Nachdem wir ggf. vertauscht haben, können wir eine Linearkombination von Zeilen durchführen, die die Lösungsmenge unverändert lässt. Wir ziehen für \(k=2,\ldots,n\) ein Vielfaches \(\ell_{k1} = - \frac{a_{k1}}{a_{11}}\) der ersten von der \(k\)\sphinxhyphen{}ten Zeile ab. \(\ell_{k1}\) ist genau so gewählt, dass \(x_1\) aus den Gleichungen verschwindet. Wir erhalten mit
\begin{equation*}
\begin{split}\tilde a_{ki} = a_{ki} + \ell_{k1} a_{1i}, \qquad \tilde y_k = y_k + \ell_{k1} y_1\end{split}
\end{equation*}
die Gleichungen
\begin{equation*}
\begin{split} \sum_{i=2}^n \tilde a_{ki} x_i = \tilde y_k, \qquad k=2,\ldots,n.\end{split}
\end{equation*}
Damit haben wir ein lineares Gleichungssystem in \(\R^{n-1 \times n-1}\) und können nun analog weiter verfahren. Die neue erste Variable ist \(x_2\) und die neue erste Zeile ist die Gleichung mit \(k=2\). Nun können wir völlig analog vorgehen bis wir bei \(k=n\) angekommen sind, hier erhalten wir einfach ein \(1x1\) System für \(x_n\), das wir direkt lösen können. Nun berechnen wir die anderen Variablen durch Rückwärtseinsetzen. Für \(x_{n-1}\) verwenden wir die erste Gleichung im \(2 \times 2\) System aus dem vorletzten Schritt, für \(x_{n-2}\) die erste Gleichung aus dem \(3 \times 3\) System im drittletzten Schritt und so weiter.  Dieses Verfahren nennt man Gauss\sphinxhyphen{}Elimination.
\label{vektorraeume/LGS:example-20}
\begin{sphinxadmonition}{note}{Example 3.19}



Wir betrachten \(Ax=y\) mit
\begin{equation*}
\begin{split} A = \left( \begin{matrix} 1 & 2 & 2 \\ 2 & 4 & 3 \\ 0 & -1 & 1 \end{matrix} \right).\end{split}
\end{equation*}
Hier ist \(a_{11}=1\), damit \(\ell_{21} = -2, \ell_{31} = 0. \) Als erste Gleichung, die wir uns für später merken, erhalten wir
\begin{equation*}
\begin{split} x_1 + 2 x_2 + 2x_3 = y_1\end{split}
\end{equation*}
und nach Elimination von \(x_1\) aus der zweiten und dritten Gleichung bleibt das \(2 \times 2 \) System.
\begin{equation*}
\begin{split} 0 x_2 -x_3 = y_2 -2 y_1, \qquad -x_2 + x_3 = y_3.\end{split}
\end{equation*}
Da nun der erste Koeffizient \(0\) ist, vertauschen wir die beiden Gleichungen und haben dann das gestaffelte System (entsprechend rechter oberer Dreiecksmatrix\textbackslash{}begin\{align*\}
x\_1 + 2 x\_2 + 2x\_3 \&= y\_1 \textbackslash{} \sphinxhyphen{}x\_2 + x\_3 \&= y\_3 \textbackslash{}
\sphinxhyphen{}x\_3 \&= y\_2 \sphinxhyphen{}2 y\_1
\textbackslash{}end\{align*\}Durch Rückwärtseinsetzen berechnen wir
\begin{equation*}
\begin{split} x_3=2y_1 - y_2, \quad x_2 = 2 y_1 -y_2 -y_3, \quad x_1 = - 7 y_1 + 4y_2 +2 y_3.\end{split}
\end{equation*}\end{sphinxadmonition}

Um die Rechnung bei der Gauss Elimination ein wenig strukturierter durchführen zu können definieren wir eine rechte obere Dreiecksmatrix \(R \in \R^{n \times n}\) im Laufe des Verfahrens. Im ersten Schritt setzen wir \(R_{1i}=a_{1i}\) für \(i=1,\ldots,n\) und \(z_1=y_1\). Im zweiten Schritt setzen wir \(R_{21} =0\) und \(R_{2i} = \tilde a_{2i}\) für \(i=2, \ldots,n\) mit \(\tilde a_{2i}\) wie oben, und so fahren wir auch für \(k=3\) bis \(m\) fort. Damit haben wir am Ende ein System \(Rx=z\), das wir durch Rückwärtseinsetzen lösen können.
\label{vektorraeume/LGS:example-21}
\begin{sphinxadmonition}{note}{Example 3.20}



Für
\begin{equation*}
\begin{split} A = \left( \begin{matrix} 1 & 2 & 2 \\ 2 & 4 & 3 \\ 0 & -1 & 1 \end{matrix} \right)\end{split}
\end{equation*}
wie oben erhalten wir
\begin{equation*}
\begin{split} R = \left( \begin{matrix} 1 & 2 & 2 \\ 0 & -1 & 1 \\ 0 & 0 & -1 \end{matrix} \right).\end{split}
\end{equation*}\end{sphinxadmonition}

Wir sehen auch, dass wir eigentlich keine Gleichungen schreiben müssen, sondern alles direkt auf Ebene der Matrix \(A\) und des Vektors \(y\) durchführen können. Die wichtigste Erkenntnis ist, dass eine Linearkombination von Zeilen in einer Matrix der Multiplikation mit einer Matrix von links entspricht. Wir führen dies für das Gauss\sphinxhyphen{}Verfahren nochmal durch. Im ersten Schritt multiplizieren wir mit einer Matrix
\begin{equation*}
\begin{split}L_1 = \left( \begin{matrix} 1 & 0 & 0 & \ldots & 0 \\ \ell_{21} & 1 & 0 &\ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\ \ell_{n1} & 0 & 0 & \ldots & 1 \end{matrix} \right)\end{split}
\end{equation*}
bzw. im \(k\)\sphinxhyphen{}ten Schritt mit einer Matrix \(L_k\), die neben der Diagonale nur Einträge in der \(k\)\sphinxhyphen{}ten Spalte unter der Diagonale hat, d.h.
\begin{equation*}
\begin{split}(L_k)_{ij} = \left\{ 
\begin{matrix} 
1 & i=j \\ 0 & i < j \\ 
0 & i > j, j \neq k \\ 
\ell_{ik} & i=k,j >k
\end{matrix}
\right\}.\end{split}
\end{equation*}
Damit entspricht das Gauss\sphinxhyphen{}Verfahren einfach einer wiederholten Multiplikation des Systems \(Ax=y\) mit solchen Matrizen, bis wir
\begin{equation*}
\begin{split} Rx = L_{n-1} L_{n-2} \ldots L_2 L_1 A x = L_{n-1} L_{n-2} Ax  \ldots L_2 L_1 y = z\end{split}
\end{equation*}
erhalten. Man prüft leicht nach, dass \(L_k\) invertierbar ist mit einer sehr einfach zu berechnenden Inverse
\begin{equation*}
\begin{split}(L_k^{-1})_{ij} = \left\{ \begin{matrix} 1 & i=j \\ 0 & i < j \\ 0 & i > j, j \neq k \\ - \ell_{ik} & i=k,j > k\end{matrix}\right.\end{split}
\end{equation*}
Damit können wir auch die Identität
\begin{equation*}
\begin{split}  R  = L_{n-1} L_{n-2} \ldots L_2 L_1 A\end{split}
\end{equation*}
nochmal neu betrachten. Es gilt dann
\begin{equation*}
\begin{split} A =  L_1^{-1}   L_2^{-1} \ldots  L_{n-2}^{-1} L_{n-1}^{-1} R  = L R,\end{split}
\end{equation*}
wobei \(L= L_1^{-1}   L_2^{-1} \ldots  L_{n-2}^{-1} L_{n-1}^{-1}\). Nun kann man leicht nachrechnen, dass \(L\) eine linke untere Dreiecksmatrix ist, deren Diagonaleinträge alle gleich eins sind. Wir haben also damit \(A\) in eine linke untere und eine rechte obere Dreiecksmatrix zerlegt. Hat man dies einmal berechnet, dann ist es genauso aufwändig, sich die Matrix \(A\) zu merken wie \(L\) und \(R\) (für \(L\) die Einträge unter der Diagonale, für \(R\) auf und über der Diagonale). Sind \(L\) und \(R\) bekannt, kann das System \(Ax=y\) leicht gelöst werden durch
\begin{equation*}
\begin{split} L z = y, \qquad Rx = z.\end{split}
\end{equation*}
Damit müssen wir einmal und Vorwärts\sphinxhyphen{} und einmal Rückwärtseinsetzen.

Kommt nun auch die Vertauschung von Zeilen hinzu, so benötigen wir auch dafür geeignete Matrizen, die sogenannten Permutationsmatrizen. Zu jeder Permutation \(\pi \in \Pi_n\) können wir eine Permutationsmatrix \(P^\pi \in \R^{n \times n}\) definieren als \(P_{ij}^\pi = (\delta_{i\pi(i)})\), d.h. \(P^\pi\) hat in jeder Zeile und Spalte einen Eintrag eins und \(n-1\) Einträge null. Eine Multiplikation \(PA\) entspricht einer Vertauschung von Zeilen \(AP\) einer Vertauschung von Spalten. Im Fall des Gauss\sphinxhyphen{}Verfahrens benötigt man in jedem Schritt nur die Vertauschung der \(k\)\sphinxhyphen{}ten Zeile mit einer Zeile \(p \geq k\), d.h. die Einträge von \(P^\pi\) entsprechen in \(n-2\) Zeilen der Einheitsmatrix, nur in Zeile \(k\) ist das \(p\)\sphinxhyphen{}te Element gleich eins und umgekehrt. Das Gauss\sphinxhyphen{}Verfahren mit Vertauschung ist dann
\begin{equation*}
\begin{split} R=L_{n-1} P_{n-1} L_{n-2} P_{n-2}\ldots L_2 P_2 L_1 P_1 A ,\end{split}
\end{equation*}
mit entsprechenden Permutationsmatrizen \(P_j\) für Zweiervertauschungen. Die Zerlegung insgesamt kann dann am Ende in der Form
\begin{equation*}
\begin{split} P A = L R\end{split}
\end{equation*}
geschrieben werden mit einer allgemeinen Permutationsmatrix \(P\).


\chapter{Metriken und Konvergenz}
\label{\detokenize{metrik/metrik:metriken-und-konvergenz}}\label{\detokenize{metrik/metrik::doc}}
Im Folgenden wollen wir uns mit allgemeinen Konvergenzbegriffen beschäftigen. Wir haben schon die Konvergenz reeller Folgen betrachtet, dazu haben wir Abstände über den Betrag gemessen. Im \(\R^N\) haben wir die Euklidische Norm als geeignetes Maß für Abstände kennengelernt. Dies wollen wir nun verallgemeinern.


\section{Metriken und Normen}
\label{\detokenize{metrik/normen:metriken-und-normen}}\label{\detokenize{metrik/normen::doc}}\label{metrik/normen:definition-0}
\begin{sphinxadmonition}{note}{Definition 4.1}



Eine Menge \(X\) mit einer Abbildung \(d: X \times X \rightarrow \R\), welche die folgenden Eigenschaften erfüllt, heisst \sphinxstyleemphasis{metrischer Raum}:
\begin{itemize}
\item {} 
\(d(x,y) \geq 0\) für alle \(x,y \in X\) und \(d(x,y) = 0\) genau dann wenn \(x=y\) (Positivität).

\item {} 
\(d(x,y) = d(y,x)\) für alle \(x,y \in X\)  (Symmetrie).

\item {} 
\(d(x,z) \leq d(x,y) + d(y,z)\) für alle \(x,y,z \in X\)  (Dreiecksungleichung).
Die Abbildung \(d\) heisst dann Metrik auf \(X\).

\end{itemize}
\end{sphinxadmonition}

Eine Metrik ist ein abstrakter Abstandsbegriff auf Mengen, wir beachten, dass \(X\) keinerlei Vektorraumstruktur haben muss.  Liegt eine solche vor, dann kann man einfache Metriken passend zu Linearkombinationen definieren, wie wir auch aus bekannten Beispielen sehen.In \(\R\) ist \(d(x,y) = |x-y|\) eine Metrik, im \(\R^n\) ist \(d(x,y) = \Vert x -y \Vert \) eine Metrik, wobei \(\Vert \cdot \Vert\) die Euklidische Norm ist. Allgemein definieren wir Normen folgendermaßen:
\label{metrik/normen:definition-1}
\begin{sphinxadmonition}{note}{Definition 4.2}



Eine Abbildung \(\Vert \cdot \Vert:X\rightarrow \R\) auf einem Vektorraum \(X\) heisst \sphinxstyleemphasis{Norm}, wenn die folgenden Eigenschaften erfüllt sind:
\begin{itemize}
\item {} 
\(\Vert x \Vert \geq 0\) für alle \(x \in X\) und \(\Vert x \Vert = 0\) genau dann wenn \(x=y\) (Positivität).

\item {} 
\(\Vert \alpha x \Vert = |\alpha|~\Vert x \Vert\) für alle \(\alpha \in \R\). \(x\in X\)  (Absolute Homogenität).

\item {} 
\(\Vert x+y \Vert \leq\Vert x  \Vert + \Vert y  \Vert\) für alle \(x,y  \in X\)  (Dreiecksungleichung).
\((X,\Vert \cdot \Vert)\)  nennen wir dann einen normierten Raum oder normierten Vektorraum.

\end{itemize}
\end{sphinxadmonition}

Aus Normen erhalten wir immer spezielle Metriken, wie der folgende Satz zeigt:
\label{metrik/normen:theorem-2}
\begin{sphinxadmonition}{note}{Theorem 4.1}



Ist \((X,\Vert \cdot \Vert)\) ein normierter Raum, dann ist \(d\) definiert durch
\begin{equation*}
\begin{split} d(x,y) = \Vert x - y \Vert ,\qquad \forall x,y \in X\end{split}
\end{equation*}
eine Metrik auf \(X\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Wir sehen sofort, dass \(d(x,y) \geq 0\) wegen der Positivität der Norm gilt und \(d(x,y) = \Vert x - y\Vert = 0\) gilt nur für \(x-y = 0\), also \(x=y\). Die Symmetrie folgt aus der absoluten Homogenität mit der Wahl \(\alpha = 1\), dann ist
\begin{equation*}
\begin{split} d(y,x) = \Vert y- x \Vert = \Vert (-1)(x-y) \Vert = |-1|~\Vert x -y \Vert = \Vert x-y \Vert = d(x,y).\end{split}
\end{equation*}
Die Dreiecksungleichung ist  gleichbedeutend zur Dreiecksungleichung einer Metrik wenn wir als Vektorraumelemente (\(x\) und \(y\)) \(x-y\) und \(y-z\) einsetzen.
\end{sphinxadmonition}
\label{metrik/normen:example-3}
\begin{sphinxadmonition}{note}{Example 4.1}



Wir prüfen kurz nach, dass die Euklidische Norm
\begin{equation*}
\begin{split} \Vert x \Vert = \sqrt{\sum_{i=1}^N x_i^2}\end{split}
\end{equation*}
im \(\R^N\) tatsächlich eine Norm gemäß der obigen Definition ist.Es gilt per Definition \(\Vert x \Vert \geq 0\) und \(\Vert x \Vert = 0\) impliziert \(\sum_{i=1}^N x_i^2=0\). Dies ist nur möglich für \(x_1=x_2=\ldots=x_N=0\), also \(x=0\). Die absolute Homogenität erhalten wir aus
\begin{equation*}
\begin{split} \Vert \alpha x \Vert = \sqrt{\sum_{i=1}^N \alpha ^2 x_i^2} = \sqrt{\alpha^2 (\sum_{i=1}^N \alpha ^2 x_i^2)} = |\alpha|~ \sqrt{\sum_{i=1}^N x_i^2}. \end{split}
\end{equation*}
Die Dreiecksungleichung können wir äquivalent quadrieren, da sie aus lauter positiven Termen besteht. Also ist sie äquivalent zu
\begin{equation*}
\begin{split}  {\sum_{i=1}^N (x_i+y_i)^2} \leq \sum_{i=1}^N x_i^2 + \sum_{i=1}^N y_i^2 + 2 \sqrt{\sum_{i=1}^N x_i^2} \sqrt{\sum_{i=1}^N y_i^2}.\end{split}
\end{equation*}
Quadrieren wir die Summe auf der linken Seite aus und kürzen die entsprechenden Terme mit \(x_i^2\) und \(y_i^2\), so bleibt
\begin{equation*}
\begin{split} 2 \sum_{i=1}^N x_i y_i \leq 2 \sqrt{\sum_{i=1}^N x_i^2} \sqrt{\sum_{i=1}^N y_i^2},\end{split}
\end{equation*}
was aus der Cauchy\sphinxhyphen{}Schwarz Ungleichung folgt.
\end{sphinxadmonition}

Weitere Beispiele für Normen sind (siehe auch die Übung)
\begin{itemize}
\item {} 
Die \(p\)\sphinxhyphen{}Norm

\end{itemize}
\begin{equation*}
\begin{split} \Vert x \Vert_p = \left( \sum_{i=1}^N |x_i|^p \right)^{1/p}\end{split}
\end{equation*}
ist für \(1 \leq p < \infty\) eine Norm auf \(\R^N\) (eine Verallgemeinerung der Euklidischen Norm).
\begin{itemize}
\item {} 
Die Maximumsnorm

\end{itemize}
\begin{equation*}
\begin{split} \Vert x \Vert_\infty = \max_{i=1,\ldots,N} |x_i|\end{split}
\end{equation*}
ist eine Norm auf \(\R^N\).
\begin{itemize}
\item {} 
Die Supremumsnorm

\end{itemize}
\begin{equation*}
\begin{split} \Vert x \Vert_\infty = \sup_{i \in \N} |x_i|\end{split}
\end{equation*}
ist eine Norm auf \(\ell^\infty(\N)\).Aus dem letzten Beispiel sehen wir auch, dass wir Normen verwenden können um Teilräume zu definieren. \(\ell^\infty(\N)\) ist ja genau jener Teilraum von \(\R^\N\) auf dem die Supremumsnorm endliche Werte anwendet. Haben wir also eine Abbildung von \(X  \) nach \(\R \cup \{+ \infty\}\), sodass die Eigenschaften der Positivität, absoluten Homogenität und Dreiecksungleichung erfüllt sind, so ist die Teilmenge \(\tilde X\) auf der die Abbildung endlich ist, ein normierter Raum. Die Teilraumeigenschaft überprüfen wir direkt mit der absoluten Homogenität und Dreiecksungleichung.
Nicht alle Metriken entstehen aus Normen, ein wichtiges Beispiel einer Metrik, die nicht aus einem normierten Vektorraum entsteht ist die geodätische Distanz auf der Erdoberfläche. Um dies mathematische zu vereinfachen, betrachten wir \(X\) als den Einheitskreis im \(R^2\) und als Metrik \(d(x,y)\) die Länge des kürzeren Kreisbogens zwischen \(x\) und \(y\). Dieser entspricht genau dem (spitzen) Winkel \(\varphi \in [0,\pi]\) zwischen \(x\) und \(y\). Offensichtlich ist \(d(x,y)\) dann nichtnegativ und \(d(x,y)=0\) gilt nur, wenn der Winkel gleich Null ist, d.h. \(x =y\). Auch die Symmetrie ist offensichtlich, und die Dreiecksungleichung folgt, da der Winkel zwischen \(x\) und \(z\) entweder der Summe aus den Winkeln zwischen \(x\), \(y\) sowie \(y\), \(z\) ist, oder kleiner als einer der beiden Winkel.


\section{Konvergente Folgen}
\label{\detokenize{metrik/konvfolgen:konvergente-folgen}}\label{\detokenize{metrik/konvfolgen::doc}}
Mit Hilfe einer Metrik können wir Konvergenz von Folgen und ähnliche Begriffe definieren. Zunächst beginnen wir mit Umgebungen:
\label{metrik/konvfolgen:definition-0}
\begin{sphinxadmonition}{note}{Definition 4.3}



Sei \(X\) ein metrischer Raum und \(\epsilon \in \R_+\). Dann heisst
\begin{equation*}
\begin{split} U_\epsilon(x) = \{ y \in X~|~d(x,y) < \epsilon \}\end{split}
\end{equation*}
\(\epsilon\)\sphinxhyphen{}Umgebung von \(x\)
\end{sphinxadmonition}
\label{metrik/konvfolgen:definition-1}
\begin{sphinxadmonition}{note}{Definition 4.4}



Sei \(X\) ein metrischer Raum und \(M \subset X\). \(M\) heisst offen, wenn für alle \(x \in M\) eine \(\epsilon\)\sphinxhyphen{}Umgebung \(U_\epsilon(x) \subset M\) existiert (wobei \(\epsilon\) von \(x\) abhängen kann). \(M\) heisst abgeschlossen, wenn \(X \setminus M\) offen ist.
\end{sphinxadmonition}

Wir beachten, dass die leere Menge immer offen ist, damit ist \(X =X \setminus \emptyset\) abgeschlossen. Andererseits ist \(X\) natürlich auch offen, da ja alle \(\epsilon\)\sphinxhyphen{}Umgebungen per Definition Teilmengen von \(X\) sind. Damit ist auch wieder \(\emptyset =  X \setminus X\) abgeschlossen. Die leere Menge und \(X\) sind aber die einzigen Mengen, die in \(X\) abgeschlossen und offen sind.
\label{metrik/konvfolgen:example-2}
\begin{sphinxadmonition}{note}{Example 4.2}



Sei \(X=\R\) und \(a < b \in \R\), dann ist das Intervall \([a,b]\) abgeschlossen und das Intervall \((a,b)\) offen. Die Intervalle \([a,b)\) bzw. \((a,b]\) sind weder abgeschlossen noch offen.
\end{sphinxadmonition}
\label{metrik/konvfolgen:example-3}
\begin{sphinxadmonition}{note}{Example 4.3}



Sei \(X=\R^2\) mit der Euklidischen Norm, dann sind \(\epsilon\)\sphinxhyphen{}Umgebungen genau Kreisscheiben um \(x\) mit Radius \(\epsilon\), wobei der Kreis mit Radius \(\epsilon\) ausgenommen ist.
\end{sphinxadmonition}

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Augustin Cauchy}

\sphinxhref{https://de.wikipedia.org/wiki/Augustin-Louis\_Cauchy}{Augustin\sphinxhyphen{}Louis Cauchy} (* 21. August 1789 in Paris; † 23. Mai 1857 in Sceaux) war ein französischer Mathematiker.
\end{sphinxShadowBox}

Konvergente und Cauchy\sphinxhyphen{}Folgen in metrischen Räumen können wir exakt wie in \(\R\) definieren, in dem wir einfach die spezielle Betragsmetrik durch eine allgemeine Metrik ersetzen:
\label{metrik/konvfolgen:definition-4}
\begin{sphinxadmonition}{note}{Definition 4.5}



Sei \(X\) ein metrischer Raum und \((x_n)\) eine Folge in \(X\). \((x_n)\) heißt konvergent mit Grenzwert \(\overline{x}\), wenn
\begin{equation*}
\begin{split} \forall \epsilon  >0 ~\exists n_0 \in \N ~\forall n \geq n_0: d(x_n, \overline{x}) < \epsilon.\end{split}
\end{equation*}
\((x_n)\) heisst Cauchy\sphinxhyphen{}Folge, wenn
\begin{equation*}
\begin{split} \forall \epsilon  >0 ~\exists n_0 \in \N ~\forall m,n \geq n_0: d(x_n, x_m) < \epsilon.\end{split}
\end{equation*}\end{sphinxadmonition}

Wir sehen also, dass konvergente Folgen ab einem bestimmten Index in einer \(\epsilon\)\sphinxhyphen{}Umgebung um den Grenzwert liegen.

Es gilt folgendes einfach zu beweisende Resultat:
\label{metrik/konvfolgen:nullfolgenmetrik}
\begin{sphinxadmonition}{note}{Lemma 4.1}



Die Folge \((x_n)\) im metrischen Raum \((X,d)\) konvergiert genau dann gegen \(\overline{x}\), wenn \(d(x_n,\overline{x})\) eine Nullfolge in \(\R\) ist.
\end{sphinxadmonition}

Genau wie in \(\R\) können wir auch leicht folgendes Resultat beweisen:
\label{metrik/konvfolgen:theorem-6}
\begin{sphinxadmonition}{note}{Theorem 4.2}



Jede konvergente Folge in einem metrischen Raum ist eine Cauchy\sphinxhyphen{}Folge.
\end{sphinxadmonition}

Wir haben schon gesehen, dass nicht in jedem metrischen Raum die Umkehrung gilt (nicht alle Cauchy\sphinxhyphen{}Folgen konvergieren), z.B. in \(\Q\) mit der Betragsmetrik.
\label{metrik/konvfolgen:definition-7}
\begin{sphinxadmonition}{note}{Definition 4.6}



Ein metrischer Raum \((X,d)\) heisst vollständig, wenn jede Cauchy\sphinxhyphen{}Folge in \(X\) konvergiert.
\end{sphinxadmonition}

Wir kennen bereits ein Beispiel eines vollständigen Raums, nämlich \(\R\) mit der Betragsnorm. Dies gilt auch im \(\R^N\):
\label{metrik/konvfolgen:theorem-8}
\begin{sphinxadmonition}{note}{Theorem 4.3}



Für \(N \in \N\) ist \(\R^N\) mit der durch die Euklidischen Norm definierten Metrik vollständig.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Ist \((x_n)\) eine Cauchy\sphinxhyphen{}Folge in \(\R^N\), so gilt für die Koordinatenfolge
\begin{equation*}
\begin{split} x_n^{(i)} = e_i \cdot x_n,\end{split}
\end{equation*}
die Ungleichung
\begin{equation*}
\begin{split} | x_n^{(i)} -  x_m^{(i)}  | \leq \sqrt{ \sum_{j=1}^n (x_n^{(j)} -  x_m^{(i)} } = \Vert x_n - x_m \Vert .\end{split}
\end{equation*}
Damit ist \((x_n^{i})\) eine Cauchy\sphinxhyphen{}Folge in \(\R\) und somit konvergent. Wir bezeichnen den Grenzwert mit \(\overline{x}^{(i)}\) und den Vektor
\(\overline{x}=(\overline{x}^{(i)})_{i=1,\ldots,N}\).

Nun gibt es für jedes \(\epsilon > 0\) ein \(n_0^{(i)} \in \N\), sodass für alle \( n \geq n_0^{i)}\) gilt:
\begin{equation*}
\begin{split} \vert x_n^{i} -\overline{x}^{(i)} \vert < \frac{\epsilon}{\sqrt{N}}.\end{split}
\end{equation*}
Für \(n_0 = \max_{i=1,\ldots,N} n_0^{(i)}\) folgt.
\begin{equation*}
\begin{split} \forall n \geq n_0: \Vert x_n - \overline{x} \Vert = \sqrt{\sum_{i=1}^N (x_n^{(i)} - \overline{x}^{(i)})^2} < \sqrt{\sum_{i=1}^N \frac{\epsilon^2}N} = \epsilon.\end{split}
\end{equation*}
Damit konvergiert \(x_n\) gegen \(\overline{x}\).
\end{sphinxadmonition}

Mit ähnlichen Argumenten sehen wir, dass eine Folge \((x_n)\) im \(\R^N\) mit der Euklidischen Metrik genau dann konvergiert, wenn alle Koordinatenfolgen \((x_n^{i})\) in \(\R\) konvergieren. Mit den Eigenschaften für Grenzwerte in \(\R\) zeigen wir auch für konvergente Folgen \((x_n)\) und \((y_n)\) im \(\R^N\):
\begin{align*}
\lim (x_n + y_n) &= \lim x_n + \lim y_n \\
\lim (x_n \cdot y_n) &= \lim x_n \cdot \lim y_n.
\end{align*}

\section{Teilfolgen}
\label{\detokenize{metrik/teilfolgen:teilfolgen}}\label{\detokenize{metrik/teilfolgen::doc}}
Nicht immer konvergiert die ganze Folge, wie wir schon in \(\R\) am Beispiel \(x_n = (-1)^n\) sehen. Hier konvergieren offensichtlich die sogenannten Teilfolgen \(x_{2n}\) (gegen \(+1\)) und \(x_{2n+1}\) (gegen \(-1\)), nicht aber die ganze Folge.
\label{metrik/teilfolgen:definition-0}
\begin{sphinxadmonition}{note}{Definition 4.7}



Sei \(K: \N \rightarrow \N\) eine streng monoton wachsende Abbildung (\(K(i) > K(j)\) für \(i > j\)). Dann heisst \((x_{K(n)})_{n \in \N}\) Teilfolge von \((x_n). \)
\end{sphinxadmonition}

Wir werden im Folgenden für Teilfolgen auch kurz \((x_{k_n})_{n \in \N}\) schreiben.
Wir können das Problem der Konvergenz von Teilfolgen nun näher untersuchen, dazu führen wir zunächst das Konzept eines Häufungspunktes ein:
\label{metrik/teilfolgen:definition-1}
\begin{sphinxadmonition}{note}{Definition 4.8}



Sei \((X,d)\) ein metrischer Raum. Dann heisst \(y \in X\) Häufungspunkt der Folge \((x_n) \subset X\), wenn
\begin{equation*}
\begin{split} \forall \epsilon > 0, m \in \N ~\exists n > m: d(x_n,y) < \epsilon.\end{split}
\end{equation*}\end{sphinxadmonition}
\label{metrik/teilfolgen:example-2}
\begin{sphinxadmonition}{note}{Example 4.4}



Die reelle Folge \(x_n = (-1)^n\) besitzt die Häufungspunkte \(+1\) und \(-1\). Gleiches gilt für \(x_n = (-1)^n + \frac{1}{n+1}\).
\end{sphinxadmonition}
\label{metrik/teilfolgen:theorem-3}
\begin{sphinxadmonition}{note}{Theorem 4.4}



Sei \((X,d)\) ein metrischer Raum und \((x_n)\) eine Folge in \(X\). Dann gilt:

*\(i)\) Ist \(x_n\) konvergent gegen \(\overline{x} \in X\), dann ist \(\overline{x}\) der einzige Häufungspunkt von \((x_n)\).
*\(ii)\) Ist \(y\) Häufungspunkt von \((x_n)\), dann existiert eine konvergente Teilfolge \((x_{n_k})\) mit Grenzwert \(y\) und umgekehrt.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  (i) Aus der Definition ist klar, dass \(\overline{x}\) Häufungspunkt ist: Sei \(x_n\) konvergent und \(\epsilon > 0\). Dann ist für ein \(n_0 \in \N\) und \(n \geq n_0\) auch \(d(x_n, \overline{x}) < \epsilon\). Falls \(m < n_0\) gilt, können wir \(n=n_0\) wählen, sonst einfach \(n=m+1\). Nehmen wir an \(y \neq \overline{x}\) ist Häufungspunkt und sei \(\epsilon < \frac{d(\overline{x},y)}2\). Dann gilt für alle \(n \geq 0\) wegen der Dreiecksungleichung
\begin{equation*}
\begin{split} d(x_n,y) \geq d(y,\overline{x}) - d(x_n,\overline{x}) > 2 \epsilon - \epsilon = \epsilon,\end{split}
\end{equation*}
ein Widerspruch zur Häufungspunkteigenschaft.
(ii)Ist \(y\) Häufungspunkt, dann wählen wir hintereinander \(\epsilon_k = \frac{1}k\), \(k \in \N \setminus \{0\}\). Zunächst wählen wir für \(\epsilon_1\) \(m=1\), damit existiert ein \(n_1\), sodass \(d(x_{n_1},y) < 1\). Nun wählen wir induktive für \(\epsilon_{k+1}\)  \(m=n_k\). Dann existiert ein \(n_{k+1} > n_k\) mit
\begin{equation*}
\begin{split} d(x_{n_{k+1}},y) < \frac{1}{k+1}.\end{split}
\end{equation*}
Wir sehen sofort, dass \(x_{n_k}\) gegen \(y\) konvergiert.
Ist umgekehrt \(y\) Grenzwert einer konvergenten Teilfolge, dann gibt es für jedes \(\epsilon >0 \) und \(m \in \N\) ein \(k_0 \) mit
\begin{equation*}
\begin{split} d(x_{n_k},y) < \epsilon, \qquad \forall n_k \geq n_{k_0},\end{split}
\end{equation*}
also ist \(y\) Häufungspunkt.
\end{sphinxadmonition}

In normierten Räumen können wir wieder Summen von Folgen betrachten. Es gilt jedoch nicht immer, dass die Häufungspunkte von \(x_n+y_n\) die Summe der Häufungspunkte beider Folgen sind. Als Beispiel dafür können wir \(x_n=0\) für \(n\) ungerade und \(x_n=n\) für \(n\) gerade, sowie \(y_n=n\) für \(n\) ungerade und \(y_n=0\) für \(n\) gerade betrachten. Beide Folgen haben den Häufungspunkt \(0\), aber es gilt \(x_n+y_n = n\) für alle \(n \in \N\), damit hat diese Folge keinen Häufungspunkt. Es gilt aber folgendes Resultat:
\label{metrik/teilfolgen:lemma-4}
\begin{sphinxadmonition}{note}{Lemma 4.2}



Sei \(x_n\) eine Folge mit Häufungspunkt \(z\) und \(y_n\) eine konvergente Folge mit Grenzwert \(y\). Dann ist \(z+y\) ein Häufungswert von \(x_n + y_n\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Ist \(z\) Häufungspunkt von \((x_n)\) und \((y_n)\) konvergent, dann gibt es eine Teilfolge \((x_{n_k})\) mit Grenzwert \(z\), die Teilfolge \((y_{n_k})\) ist aber immer noch konvergent gegen \(y\). Also ist \((x_{n_k}+y_{n_k})\) eine konvergente Teilfolge von \((x_n+y_n)\) mit Grenzwert \(z+y\).
\end{sphinxadmonition}

Ein besonders wichtiges Resultat über konvergente Teilfolgen ist der Satz von Bolzano\sphinxhyphen{}Weierstrass:
\label{metrik/teilfolgen:theorem-5}
\begin{sphinxadmonition}{note}{Theorem 4.5}



Sei \((x_n) \subset \R^N\) eine beschränkte Folge, d.h. es gibt \(C \in \R\) mit
\begin{equation*}
\begin{split} \Vert x_n \Vert \leq C\end{split}
\end{equation*}
für alle \(n \in \N\). Dann existiert eine konvergente Teilfolge \((x_{n_k})\) mit Grenzwert \(\overline{x}\) und es gilt \(\Vert \overline{x} \Vert \leq C\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Wir beweisen zunächst den Fall \(N=1\) durch sogenannte Bisektion. Wir beginnen mit \(a_0=-C\) und \(b_0=C\), dann ist \(x_n \in [a_0,b_0]\) für alle \(n \in \N\). Wir setzen \(n_0 =0\) und berechnen \(c_0=\frac{a_0+b_0}2\). Eines der beiden Intervalle \([a_0,c_0]\) und \([c_0,b_0]\) muss nun unendlich viele Folgenglieder von \(x_n\) enthalten. Enthält \([a_0,c_0]\) unendlich viele, so setzen wir \(a_1=a_0\), \(b_1=c_0\), andernfalls \(a_1=c_0\), \(b_1=b_0\). Wir wählen nun
\begin{equation*}
\begin{split} n_1 = \min\{n > n_0~|~x_n \in [a_1,b_1] \}.\end{split}
\end{equation*}
Nun gehen wir induktiv weiter.
Wir nehmen an die ersten Glieder \(x_{n_0},\ldots,x_{n_k}\) der Teilfolge sind gegeben, ebenso Intervalle \([a_j,b_j]\), \(j=0,\ldots,k\),  mit \(x_{n_j} \in [a_j,b_j]\) und unendlich vielen Folgengliedern in \( [a_j,b_j]\).Nun berechnen wir \(c_{k+1}= \frac{a_k + b_k}2\) und machen die gleiche Fallunterscheidung wie im Fall \(k=0\) um\([a_{k+1},b_{k+1}]\) zu berechnen. Dazu definieren wir
\begin{equation*}
\begin{split} n_{k+1} = \min\{n > n_k~|~x_n \in [a_{k+1},b_{k+1}] \}.\end{split}
\end{equation*}
Damit erhalten wir eine unendliche Folge kleiner werdender Intervalle \([a_k,b_k]\), es gilt \(|b_k - a_k|=2^{1-k}C\).Darüber hinaus haben wir eine Teilfolge \(x_{n_k}\) konstruiert, mit \(x_{n_j} \in [a_k,b_k]\) für \(j \geq k\). Damit gilt auch
\begin{equation*}
\begin{split} \vert x_{n_i} - x_{n_j} \vert \leq 2^{1-k} C\end{split}
\end{equation*}
für \(n_i, n_j \geq n_k. \) Daraus sehen wir sofort, dass \((x_{n_j})\) eine Cauchy\sphinxhyphen{}Folge und damit konvergent ist.

Im \(\R^N\) folgt aus der Beschränktheit von \((x_n)\) auch die Beschränktheit jeder Koordinatenfolge, wegen
\begin{equation*}
\begin{split} \vert x_n^{(i)} \vert \leq \Vert x_n \Vert .\end{split}
\end{equation*}
Nun können wir eine konvergente Teilfolge von \(x_n^{1}\) wählen. Da alle Koordinatenteilfolgen \(x_{n_k}^{(i)}\) immer noch beschränkt sind, wählen wir davon eine weitere Teilfolge, sodass \(x_{n_k}^{(2)}\) konvergiert, natürlich konvergiert dann ja auch \(x_{n_k}^{(1)}\) noch. Durch schrittweises Auswählen weiterer Teilfolgen von Teilfolgen enden wir schliessliche bei einer Teilfolge, für die alle Koordinaten konvergieren und damit auch \((x_{n_k})\) selbst.
\end{sphinxadmonition}

Wir haben den Satz von Bolzano\sphinxhyphen{}Weierstrass in der Euklidischen Metrik formuliert, der Beweis zeigt aber, dass die Aussage auch für anderen Normen, etwa die Maximumsnorm, gilt, da ja nur die Konvergenz aller Koordinatenfolgen wichtig ist. Der Grund dafür ist die sogenannte Äquivalenz von Normen bzw. Metriken:
\label{metrik/teilfolgen:definition-6}
\begin{sphinxadmonition}{note}{Definition 4.9}



Zwei Metriken \(d_A\) und \(d_B\) auf \(X\) heissen äquivalent, falls Konstanten \(\beta \geq \alpha > 0\) existieren, sodass
\begin{equation*}
\begin{split}\forall x,y \in X: \quad \alpha  d_B(x,y) \leq d_A(x,y) \leq \beta d_B(x,y) .\end{split}
\end{equation*}
Zwei Normen \(\Vert \cdot \Vert_A\) und \(\Vert \cdot \Vert_B\) auf einem Vektorraum \(X\) heissen äquivalent, falls Konstanten \(\beta \geq \alpha > 0\) existieren, sodass
\begin{equation*}
\begin{split}\forall x  \in X: \quad \alpha  \Vert x \Vert_B  \leq \Vert x \Vert_A  \leq \Vert x \Vert_B  .\end{split}
\end{equation*}\end{sphinxadmonition}

Wir sehen sofort, dass für zwei äquivalente Normen auch die zugeordneten Metriken
\begin{equation*}
\begin{split} d_{A/B}(x,y) = \Vert x -y \Vert_{A/B}\end{split}
\end{equation*}
äquivalent sind.
Darüber hinaus sehen wir, dass die Konvergenz einer Folge in einer Metrik \(d_A\) auch die Konvergenz in einer äquivalenten Metrik \(d_B\) impliziert. Dazu können wir etwa {\hyperref[\detokenize{metrik/konvfolgen:nullfolgenmetrik}]{\sphinxcrossref{Lemma 4.1}}} verwenden.


\section{Reihen}
\label{\detokenize{metrik/reihen:reihen}}\label{\detokenize{metrik/reihen::doc}}
Eine spezielle Form von Folgen sind sogenannte Reihen, die wir als Summen von Folgen betrachten können:
\label{metrik/reihen:definition-0}
\begin{sphinxadmonition}{note}{Definition 4.10}



Sei \((X,\Vert \cdot \Vert)\) ein normierter Vektorraum und \((x_n)\) eine Folge in \(X\). Dann identifizieren wir die Reihe \(\sum_{k=0}^\infty x_k\) mit der Folge der Partialsummen
\begin{equation*}
\begin{split} s_n = \sum_{k=0}^n x_k.\end{split}
\end{equation*}
Falls die Folge \(s_n\) gegen einen Grenzwert \(s \in  X\) konvergiert, so bezeichnen wir
\begin{equation*}
\begin{split} s = \sum_{k=0}^\infty x_k\end{split}
\end{equation*}
und nennen die Reihe konvergent. Andernfalls nennen wir die Reihe divergent.
\end{sphinxadmonition}

Wir beachten, dass wir natürlich auch jede Folge \(x_n\) als Reihe
\begin{equation*}
\begin{split} x_n =\sum_{k=0}^n y_k\end{split}
\end{equation*}
schreiben können, mit \(y_0=x_0\) und \(y_k = x_k - x_{k-1}\) für \(k > 0\). Wenn wir aber eine Folge explizit angeben können (etwa auch durch Ausrechnen der Partialsummen), dann sprechen wir eher von einer Folge und behandeln sie auch so. Wenn wir aber die Partialsummen nicht ausrechnen können, benötigen wir eine speziellere Betrachtung.
\label{metrik/reihen:example-1}
\begin{sphinxadmonition}{note}{Example 4.5}



Für \(q \in \R\) betrachten wir die geometrische Reihe
\begin{equation*}
\begin{split} s_n = \sum_{k=0}^n q^k.\end{split}
\end{equation*}
Wie wir schon gesehen haben, können wir die Partialsummen explizit berechnen. Für \(q=1\) gilt \(s_n = n+1\) und die Reihe ist offensichtlich divergent. Für \(q\neq 1 \)gilt
\begin{equation*}
\begin{split} s_n = \frac{1-q^n}{1-q}.\end{split}
\end{equation*}
Nun sehen wir, dass für \(|q| <1\) gilt \(q^n \rightarrow 0\) für \(n \rightarrow 0\), also erhalten wir den Grenzwert
\begin{equation*}
\begin{split} s  = \sum_{k=0}^\infty q^k = \frac{1}{1-q}.\end{split}
\end{equation*}
Für \(|q|>1\) gilt \(|q^n| \rightarrow \infty\) für \(n \rightarrow \infty\). Wegen der Dreiecksungleichung ist
\begin{equation*}
\begin{split} |s_n| \geq \frac{|q^n|-1}{|q-1|} \rightarrow \infty,\end{split}
\end{equation*}
also divergiert die Reihe.
Im verbleibenden Fall \(q=-1\) gilt \(s_n=1\) für \(n\) gerade und \(s_n=0\) für \(n\) ungerade. Hier bleiben die Partialsummen zwar beschränkt, aber die Reihe konvergiert nicht.
\end{sphinxadmonition}
\label{metrik/reihen:example-2}
\begin{sphinxadmonition}{note}{Example 4.6}



Als zweites Beispiel betrachten wir für \(q > 0\) die Reihe
\begin{equation*}
\begin{split} s_n = \sum_{k=0}^n (k+1)^{-q}.\end{split}
\end{equation*}
Hier ist zunächst völlig unklar, ob die Reihe konvergiert oder divergiert. Für \(q=1\) sind die ersten Partialsummen \(1\), \(\frac{3}2\), \(\frac{11}6\), \(\frac{23}{12}\), \(\ldots\) Die Reihe scheint also zu divergieren, aber wir haben kein Kriterium um dies einfach zu entscheiden. Für \(q=2\) sind die ersten Partialsummen \(1\), \(\frac{5}4\), \(\frac{23}{18}\),\(\ldots\) Hier scheint es sich um Konvergenz zu handeln, aber wieder können wir nicht entscheiden, ob nicht einfach langsames Wachstum gegen unendlich vorliegt.
\end{sphinxadmonition}

Im Folgenden wollen wir einfache Kriterien zur Konvergenz einer Reihe kennenlernen. Ein einfaches Beispiel in einem vollständigen normierten Raum erhalten wir aus der Äquivalenz der Konvergenz zur Cauchy\sphinxhyphen{}Eigenschaft einer Folge:
\label{metrik/reihen:theorem-3}
\begin{sphinxadmonition}{note}{Theorem 4.6 (Cauchy\sphinxhyphen{}Kriterium)}



Eine Reihe in einem vollständigen metrischen Raum konvergiert genau dann, wenn es für alle \(\epsilon > 0\) ein \(n_0 \in \N\) gibt, sodass
\begin{equation*}
\begin{split} \forall m > n \geq n_0: \quad \Vert \sum_{k=n+1}^m x_k \Vert < \epsilon\end{split}
\end{equation*}
gilt.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Die obige Eigenschaft ist genau äquivalent dazu, dass die Folge \((s_n)\) der Partialsummen eine Cauchy\sphinxhyphen{}Folge ist, was wiederum äquivalent zur Konvergenz ist.
\end{sphinxadmonition}

Ein sehr nützliches Kriterium ist das \sphinxstyleemphasis{Majorantenkriterium}, dafür definieren wir zunächst den Begriff einer Majorante:
\label{metrik/reihen:definition-4}
\begin{sphinxadmonition}{note}{Definition 4.11}



Sei \(X\) ein vollständiger normierter Raum und \((x_n)\) eine Folge in \(X\). Dazu sei \((c_n)\) eine Folge in \(\R\) mt \(c_n \geq 0\) für alle \(n \in \N\), sodass für alle \(n \in \N\) gilt:
\begin{equation*}
\begin{split} \Vert x_n \Vert \leq c_n .\end{split}
\end{equation*}
Dann heisst \((c_n)\) bzw. \(\sum_{n=0}^\infty c_n\) Majorante von \((x_n)\) bzw. \(\sum_{n=0}^\infty x_n\).
\end{sphinxadmonition}

Mit Hilfe von Majoranten können wir wieder Konvergenzkriterien für Reihen in vollständigen normierten Räumen basierend auf Reihen in den positiven reellen Zahlen herleiten:
\label{metrik/reihen:theorem-5}
\begin{sphinxadmonition}{note}{Theorem 4.7 (Majorantenkriterium)}



Sei \(X\) ein vollständiger normierter Raum und \((c_n)\) eine Majorante von \((x_n)\). Falls \(\sum_{n=0}^\infty c_n\) konvergiert, dann konvergiert auch \(\sum_{n=0}^\infty x_n\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Wegen der Dreiecksungleichung gilt
\begin{align*}
\Vert s_m - s_n \Vert &= \Vert \sum_{k=n+1}^m x_k \Vert  \leq \sum_{k=n+1}^m  \Vert x_k \Vert \\
&\leq \sum_{k=n+1}^m c_k.\end{align*}
Wegen der Konvergenz der Reihe \(\sum_{n=0}^\infty c_n\) sind die zugehörigen Partialsummen auch eine Cauchy\sphinxhyphen{}Folge, d.h. für alle \(\epsilon > 0\) existiert ein \(n_0 \in \N\), sodass für \(m > n \geq n_0\) gilt:
\begin{equation*}
\begin{split} \Vert s_m - s_n \Vert \leq \sum_{k=n+1}^m c_k < \epsilon.\end{split}
\end{equation*}
Damit ist auch \((s_n)\) eine Cauchy\sphinxhyphen{}Folge und somit konvergent.
\end{sphinxadmonition}

Wir beachten, dass wir die Konvergenz von \(s_n=\sum_{k=0}^n c_k\) für \(c_k \geq 0\) relativ einfach entscheiden können. \((s_n)\) ist dann eine monoton steigende Folge, diese konvergiert genau dann wenn sie beschränkt ist. Können wir also ein \(C > 0\) finden mit
\begin{equation*}
\begin{split} \sum_{k=0}^n c_k \leq C \qquad \forall n \in \N ,\end{split}
\end{equation*}
dann konvergiert die Reihe.
Das Majorantenkriterium können wir speziell für Reihen in \(X=\R\) anwenden. Es gilt, dass \(\sum_{n=0}^\infty x_n\) konvergiert, falls  \(\sum_{n=0}^\infty |x_n|\)  konvergiert, da klarerweise \(|x_n|\) eine Majorante für \(x_n\) ist.Die Umkehrung stimmt aber nicht immer. Wir nennen deshalb eine Reihe in \(\R\) bedingt konvergent, wenn \(\sum_{n=0}^\infty x_n\) konvergiert, aber \(\sum_{n=0}^\infty |x_n|\)  divergiert. Falls auch \(\sum_{n=0}^\infty |x_n|\) konvergiert, nennen wir die Reihe absolut konvergent. Bei bedingt konvergenten Reihen ist Vorsicht geboten, da eine Umordnung der Reihe, d.h. \(\sum_{n=0}^\infty x_{\pi(n)}\) für eine bijektive Abbildung \(\pi: \N \rightarrow \N\) einen anderen Grenzwert liefern kann.
\label{metrik/reihen:example-6}
\begin{sphinxadmonition}{note}{Example 4.7}



Das Majorantenkriterium können wir nun zur Untersuchung der Konvergenz von \(\sum_{n=0}^\infty (n+1)^{-q}\) verwenden.
Sei \(q > 1\), dann wählen wir \(c_n = 2^{-q \ell}\), wobei \(\ell\) durch
\begin{equation*}
\begin{split} 2^\ell \leq n+1 \leq 2^{\ell+1}\end{split}
\end{equation*}
bestimmt ist.Dann gilt
\begin{equation*}
\begin{split} \sum_{n=0}^\infty c_n = \sum_{\ell=0}^\infty \sum_{n=2^\ell-1}^{2^{\ell+1}} c_n = \sum_{\ell=0}^\infty \sum_{n=2^\ell-1}^{2^{\ell+1}-2} 2^{-q\ell} =  \sum_{\ell=0}^\infty  2^{(1-q)\ell} = \sum_{\ell=0}^\infty  (2^{1-q})^\ell.\end{split}
\end{equation*}
Für \(q > 1\) ist \(2^{1-q} < 1\), d.h. die geometrische Reihe rechts konvergiert und damit auch die Majorantenreihe.

Im Fall \(q=1\) können wir mit dem Cauchy\sphinxhyphen{}Kriterium zeigen, dass die Reihe divergiert. Wäre sie konvergent, würde insbesondere \(s_{2n} -s_n\) gegen Null konvergieren, es gilt aber
\begin{equation*}
\begin{split} s_{2n} -s_n = \sum_{k=n+1}^{2n} \frac{1}k \geq \sum_{k=n+1}^{2n} \frac{1}{2n} = \frac{1}2.\end{split}
\end{equation*}\end{sphinxadmonition}

Für die absolute Konvergenz von Reihen in \(\R\) können wir nun etwa mit der geometrischen Reihe vergleichen, deren Konvergenz wir aus dem obigen Beispiel schon verstehen:
\label{metrik/reihen:theorem-7}
\begin{sphinxadmonition}{note}{Theorem 4.8 (Quotientenkriterium)}



Sei \((x_n)\) eine reelle Folge, sodass für ein \(n_0 \in \N\) und ein \(q \in [0,1)\) gilt:
\begin{equation*}
\begin{split}x_n \neq 0,\qquad \left\vert \frac{x_{n+1}}{x_n} \right\vert \leq q \qquad \forall~n \geq n_0.\end{split}
\end{equation*}
Dann konvergiert \(\sum_{n=0}^\infty x_n\) absolut.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Wegen \(|x_{n+1}| \leq q~|x_n|\) können wir induktiv auch
\begin{equation*}
\begin{split}|x_n| \leq q^{n-n_0} |x_{n_0}|\end{split}
\end{equation*}
folgern. Wir wählen also als Majorante \(c_n= |x_n|\) für \(n \leq n_0\) und \(c_n = q^{n-n_0} |x_{n_0}|\) für
\(n > n_0.\) Dann gilt
\begin{equation*}
\begin{split} \sum_{n=0}^\infty c_n = \sum_{n=0}^{n_0-1} |x_n| + \sum_{n=n_0}^\infty q^{n-n_0} |x_{n_0}| =\sum_{n=0}^{n_0-1} |x_n| +  |x_{n_0}| \sum_{n=0}^\infty q^{n} = =\sum_{n=0}^{n_0-1} |x_n| +  \frac{|x_{n_0}|}{1-q} < \infty.\end{split}
\end{equation*}
Also folgt die Konvergenz aus dem Majorantenkriterium.
\end{sphinxadmonition}
\label{metrik/reihen:example-8}
\begin{sphinxadmonition}{note}{Example 4.8}



Wir betrachten die Reihe \(\sum_{n=0}^\infty \frac{1}{n+1} 2^{-n}\) für \(q \in \R\). Hier ist
\begin{equation*}
\begin{split}  \left\vert \frac{x_{n+1}}{x_n} \right\vert  = \frac{n+1}{n+2} \frac{1}2 \leq \frac{1}2\end{split}
\end{equation*}
für alle \(n \in \N\), deshalb konvergiert die Reihe.
\end{sphinxadmonition}

Zum Abschluss zeigen wir noch eine recht naheliegende Eigenschaft
\label{metrik/reihen:lemma-9}
\begin{sphinxadmonition}{note}{Lemma 4.3}



Sei \(\sum_{n=0}^\infty x_n\) eine konvergente reelle Reihe. Dann ist \((|x_n|)\) eine Nullfolge.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(\epsilon >0 \). Da die Partialsummen eine Cauchy\sphinxhyphen{}Folge sind, gibt es ein \(n_0 \in \N\), sodass für \(m>n\geq n_0\) gilt
\begin{equation*}
\begin{split}|\sum_{k=n+1}^m x_k| < \epsilon.\end{split}
\end{equation*}
Nun können wir \(m=n+1\) und \(n\) beliebig wählen und erhalten \(|x_n|<\epsilon\) für alle \(n > n_0\). Also ist \((|x_n|)\)
eine Nullfolge.
\end{sphinxadmonition}


\section{Potenzreihen und Exponentialfunktion}
\label{\detokenize{metrik/potenzreihen:potenzreihen-und-exponentialfunktion}}\label{\detokenize{metrik/potenzreihen::doc}}
Potenzreichen sind spezielle Reihen von der Form \(\sum_{n=0}^\infty a_n (x-x_0)^n\), mit einer gegebenen reellen Koeffizientenfolge \((a_n)\) und einem Basispunkt \(x_0 \in \R\). Der Unterschied zu einfachen Reihen ist, dass wir verschiedene Werte für das Argument \(x\in \R\) einsetzen möchten, die Potenzreihe ist dann also eine Funktion von \(x\), idealerweise von \(\R\) nach \(\R\). Allerdings ist nicht klar für welche \(x \in \R\) die Reihe konvergiert.
\label{metrik/potenzreihen:example-0}
\begin{sphinxadmonition}{note}{Example 4.9}



Für \(x \in (-1,1)\) können wir die Funktion
\begin{equation*}
\begin{split} f: x \mapsto \sum_{n=0}^\infty x^n = \frac{1}{1-x}\end{split}
\end{equation*}
über die (geometrische) Potenzreihe definieren. Für \(|x| \geq 1\) konvergiert die Reihe nicht. Wir beachten aber, dass die Definition \(\frac{1}{1-x}\) für \(x\neq 1\) möglich ist.
\end{sphinxadmonition}

Wie sieht nun allgemein der Definitionsbereich einer über eine Potenzreihe definierten Funktion aus, d.h. für welche Argumente \(x\) konvergiert die Potenzreihe ? Wir sehen, dass die Potenzreihe für \(x=x_0\) immer definiert ist und den Wert \(0\) ergibt. Es ist naheliegend, dass die Konvergenz für \(x\) nahe bei \(x_0\) eher gegeben ist als bei größerem Abstand. Zur einfacheren Schreibweise werden wir im Folgenden Resultate für \(x_0 = 0\) formulieren, dies ist aber keine Einschränkung, denn für \(x_0 \neq 0\) verwenden wir einfach die Resultate für \(z=x-x_0\).
\label{metrik/potenzreihen:theorem-1}
\begin{sphinxadmonition}{note}{Theorem 4.9}



Sei \(\sum_{n=0}^\infty a_n x^n\) eine Potenzreihe. dann gibt es ein eindeutiges \(r \in [0,\infty]\), den sogenannten Konvergenzradius der Reihe, mit folgenden Eigenschaften:
\begin{itemize}
\item {} 
\(i)\) Die Reihe konvergiert absolut für \(|x| < r\).

\item {} 
\(ii)\) Die Reihe divergiert für \(|x| > r\).

\end{itemize}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Wir definieren
\begin{equation*}
\begin{split} r:= \sup\{ |x| ~|~ \exists x \in \R, \sum_{n=0}^\infty a_n x^n \text{ konvergiert} \}.\end{split}
\end{equation*}
Sei \(|x|<r\), dann gibt es ein \(\overline{x} \in \R\) mit \(|\overline{x}|>|x|\), sodass \(\sum_{n=0}^\infty a_n \overline{x}^n\) konvergiert. Damit ist die Folge \(|a_n|~ |\overline{x}|^n\) eine Nullfolge und insbesondere beschränkt. Sei \(C\) die obere Schranke, dann gilt
\begin{equation*}
\begin{split}|a_n|~ |x|^n \leq C \left( \frac{|x|}{\overline{x}} \right)^n.\end{split}
\end{equation*}
Wegen \(\frac{|x|}{\overline{x}} < 1\) haben wir eine konvergente geometrische Reihe als Majorante und damit istdie Potenzreihe \(\sum_{n=0}^\infty a_n x^n \) absolut konvergent für alle \(x\) mit \(|x|<r\).
Aus der Definition von  \(r\) ist umgekehrt klar, dass die Potenzreihe divergiert für \(|x|>r\).
\end{sphinxadmonition}
\label{metrik/potenzreihen:example-2}
\begin{sphinxadmonition}{note}{Example 4.10}



Wir betrachten \(\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1}x^{n+1} = \sum_{n=1}^\infty \frac{(-1)^{n-1}}{n}x^{n}. \)
Die geometrische Reihe \(\sum_{n=0}^\infty |x|^n\) ist eine Majorante, deshalb sehen wir sofort die Konvergenz für \(|x| < 1\), d.h. es folgt \(r \geq 1\). Andererseits wissen wir schon, dass die Reihe für \(x=-1\), d.h. \(-
\sum_{n=0}^\infty \frac{1}{n+1}\) nicht konvergiert. Damit folgt \(r=1\).
\end{sphinxadmonition}

Wir können auch das Quotientenkriterium auf Potenzreihen anwenden und sehen, dass Konvergenz vorliegt, wenn
\begin{equation*}
\begin{split} \forall n \geq n_0: ~\left\vert \frac{a_{n+1} x^{n+1}}{a_n x^n} \right\vert \leq q < 1\end{split}
\end{equation*}
gilt. Dies ist gleichbedeutend zu
\begin{equation*}
\begin{split} |x| \leq q \frac{|a_n|}{|a_{n+1}|}\end{split}
\end{equation*}
für alle \(n \geq n_0\), und wir sehen also, dass die Reihe für
\begin{equation*}
\begin{split} |x|<\inf_{n \geq n_0} \frac{|a_n|}{|a_{n+1}|}\end{split}
\end{equation*}
immer konvergiert. Also gilt auch
\begin{equation*}
\begin{split} r \geq \inf_{n \geq n_0} \frac{|a_n|}{|a_{n+1}|} .\end{split}
\end{equation*}
Da wir \(n_0\) beliebig groß wählen können, gilt sogar
\begin{equation*}
\begin{split} r \geq \lim_{n_0 \rightarrow \infty} \inf_{n \geq n_0} \frac{|a_n|}{|a_{n+1}|} .\end{split}
\end{equation*}
Dieser Grenzwert existiert, da \( \inf_{n \geq n_0} \frac{|a_n|}{|a_{n+1}|}\) eine monoton fallende Folge ist (das Infimum wird über kleiner werdende Menge genommen, die nach unten beschränkt ist).
Dies führt uns nebenbei  auf die Definition des Limes inferior, für eine reelle Folge \(x_n\) definieren wir
\begin{equation*}
\begin{split} \lim\inf_{n \rightarrow \infty} x_n = \lim_{n \rightarrow \infty} \inf_{m \geq n} x_m .\end{split}
\end{equation*}
Analog definieren wir auch den Limes superior
\begin{equation*}
\begin{split} \lim\sup_{n \rightarrow \infty} x_n = \lim_{n \rightarrow \infty} \sup_{m \geq n} x_m .\end{split}
\end{equation*}
Man kann sogar zeigen, dass eine Folge konvergiert, wenn Limes inferior und Limes superior existieren und es gilt \(\liminf x_n = \lim\sup x_n\).


\subsection{Exponentialfunktion und Multiplikation von Reihen}
\label{\detokenize{metrik/potenzreihen:exponentialfunktion-und-multiplikation-von-reihen}}
Wir definieren nun die Exponentialfunktion \(\exp(x)\) oder auch \(e^x\) über eine Potenzreihe als
\begin{equation*}
\begin{split} e^x:= \sum_{n=0}^\infty \frac{1}{n!} x^n\end{split}
\end{equation*}
wobei \(0!=1\) und \(n! = \prod_{i=1}^n i\) für \(n > 0\). Wenden wir das Quotientenkriterium an, so folgt
\begin{equation*}
\begin{split} r \geq \lim\inf_n n+1 = \infty,\end{split}
\end{equation*}
also konvergiert die Exponentialreihe für alle \(x \in \R\).
Analog können wir auch Sinus\sphinxhyphen{} und Kosinusfunktionen als Verwandte der Exponentialfunktion über Potenzreihen definieren. Es gilt
\begin{align*}
\sin(x) &= \sum_{n=0}^\infty (-1)^n \frac{x^{2n+1}}{(2n+1)!} \\
\cos(x) &= \sum_{n=0}^\infty (-1)^n \frac{x^{2n }}{(2n )!}.
\end{align*}
Auch hier sehen wir, z.B. aus dem Quotientenkriterium, dass der Konvergenzradius \(r=\infty\) ist.

Wir haben schon gesehen, dass die Summe von konvergenten Reihen wieder konvergent ist. Hat die Reihe \(\sum_{n=0}^\infty a_n x^n\) den Konvergenzradius \(r_1\) und die Reihe  \(\sum_{n=0}^\infty b_n x^n\) den Konvergenzradius \(r_2\), dann konvergiert  \(\sum_{n=0}^\infty (a_n+b_n) x^n\)  mit  Konvergenzradius \(r \geq \min\{r_1,r_2\}.\) Bei unterschiedlichen Vorzeichen kann der Konvergenzradius der Summe natürlich viel größer als die Summe zu sein, wie wir am Beispiel \(a_n = 1\), \(b_n = -1\) für alle \(n\) sehen.
Konvergente Reihen in \(\R\) können wir auch multiplizieren, dementsprechend gilt dasselbe für Potenzreihen, diese bleiben auch Potenzreihen, es gilt innerhalb der Konvergenzradien
\begin{equation*}
\begin{split} \left( \sum_{n=0}^\infty a_n x^n \right) \left( \sum_{n=0}^\infty b_n x^n \right) = \sum_{n=0}^\infty c_n x^n\end{split}
\end{equation*}
mit geeigneten Koeffizienten \(c_n\). Wir beginnen mit den endlichen Summen
\begin{align*}\left( \sum_{n=0}^m a_n x^n \right) \left( \sum_{n=0}^m b_n x^n \right) =& (a_0 + a_1x+ \ldots + a_m x^m)(b_0 + b_1x+ \ldots + b_m x^m) \\
=& a_0 b_0 + (a_0b_1+a_1b_0)x + (a_0 b_2 + a_1 b_1 + a_2 b_0)x^2 + \ldots \\ & + \sum_{j=0}^m a_j b_{m-j} x^m + \ldots +a_m b_m x^m .
\end{align*}
Betrachten wir nun die endliche Reihe bis \(m+1\), so kommen nur Terme dazu, die Vielfache von \(x^j\) mit \(j > m\) sind. Die Koeffizienten der ersten \(m\) Terme ändern sich nicht mehr. Damit zeigen wir induktiv
\begin{equation*}
\begin{split} c_n =  \sum_{j=0}^n a_j b_{n-j}  ,\end{split}
\end{equation*}
man bezeichnet dies auch als Faltung der Koeffizienten \((a_j), (b_j)\).
Als Anwendung können wir die Exponentialfunktion betrachten, etwa
\begin{equation*}
\begin{split} e^x e^{-x} = \sum_{n=0}^\infty \frac{1}{n!} x^n  \sum_{n=0}^\infty \frac{(-1)^n}{n!} x^n .\end{split}
\end{equation*}
Hier gilt  dann
\begin{equation*}
\begin{split} c_n =  \sum_{j=0}^n \frac{(-1)^{n-j}}{j! (n-j)!} .\end{split}
\end{equation*}
Für \(n=0\) erhalten wir \(c_0 =1\), für \(n > 0\) können wir den Binomischen Lehrsatz verwenden:
\label{metrik/potenzreihen:lemma-3}
\begin{sphinxadmonition}{note}{Lemma 4.4}



Für \(x,y \in \R\) und \(n \in \N\setminus\{0\}\) gilt
\begin{equation*}
\begin{split} (x+y)^n = \sum_{j=0}^\infty (\begin{matrix} n\\j \end{matrix} ) ~  x^j~ y^{n-j}\end{split}
\end{equation*}
mit dem Binomialkoeffizienten
\begin{equation*}
\begin{split}  (\begin{matrix} n\\j \end{matrix} ) = \frac{n!}{j! (n-j)!}.\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Wir beweisen das Resultat durch vollständige Induktion. Für \(n=0\) ist
\begin{equation*}
\begin{split}(x+y)^0 = 1 =  \sum_{j=0}^0 (\begin{matrix} 0\\0 \end{matrix} ) ~  x^0~ y^0 .\end{split}
\end{equation*}
Für \(n=1\) ist
\begin{equation*}
\begin{split} (x+y) = (\begin{matrix} 0\\0 \end{matrix} ) ~  x^1~ y^0 + (\begin{matrix} 1\\0 \end{matrix} ) ~  x^0~ y^1.\end{split}
\end{equation*}
Nun nehmen wir an das Resultat gilt für ein \(n \in \N\) und folgern die Gültigkeit für \(n+1\).
Wir haben
\begin{align*}
(x+y)^{n+1} &= (x+y) (x+y)^n =  (x+y) \sum_{j=0}^n (\begin{matrix} n\\j \end{matrix} ) ~  x^j~ y^{n-j} \\
&=\sum_{j=0}^n (\begin{matrix} n\\j \end{matrix} ) ~  x^{j+1}~ y^{n-j} +\sum_{j=0}^n (\begin{matrix} n\\j \end{matrix} ) ~  x^j~ y^{n+1-j}  \\
&= \sum_{j=1}^{n+1} (\begin{matrix} n\\j \end{matrix} ) ~  x^{j}~ y^{n+1-j}  + \sum_{j=0}^n (\begin{matrix} n\\j \end{matrix} ) ~  x^j~ y^{n+1-j} \\
&= (\begin{matrix} n+1\\ 0 \end{matrix} ) y^{n+1}+ \sum_{j=1}^{n } \left( (\begin{matrix} n\\j \end{matrix} ) +  (\begin{matrix} n\\j-1 \end{matrix} ) \right) ~  x^{j}~ y^{n+1-j} + (\begin{matrix} 0\\ 0 \end{matrix} ) x^{n+1}
\end{align*}
wobei wir verwendet haben, dass
\begin{equation*}
\begin{split}1 =
\begin{pmatrix} n+1\\ 0 \end{pmatrix} 
= \begin{pmatrix} n\\ 0 \end{pmatrix}.\end{split}
\end{equation*}
Nun rechnen wir noch leicht nach, dass
\begin{equation*}
\begin{split}\begin{pmatrix} n\\j \end{pmatrix} +  \begin{pmatrix} n\\j-1 \end{pmatrix}
=
\begin{pmatrix} n+1\\j \end{pmatrix}\end{split}
\end{equation*}
gilt und erhalten damit die binomische Formel auch für \(n+1\).
\end{sphinxadmonition}

Setzen wir dies in die obige Formel für \(e^x e^-x\) ein, so folgt für \(n > 0\)
\begin{equation*}
\begin{split}c_n = \frac{1}{n!}(-1+1)^n = 0 .\end{split}
\end{equation*}
Also ist tatsächlich
\begin{equation*}
\begin{split} e^{-x} = \frac{1}{e^x}.\end{split}
\end{equation*}
Mit der gleichen Formel für \(e^x e^x\) erhalten wir analog
\begin{equation*}
\begin{split} c_n = \frac{1}{n!}(1+1)^n =  \frac{1}{n!}(2)^n .\end{split}
\end{equation*}
Setzen wir dies ein, so folgt \(e^x e^x = e^{2x}\).
Allgemeiner können wir mit Hilfe der binomischen Formel auch \(e^x e^y\) berechnen. Für das Produkt von Potenzreihen mit verschiedenen Argumenten erhalten wir analog zur Rechnung oben
\begin{equation*}
\begin{split}\left( \sum_{n=0}^\infty  a_n x^n \right) \left( \sum_{n=0}^\infty b_y x^n \right) =
\sum_{n=0}^\infty \sum_{k=0}^n a_k b_{n-k} x^k y^{n-k}.\end{split}
\end{equation*}
Damit folgt
\begin{equation*}
\begin{split} e^x e^y = \sum_{n=0}^\infty \sum_{k=0}^n \frac{1}{k!} \frac{1}{(n-k)!} x^k y^{n-k} =
\sum_{n=0}^\infty \frac{1}{n!} \sum_{k=0}^n (\begin{matrix} n\\ k \end{matrix} )  x^k y^{n-k}.\end{split}
\end{equation*}
Setzen wir die Binomische Formel ein, so erhalten wir
\begin{equation*}
\begin{split} e^x e^y = \sum_{n=0}^\infty \frac{1}{n!}  (x+y)^n = e^{x+y},\end{split}
\end{equation*}
die charakteristische Eigenschaft einer Exponentialfunktion.


\chapter{Stetigkeit}
\label{\detokenize{stetigkeit/stetigkeit:stetigkeit}}\label{\detokenize{stetigkeit/stetigkeit::doc}}
Wir haben uns bei den Potenzreihen schon mit Funktionen beschäftigt, nun wollen wir diese näher betrachten und das Konzept der Stetigkeit einführen. Bei reellen Funktionen verstehen wir stetige Funktionen als solche ohne Sprungstellen und Singularitäten, diese Funktionen kann man aufzeichnen ohne den Stift vom Blatt zu lösen. Allgemeiner können wir das Konzept von Folgenstetigkeit definieren:
\label{stetigkeit/stetigkeit:definition-0}
\begin{sphinxadmonition}{note}{Definition 5.1}



Sei \(f: X \rightarrow Y\) eine Abbildung zwischen den metrischen Räumen \((X,d_X)\) und \((Y,d_Y)\). Dann heisst \(f\) folgenstetig in \(x \in X\), wenn für jede Folge \((x_n) \subset X\) mit Grenzwert \(x\) auch \(f(x_n) \rightarrow f(x)\) gilt. Die Funktionen heisst folgenstetig in einer Menge \(M \subset X\), wenn sie in jedem \(x \in M\) folgenstetig ist.
\end{sphinxadmonition}
\label{stetigkeit/stetigkeit:example-1}
\begin{sphinxadmonition}{note}{Example 5.1}



Die Funktion \(f: \R \rightarrow \R, x \mapsto x^2\) ist stetig, da für \(x_n \rightarrow x\) auch \(x_n^2 \rightarrow x\) konvergiert
\end{sphinxadmonition}
\label{stetigkeit/stetigkeit:example-2}
\begin{sphinxadmonition}{note}{Example 5.2}



Die sogenannte Heaviside\sphinxhyphen{}Funktion \(H: \R \rightarrow \R\) mit \(H(x) = 1\) für \(x \geq 0\) und \(H(x)=0\) für \(x < 0\) ist im Punkt \(x =0\) unstetig, sonst stetig. Wählen wir die Folge \(x_n = -\frac{1}n\), so ist \(H(x_n) = 0\) für alle \(n\), damit auch \(\lim_n H(x_n) = 0 \neq H(0)\).
\end{sphinxadmonition}

Eine alternative Definition von Stetigkeit basiert auf dem Verhalten der Funktion in \(\epsilon\)\sphinxhyphen{}Umgebungen:
\label{stetigkeit/stetigkeit:definition-3}
\begin{sphinxadmonition}{note}{Definition 5.2}



Eine Funktion \(f: X \rightarrow Y\) heisst stetig in \(x \in X\), wen
\begin{equation*}
\begin{split} \forall \epsilon > 0 ~\exists \delta > 0 ~\forall \tilde{x} \in U_\delta(x): f(\tilde x) \in U_\epsilon(f(x)),\end{split}
\end{equation*}
wobei
\begin{align*}
U_\delta(x) &= \{\tilde x \in X~|~d_X(x,\tilde x) < \delta \} \\
U_\epsilon(f(x)) &= \{\tilde y \in Y~|~d_Y(f(x),\tilde y) < \epsilon \}.
\end{align*}
Die Funktion \(f\) heisst stetig in \(M \subset X\), wenn sie in allen Punkten \(x\in M\) stetig ist.
\end{sphinxadmonition}
\label{stetigkeit/stetigkeit:theorem-4}
\begin{sphinxadmonition}{note}{Theorem 5.1}



Eine Funktion \(f: X \rightarrow Y\) ist genau dann stetig, wenn sie folgenstetig ist.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(f\) stetig in \(X\) und \(x_n\) eine Folge mit Grenzwert \(x\). Dann gibt es ein \(n_0 \in \N\), sodass für alle \(n \geq n_0\) gilt: \( d_X(x,x_n) < \delta\) und daraus folgt \( d_y(f(x),f(x_n)) < \epsilon\). Damit ist \(f\) folgenstetig.

Nehmen wir umgekehrt an \(f\) sei nicht stetig. Dann gibt es für ein \(\epsilon > 0\) kein \(\delta > 0\) mi
\begin{equation*}
\begin{split} f(U_\delta(x)) \subset U_\epsilon (f(x)).\end{split}
\end{equation*}
Insbesondere gibt es für jedes \(n \in \N\) ein \(x_n\) mit \(d_X(x,x_n) < \frac{1}n\) und \(d_Y(f(x),f(x_n)) > \epsilon\). Damit konvergiert \(x_n\) gegen \(x\) aber nicht \(f(x_n)\) gegen \(f(x)\), also ist \(f\) nicht folgenstetig.
\end{sphinxadmonition}

Mit der Äquivalenz der Definitionen können wir verschiedene Eigenschaften von stetigen Funktionen beweisen, indem wir die jeweils passende Eigenschaft benutzen:
\label{stetigkeit/stetigkeit:theorem-5}
\begin{sphinxadmonition}{note}{Theorem 5.2 (Seien \protect\((X,d_X)\protect\), \protect\((Y,d_Y)\protect\), \protect\((Z,d_Z)\protect\) metrische Räume und \protect\(f: X \rightarrow Y\protect\), \protect\(g: Y \rightarrow Z\protect\) so, dass \protect\(f\protect\) stetig bei \protect\(x\protect\) und \protect\(g\protect\) stetig bei \protect\(y=f(x)\protect\) ist. Dann ist die Hintereinanderausführung \protect\(f \circ g\protect\) stetig bei \protect\(x\protect\).)}


\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \((x_n)\) eine Folge mit \(x_n \rightarrow x\). Dann gilt wegen der Stetigkeit von \(f\) auch \(y_n=f(x_n) \rightarrow f(x) = y\). Wegen der Stetigkeit von \(g\) bei \(y=f(x)\) folgt dann \(g(y_n) \rightarrow g(y)\), also \(g\circ f(x_n) \rightarrow g \circ f(x)\), also ist \(g \circ f\) stetig bei \(x\).
\end{sphinxadmonition}
\label{stetigkeit/stetigkeit:theorem-6}
\begin{sphinxadmonition}{note}{Theorem 5.3 (Sei \protect\((X,d)\protect\) ein metrischer Raum und \protect\(f: X \rightarrow \R^n\protect\), \protect\(g: X \rightarrow \R^n\protect\) seien stetig bei \protect\(x \in X\protect\). Dann sind auch \protect\(f +g \protect\) und \protect\(f \circ g\protect\) stetig bei \protect\(x\protect\).)}


\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Für \(x_n \rightarrow x\) folgt \(f(x_n) \rightarrow f(x)\) und \(g(x_n) \rightarrow g(x)\). Damit gilt wegen den Eigenschaften konvergenter Folgen auch \(f(x_n) + g(x_n) \rightarrow f(x) + g(x)\) und
\(f(x_n) \cdot g(x_n) \rightarrow f(x) \cdot g(x)\). Also sind \(f+g\) und \(f \cdot g\) stetig.
\end{sphinxadmonition}

\textbackslash{}begin\{cor\}
Alle Polynome \(p: \R  \rightarrow \R\), \( x \mapsto \sum_{j=0}^n a_j x^j\) sind stetig auf \(\R\).
\textbackslash{}end\{cor\}
Eine etwas stärkere Eigenschaft als Stetigkeit ist die Hölder\sphinxhyphen{} oder auch Lipschitz\sphinxhyphen{}Stetigkeit:
\label{stetigkeit/stetigkeit:definition-7}
\begin{sphinxadmonition}{note}{Definition 5.3}



Eine Funktion \(f: X \rightarrow Y\) zwischen metrischen Räumen heisst Hölder\sphinxhyphen{}stetig mit Exponent \(0 < \alpha \leq 1\),wenn ein \(c \in \R\) existiert, sodass
\begin{equation*}
\begin{split} \forall x,y \in X: d_Y(f(x),f(y)) \leq c d_X(x,y)^\alpha\end{split}
\end{equation*}
gilt. Im Fall \(\alpha = 1\) heisst \(f\) Lipschitz\sphinxhyphen{}stetig.
Die Funktion heisst lokal Hölder\sphinxhyphen{}stetig mit Exponent \(\alpha\) (bzw. lokal Lipschitz\sphinxhyphen{}stetig, falls \(\alpha =1\)), wenn für alle \(x \in X\) eine Umgebung \(U_\epsilon(x)\) und eine Konstante \(C=C(x)\) existiert, sodass
\begin{equation*}
\begin{split} \forall y \in U_\epsilon(x): d_Y(f(x),f(y)) \leq C d_X(x,y) .\end{split}
\end{equation*}\end{sphinxadmonition}

Es ist klar, dass Hölder\sphinxhyphen{}Stetigkeit auch lokale Hölder\sphinxhyphen{}Stetigkeit impliziert. Lokale Hölder\sphinxhyphen{}Stetigkeit impliziert immer auch Stetigkeit:
\label{stetigkeit/stetigkeit:theorem-8}
\begin{sphinxadmonition}{note}{Theorem 5.4}



Sei \(f: X \rightarrow Y\) lokal Hölder\sphinxhyphen{}stetig mit Exponent \(\alpha > 0\). Dann ist \(f\) stetig in \(x\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(x \in X\) und \(x_n\) eine Folge, die gegen \(x\) konvergiert. Dann gibt es für jedes \(\epsilon > 0\) ein \(n_0\), sodas
\begin{equation*}
\begin{split} d(x,x_n) < \left( \frac{\epsilon}{C} \right)^{\frac{1}\alpha} .\end{split}
\end{equation*}
Daraus folgt für alle \$n \textbackslash{}geq n\_0
\begin{equation*}
\begin{split} d_Y(f(x),f(x_n)) < \epsilon ,\end{split}
\end{equation*}
also gilt \(f(x_n) \rightarrow f(x). \)
\end{sphinxadmonition}
\label{stetigkeit/stetigkeit:example-9}
\begin{sphinxadmonition}{note}{Example 5.3}



Sei \(m \in \N\), dann ist die Funktion \(f(x)=x^m\) lokal Lipschitz\sphinxhyphen{}stetig mit \(C(x) = ( m+1) |x|^{m-1}\).
\end{sphinxadmonition}
\label{stetigkeit/stetigkeit:example-10}
\begin{sphinxadmonition}{note}{Example 5.4}



Die Funktion \(f(x)=|x|\) ist global Lipschitz\sphinxhyphen{}stetig mit \(C=1\).
\end{sphinxadmonition}


\section{Eigenschaften stetiger reeller Funktionen}
\label{\detokenize{stetigkeit/eigenschaften:eigenschaften-stetiger-reeller-funktionen}}\label{\detokenize{stetigkeit/eigenschaften::doc}}
Im Fall reeller Funktionen \(f: \R \rightarrow \R\) hat die Stetigkeit einige spezielle Eigenschaften. Intuitiv ist eine reelle Funktion stetig, wenn wir sie zeichnen können, ohne den Stift vom Blatt zu nehmen. Dies bedeutet aber auch, um von einem Funktionswert zum anderen zu kommen, müssen wir alle dazwischen passieren. Mathematisch formalisiert dies der sogenannte \{\textbackslash{}em Zwischenwertsatz\}:
\label{stetigkeit/eigenschaften:theorem-0}
\begin{sphinxadmonition}{note}{Theorem 5.5}



Sei  \(a<b \in \R\), \(f: [a,b] \rightarrow \R\) stetig und wir definieren
\begin{equation*}
\begin{split} A = \min\{f(a),f(b)\}, \quad B = \max\{f(a),f(b)\}.\end{split}
\end{equation*}
Dann gibt es für jedes \(y \in [A,B]\) ein \(x \in [a,b]\) mit \(f(x) =y\), d.h. \(f([a,b]) \supset [A,B]\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(y \in [A,B]\). Wir nehmen an \(f(a) \leq f(b)\), der Fall \(f(b) \geq f(a)\) ist analog. Wir konstruieren wieder Folgen \([a_n,b_n]\) durch Bisektion. Sei dazu \(a_0=a, b_0=b\). Dann setzen wir \(c_0 = \frac{a_0+b_0}2\) und berechnen \(f(c_0)\). Ist \(f(c_0) > C\), dann setzen wir \(a_1=a_0\), \(b_1=c_0\), andernfalls \(a_1=c_0\), \(b_1=b_0\). Gehen wir so weiter, dann konstruieren wir eine monoton wachsende Folge \(a_n\) und eine monoton fallende Folge \(b_n\) mit \(a_n > b_n\), \(|b_n-a_n|=\frac{b-a}{2^n}\), \(f(a_n) \leq C\), \(f(b_n) \geq C\). Somit folgt, das \(a_n\) und \(b_n\) konvergieren und da \(|b_n-a_n|\rightarrow 0\) gilt, haben die beiden den gleichen Grenzwert \(x \in [a,b]\).

Wegen der Stetigkeit von \(f\) folg
\begin{equation*}
\begin{split} \lim_{n \rightarrow \infty} f(a_n) = \lim_{n \rightarrow \infty} f(b_n) = C.\end{split}
\end{equation*}
Wäre nun \(f(x) > C\), dann gilt mit \(\epsilon = f(x) - C\) imme
\begin{equation*}
\begin{split} |f(x) - f(a_n)| = f(x) - f(a_n) \geq f(x) - C = \epsilon\end{split}
\end{equation*}
ein Widerspruch zur Konvergenz von \(a_n\). Analog würde \(f(x) < C\) der Konvergenz von \(b_n\) widersprechen. Also folgt\(f(x) = C\).
\end{sphinxadmonition}

\textbackslash{}begin\{cor\}
Für \(a <b \in \R\) gilt: \(f([a,b])\) ist ein Intervall in \(\R\).
\textbackslash{}end\{cor\}

\textbackslash{}begin\{cor\}
Sei \(f\) stetig und gibt es \(x_+\) und \(x_-\) mit \(f(x_+) \geq 0\) und \(f(x_-) \leq 0\), dann hat \(f\) zumindest eine Nullstelle zwischen \(x_-\) und \(x_+\).
\textbackslash{}end\{cor\}

Für stetige Funktionen können wir nicht nur Nullstellen finden, sondern auch Minima und Maxima:
\label{stetigkeit/eigenschaften:definition-1}
\begin{sphinxadmonition}{note}{Definition 5.4}



Sei \(f: X \rightarrow \R\) eine reellwertige Funktion. Dann heisst \(\overline{x} \in X\) Minimalstelle (und \(f(\overline{x})\) Minimalwert), wen
\begin{equation*}
\begin{split} \forall x \in X: f(\overline{x}) \leq f(x).\end{split}
\end{equation*}
Darüber hinaus heisst \(\overline{x} \in X\) Maximalstelle (und \(f(\overline{x})\) Maximalwert), wen
\begin{equation*}
\begin{split} \forall x \in X: f(\overline{x}) \geq f(x).\end{split}
\end{equation*}\end{sphinxadmonition}

Wichtig für die Existenz von Minimal\sphinxhyphen{} bzw. Maximalstelle ist das Konzept der Kompaktheit:
\label{stetigkeit/eigenschaften:definition-2}
\begin{sphinxadmonition}{note}{Definition 5.5}



Eine Teilmenge \(M \subset X\) heisst kompakt, wenn jede Folge \((x_n)\) mit \(x_n \in M\) für \(n \in \N\) eine konvergente Teilfolge hat, deren Grenzwert in \(M\) liegt.
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}
Proof.  Ist \(M\) abgeschlossen und beschränkt, dann folgt aus dem Satz von Bolzano\sphinxhyphen{}Weierstrass, dass jede Folge in \(M\) eine konvergente Teilfolge \((x_{n_k})\) hat. Da \(M\) abgeschlossen ist, ist \(\R^n \setminus M\) offen. D.h. für jedes \(z \in \R^n \setminus M\) gibt es eine \(\epsilon\)\sphinxhyphen{}Umgebung mit \(U_\epsilon \subset \R^n \setminus M\). Daraus folgt wegen \(x_n \in M\), dass \(\Vert x_n - z \Vert \geq \epsilon\) ist, also kann \(z\) nicht der Grenzwert von \(x_n\) sein. Daraus folgt \(\lim_k x_{n_k} \in M\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Wir beweisen nur die Existenz einer Minimalstelle, die Maximalstelle ist analog. Sei \(\alpha = \inf_{x \in M} f(x)\). Dann gibt es für jedes \(n \in \N\) ein \(x_n \in M\) mit \(f(x_n) < \alpha+ \frac{1}n\). Da \(M\) kompakt ist hat \(x_n\) eine konvergente Teilfolge \((x_{n_k})\) mit Grenzwert \(\overline{x} \in M\). Wegen der Stetigkeit folgt
\begin{equation*}
\begin{split} f(\overline{x}) = \lim_k f(x_{n_k}) \leq \lim \alpha - \frac{1}{n_k} = \alpha.\end{split}
\end{equation*}
Also ist \(\overline{x}\) Minimalstelle.
\end{sphinxadmonition}
\label{stetigkeit/eigenschaften:definition-3}
\begin{sphinxadmonition}{note}{Definition 5.6}



Eine reelle Funktion \(f: M \rightarrow \R\) mit \(M \subset \R\) heisst
\begin{itemize}
\item {} 
monoton wachsend (bzw. streng monoton wachsend), wenn für alle \$y > x

\end{itemize}
\begin{equation*}
\begin{split} f(y) \geq f(x) \qquad (\text{ bzw. } f(y) > f(x).\end{split}
\end{equation*}\begin{itemize}
\item {} 
monoton fallend (bzw. streng monoton fallend), wenn für alle \$y > x

\end{itemize}
\begin{equation*}
\begin{split} f(y) \leq f(x) \qquad (\text{ bzw. } f(y) < f(x). )\end{split}
\end{equation*}\end{sphinxadmonition}
\label{stetigkeit/eigenschaften:theorem-4}
\begin{sphinxadmonition}{note}{Theorem 5.6 (Für \protect\(D \subset \R\protect\) und \protect\(f: D \rightarrow \R\protect\) gelten die folgenden Eigenschaften:)}


\begin{itemize}
\item {} 
\(i)\) Ist f streng monoton, dann ist \(f\) injektiv.

\item {} 
\(ii)\) Ist \(D=[a,b]\) und \(f\) injektiv, dann ist \(f\) streng monoton.

\item {} 
\(iii)\) Ist \(D=[a,b]\) und \(f:D \rightarrow [c,d]\) injektiv, dann ist \(f^{-1}\) stetig.

\end{itemize}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Für \(i)\) nehmen wir an es gibt \(x_1 \neq x_2\) mit \(f(x_1) = f(x_2)\), o.B.d.A. \(x_1 < x_2\). Wegen der strengen Monotonie gilt aber \(f(x_1) < f(x_2)\) (falls monoton steigend) oder \(f(x_1) > f(x_2)\) (falls monoton fallend), beides widerspricht \(f(x_1) = f(x_2)\).
Für \(ii)\) nehmen wir an \(f\) wäre nicht streng monoton, d.h. es gibt \(x_1 < x_2 < x_3\) mit \(f(x_1) < f(x_2)\), \(f(x_3) < f(x_2)\) oder \(f(x_1) < f(x_2)\), \(f(x_3) < f(x_2)\). Im ersten Fall gibt es ein \(C \in (f(x_1),f(x_2)) \cap (f(x_2),f(x_3)\) und wegen dem Zwischenwertsatz existieren dann \(z_1 \in (x_1,x_2)\) und \(z_2 \in (x_2,x_3)\) mit\(C = f(z_1) = f(z_2)\). Damit ist \(f\) nicht injektiv. Der zweite Fall ist analog.

\(iii)\) Nach \(ii)\) ist \(f\) streng monoton, wir nehmen an \(f\) ist streng monoton wachsend, der Fall einer streng monoton fallenden Funktion ist analog. Sei \(y_n\) eine Folge mit Grenzwert \(\overline{y}\). Dann gibt es eine monotone Teilfolge \(y_{n_k}\) mit demselben Grenzwert. Nun sei \(x_n = f^{-1}(y_n)\). Dann ist \(x_{n_k}\) monoton und beschränkt, also konvergent. Sei \(x= \lim x_{n_k}\). Nehmen wir an \(f(\overline{y}) > x\), dann ist wegen der strengen Monotonie \(f^{-1}(y) > x\) für alle \(y \geq \overline{y}\), und \(f^{-1}(y) < x\) für alle \(y < \overline{y}\).Damit ist \(f^{-1}\) nicht surjektiv, was der Bijektivität widerspricht. Also gilt \(f^{-1}(\overline{y}) = x\). Abschließend folgern wir aus der Konvergenz der Teilfolge auch noch die Konvergenz der gesamten Folge \((x_n)\). Wegen der Monotonie folgt \(x_{n_k} < x_n\) für \(y_{n_k} < y_n\). Sei also \(n_k\) so, dass  \(|y_{n_k} - \overline{y}| < \epsilon\) und \(m\) so, dass
\begin{equation*}
\begin{split} |y_{n} - \overline{y}| < |y_{n_k} - \overline{y}| < \epsilon\end{split}
\end{equation*}
für \(n \geq m\) ist. Dann folgt auc
\begin{equation*}
\begin{split} |x_n - x|  = x - x_n < x-x_{n_k} = |x-x_{n_k}|.\end{split}
\end{equation*}
Da die rechte Seite gegen beliebig klein wird mit \(k\) gegen unendlich, konvergiert also auch \(x_n\) gegen \(x\).
\end{sphinxadmonition}


\section{Grenzwerte von Funktionen}
\label{\detokenize{stetigkeit/grenzwerte:grenzwerte-von-funktionen}}\label{\detokenize{stetigkeit/grenzwerte::doc}}
Wir betrachten nun einige Grenzwerte von Funktionen. Dazu benötigen wir zunächst das Konzept des Häufungspunkts einer Menge. Wir nennen einen Punkt  \(y \in M \subset X\) einer Teilmenge eines metrischen Raums \(X\) Häufungspunkt von \(N \subset M\), wenn eine Folge \(y_n\) in \(N\) existiert mit \(y= \lim y_n\). So ist etwa jedes \(y \in \R\) Häufungspunkt von \(\Q\). Man kann leicht zeigen, dass \(M\) genau dann abgeschlossen ist, wenn \(M\) alle seine Häufungspunkte enthält.
\label{stetigkeit/grenzwerte:definition-0}
\begin{sphinxadmonition}{note}{Definition 5.7}



Sei \(f: X \rightarrow Y\) eine Funktion zwischen metrischen Räumen und \(x \in X\) Häufungspunkt der Menge \(M \subset X\). Dann hat die Funktion \(f: M \rightarrow Y\) in \(x \in X\) den Grenzwert \(y\), wenn für jede Folge \(x_n \rightarrow x\) gilt \(f(x_n) = y\).
\end{sphinxadmonition}
\label{stetigkeit/grenzwerte:example-1}
\begin{sphinxadmonition}{note}{Example 5.5}



Sei \(f(x) =cx\) für \(M = \Q\). Dann ist für \(x \in \R\) und \(y=cx\) auch \(y\) ein Grenzwert von \(f\) bei \(x\).
\end{sphinxadmonition}
\label{stetigkeit/grenzwerte:definition-2}
\begin{sphinxadmonition}{note}{Definition 5.8}



Eine auf \(D \subset \R\) definierte Funktion besitzt den rechtsseitigen Grenzwert \(y\) bei \(x \in \R \cup \{-\infty\}\), wenn \(x\)  Häufungspunkt von \(D_x^+=D \cup (x,\infty)\) ist und \(y\) Grenzwert von \(f\) in \(D_x^+\) ist. D.h. für alle Folgen \((x_n) \subset D_x^+\) mit \(x_n \rightarrow x\) gilt \(f(x_n) \rightarrow y\).Analog definiert man den linksseitigen Grenzwert bei \(x \in \R \cup \{+\infty\}\) mit \(D_x^-=D \cup (-\infty,x)\).
\end{sphinxadmonition}
\label{stetigkeit/grenzwerte:example-3}
\begin{sphinxadmonition}{note}{Example 5.6}



Die Heaviside\sphinxhyphen{}Funktion hat in \(x=0\) den rechtsseitigen Grenzwert \(1\) und den linksseitigen Grenzwert \(0\).
\end{sphinxadmonition}

Eine reelle Funktion ist stetig in \(x\) genau dann, wenn der rechts\sphinxhyphen{} und linksseite Grenzwert existieren und übereinstimmen.


\section{Gleichmäßige Stetigkeit und gleichmäßige Konvergenz}
\label{\detokenize{stetigkeit/glm:gleichmaszige-stetigkeit-und-gleichmaszige-konvergenz}}\label{\detokenize{stetigkeit/glm::doc}}
Bei der Definition der Stetigkeit haben wir zu jedem \(x\) möglicherweise einen anderen Zusammenhang zwischen \(\epsilon\) und \(\delta\), in gewisser Weise kann eine Funktion recht ungleichmäßig in ihrer Stetigkeit sein. Umgekehrt können wir gleichmäßige Stetigkeit definieren, wenn wir den \(\epsilon\)\sphinxhyphen{}\(\delta\) Zusammenhang global machen:
\label{stetigkeit/glm:definition-0}
\begin{sphinxadmonition}{note}{Definition 5.9}



Eine Funktion \(f: X\rightarrow Y\) zwischen metrischen Räumen \(X\) und \(Y\) heisst \{\textbackslash{}em gleichmäßig stetig\}, wenn es zu jedem \(\epsilon > 0\) ein \(\delta > 0\) gibt, sodass
\begin{equation*}
\begin{split} d_Y(f(x_1),f(x_2)) < \epsilon\end{split}
\end{equation*}
für alle \(x_1,x_2 \in X\) mit \(d_X(x_1,x_2) < \delta\) gilt.
\end{sphinxadmonition}
\label{stetigkeit/glm:example-1}
\begin{sphinxadmonition}{note}{Example 5.7}



Die Funktion \(f:x \mapsto x^2\) ist gleichmäßig stetig auf jedem Intervall \([a,b]\), aber nicht auf ganz \(\R\).
\end{sphinxadmonition}

Die Eigenschaft von \(x \mapsto x^2\) ist kein Zufall, es gilt der Satz von Heine\sphinxhyphen{}Cantor (hier ohne Beweis):
\label{stetigkeit/glm:theorem-2}
\begin{sphinxadmonition}{note}{Theorem 5.7}



Sei \(K \subset \R^m\) kompakt, dann ist jede stetige Abbildung \(f:K \rightarrow \R^n\) gleichmäßig stetig.
\end{sphinxadmonition}

Wir können auch Folgen von Funktionen betrachten, sowie deren mögliche Grenzwerte:
\label{stetigkeit/glm:definition-3}
\begin{sphinxadmonition}{note}{Definition 5.10}



Sei \(f_n: M \subset X \rightarrow Y\), \(n \in N\), eine Folge von Abbildungen zwischen metrischen Räumen \(X\) und \(Y\). Wir sagen:
\begin{itemize}
\item {} 
\(f_n\) konvergiert punktweise gegen \(f: M \rightarrow Y\), wenn für alle \(x \in M\) die Folge \((f_n(x))\) gegen \(f(x)\) konvergiert.

\item {} 
\(f_n\) konvergiert gleichmäßig gegen \(f: M \rightarrow Y\), wenn für alle \(\epsilon > 0\) ein \(n_0\) existiert, sodass für alle \(n \geq n_0\) und \(x \in M\) gilt:

\end{itemize}
\begin{equation*}
\begin{split} d(f(x),f_n(x)) < \epsilon.\end{split}
\end{equation*}\end{sphinxadmonition}

Wir sehen, dass sich beim Übergang von der punktweisen zur gleichmäßigen Konvergenz ein Quantor ändert. Während bei der punktweisen Konvergenz für alle \(x\) ein \(n_0\) (abhängig von \(x\)) existiert, muss \(n_0\) bei der gleichmäßigen Konvergenz für alle \(x\) das gleiche sein. Daraus sehen wir auch sofort, dass gleichmäßige Konvergenz einer Funktionenfolge immer auch punktweise Konvergenz impliziert.
Ein Unterschied der beiden Arten von Konvergenz ist die Eigenschaft des Grenzwerts stetiger Funktionen.
\label{stetigkeit/glm:example-4}
\begin{sphinxadmonition}{note}{Example 5.8}



Sei \(f_n: [0,2] \rightarrow \R\) definiert durch
\begin{equation*}
\begin{split} f_n(x) = \left\{ \begin{matrix} x^n & x < 1 \\ 1 & \text{sonst.} \end{matrix} \right.\end{split}
\end{equation*}
Dann sehen wir, dass \(f_n\) eine Folge stetiger Funktionen ist, die punktweise gege
\begin{equation*}
\begin{split} f (x) = \left\{ \begin{matrix} 0 & x < 1 \\ 1 & \text{sonst,} \end{matrix} \right.\end{split}
\end{equation*}
konvergiert. Punktweise Konvergenz erhält also nicht die Stetigkeit für den Grenzwert, wir werden aber sehen, dass dies bei gleichmäßiger Konvergenz der Fall ist.
\end{sphinxadmonition}
\label{stetigkeit/glm:theorem-5}
\begin{sphinxadmonition}{note}{Theorem 5.8}



Sei \(f_n: M \subset X \rightarrow Y\) eine Folge stetiger Funktionen zwischen metrischen Räumen, die gleichmäßig gegen \(f: M \rightarrow Y\) konvergiert. Dann ist \(f\) stetig.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Sei \(x \in M\). Wegen der gleichmäßigen Konvergenz von \(f_n\) gibt es für \(\epsilon > 0\) ein \(n_0 \in \N\), sodass für alle \(n \geq n_0\) gilt:
\begin{equation*}
\begin{split} d(f_n(x),f(x)) < \frac{\epsilon}3.\end{split}
\end{equation*}
Dies gilt insbesondere für \(n=n_0\). Nun gibt es wegen der Stetigkeit von \(f_{n_0}\) zu \(\epsilon > 0\) ein \(\delta > 0\), sodas
\begin{equation*}
\begin{split} d(f_{n_0}(x),f_{n_0}(y)) < \frac{\epsilon}3,\end{split}
\end{equation*}
gilt für \(d(x,y) < \delta\). Aus der Dreiecksungleichung folgt für solche \(y\) aber auch\textbackslash{}begin\{align*\} d(f(x),f(y)) \&\textbackslash{}leq d(f(x),f\_\{n\_0\}(y)) + d( f\_\{n\_0\}(y),f(y)) \textbackslash{} \&\textbackslash{}leq d(f(x),f\_\{n\_0\}(x)) + d + d( f\_\{n\_0\}(x),f\_\{n\_0\}(y)) + f\_\{n\_0\}(y),f(y)) \textbackslash{} \&< \textbackslash{}frac\{\textbackslash{}epsilon\}3 + \textbackslash{}frac\{\textbackslash{}epsilon\}3 + \textbackslash{}frac\{\textbackslash{}epsilon\}3 = \textbackslash{}epsilon.\textbackslash{}end\{align*\}
Also ist \(f\) stetig.
\end{sphinxadmonition}


\section{Die Exponentialfunktion}
\label{\detokenize{stetigkeit/exp:die-exponentialfunktion}}\label{\detokenize{stetigkeit/exp::doc}}
Wir betrachten nun die Exponentialfunktion noch ein wenig näher, wir haben ja im letzten Kapitel schon die Identität
\begin{equation*}
\begin{split} e^x e^y = e^{x+y}\end{split}
\end{equation*}
für alle \(x,y \in \R\) hergeleitet. Dazu sehen wir aus der Form der Potenzreihe, die nur positive Koeffizienten hat, dass \(e^x > 1\) für alle \(x > 0\) gilt. Daraus folgt natürlich aus
\begin{equation*}
\begin{split} e^{-x} = \frac{1}{e^x} > 0,\end{split}
\end{equation*}
also \(e^y > 0\) für \(y < 0\). Also ist die Exponentialfunktion immer positiv.
Aus diesen Eigenschaften folgern wir auch, dass die Exponentialfunktion streng monoton wachsend ist. Für \(y > x\) gilt ja
\begin{equation*}
\begin{split} e^y = e^x e^{y-x} > e^x.\end{split}
\end{equation*}
Nun können wir noch die Grenzwerte \(x \rightarrow \infty\) und \(x \rightarrow - \infty\) betrachten. Für \(x > 0\) gilt \(
e^x > 1+x\), und die rechte Seite wächst monoton gegen \(\infty\). Also folgt \(\lim_{x \rightarrow \infty} e^x = \infty. \) Aus \(e^{-x} = \frac{1}{e^x} \) folgt dann sofort \(\lim_{x \rightarrow - \infty} e^x = 0. \)
Nun weisen wir noch die Stetigkeit nach:
\label{stetigkeit/exp:theorem-0}
\begin{sphinxadmonition}{note}{Theorem 5.9}



Die Exponentialfunktion ist lokal Lipschitz stetig auf \(\R\) und damit insbesondere stetig.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Seien \(x  < y \in \R\). Dann gil
\begin{equation*}
\begin{split} \left\vert e^y - e^x \right\vert = e^y - e^x = e^x ( e^{y-x} - 1).\end{split}
\end{equation*}
Sei nun \(z=y-x \in (0,\epsilon)\), dann gilt
\begin{equation*}
\begin{split}  \left\vert e^z -  1 \right\vert = \sum_{n=1}^\infty \frac{1}{n!} z^n = z \sum_{n=0}^\infty \frac{1}{(n+1)!} z^n  \leq z \sum_{n=0}^\infty \frac{1}{n!} \epsilon^n = z e^\epsilon.\end{split}
\end{equation*}
Also gilt für \(|x-y| < \epsilon\) auc
\begin{equation*}
\begin{split}  \left\vert e^y - e^x \right\vert \leq e^{x+\epsilon} |y-x|,\end{split}
\end{equation*}
d.h. die Exponentialfunktion ist lokal Lipschitz\sphinxhyphen{}stetig.
\end{sphinxadmonition}

Nun haben wir gesehen, dass \(e^x: \R \rightarrow \R\) eine positive, stetige, streng monotone Funktion ist. Nach dem Zwischenwertsatz ist die Exponentialfunktion surjektiv nach \(\R^+\), da wir \(x\) mit \(e^x\) beliebig groß oder beliebig nahe bei \(0\) finden können. Damit wissen wir auch, dass die Exponentialfunktion eine stetige Inverse auf \(\R^+\) hat, die wir Logarithmus nennen
\begin{equation*}
\begin{split} \log: \R^+ \rightarrow \R, x \rightarrow \log(x),\end{split}
\end{equation*}
wobei \(\log(e^x) =x\). Aus den Eigenschaften der Exponentialfunktion folgern wir
\begin{equation*}
\begin{split} \lim_{x\rightarrow \infty} \log(x) = \infty, \qquad \lim_{x\rightarrow 0_+} \log(x) = -\infty,\end{split}
\end{equation*}
wobei \(\lim_{x\rightarrow 0_+}\) den rechtsseitigen Grenzwert bezeichnet.


\section{Trigonometrische Funktionen}
\label{\detokenize{stetigkeit/trig:trigonometrische-funktionen}}\label{\detokenize{stetigkeit/trig::doc}}
Um die trigonometrischen Funktionen Sinus (\(\sin(x)\)) und Cosinus (\(\cos(x)\)) einfach zu analysieren, erweitern wir die Potenzreihe für die Exponentialfunktion auf die komplexen Zahlen
\begin{equation*}
\begin{split} e^z:= \sum_{n=0}^\infty \frac{1}{n!} z^n, \qquad z \in \C.\end{split}
\end{equation*}
Man sieht leicht, dass diese Reihe auch in ganz \(\C\) konvergiert. Verwendet man das Argument \(z=\i x\) mit \(x \in \R\), so rechnet man aus den Potenzreihen leicht die sogenannte Euler\sphinxhyphen{}Formel nach
\begin{equation*}
\begin{split} e^{\i x} = \cos(x) + \i \sin(x),\end{split}
\end{equation*}
bzw. umgekehrt
\begin{equation*}
\begin{split} \cos(x) = \text{Re}(e^{\i x}), \qquad \sin(x) = \text{Im}(e^{\i x}).\end{split}
\end{equation*}
Aus den Eigenschaften der Exponentialfunktion folgen dann auch die Summationstheoreme für Sinus und Cosinus
\begin{align*}
e^{\i (x+y)} &= e^{\i x}e{\i y} = (\cos(x) + \i \sin(x)) (\cos(y) + \i \sin(y))  \\
&= \cos(x) \cos(y) - \sin(x) \sin(y) + \i ( \cos(x) \sin(y) + \sin(y) \cos(x)).
\end{align*}
Durch Vergleich der Real\sphinxhyphen{} und Imaginärteile erhält ma
\begin{equation*}
\begin{split} \cos(x+y) = \cos(x) \cos(y) - \sin(x) \sin(y)\end{split}
\end{equation*}
un
\begin{equation*}
\begin{split} \sin(x+y) =  \cos(x) \sin(y) + \sin(y) \cos(x).\end{split}
\end{equation*}
Wir sehen aus den Eigenschaften der Potentialreihen leicht, dass Cosinus bzw. Sinus symmetrisch bzw. antisymmetrisch sind, d.h.
\begin{equation*}
\begin{split} \cos(-x) = \cos(x)  , \qquad \sin(-x) = -\sin(x) .\end{split}
\end{equation*}
Verwenden wir dies im obigen Summationstheorem für den Cosinus für \(y=-x\), so erhalten wir
\begin{equation*}
\begin{split} 1 = \cos(0) = \cos(x)^2 + \sin(x)^2\end{split}
\end{equation*}
für alle \(x \in \R\).

Nun wollen wir noch nachweisen, dass Sinus\sphinxhyphen{} und Cosinus periodische Funktionen sind.
\label{stetigkeit/trig:lemma-0}
\begin{sphinxadmonition}{note}{Lemma 5.1}



Es gibt ein \(\pi \in \R^+\) mit \(e^{\i \frac{\pi}2} = \i\) und \(e^{\i x} \neq \i\) für \(x \in [0,\pi)\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Wir definieren
\begin{equation*}
\begin{split} \frac{\pi}2 = \inf\{x \in \R^+~|~e^{\i x } = \i \}.\end{split}
\end{equation*}
Wir wissen \(\cos(0)=1 > 0\) und es gilt
\begin{equation*}
\begin{split} \cos(2)= 1 -2 + \sum_{k=2}^\infty (-1)^k \frac{2^{2k}}{(2k)!} \leq -1 + \sum_{k=2}^\infty  \frac{2^{2k}}{(2k)!} .\end{split}
\end{equation*}
Nun zeigt man induktiv leicht für \(k \geq 2\):
\begin{equation*}
\begin{split} (2k)! \geq 4^{2k-4} 4! ,\end{split}
\end{equation*}
also
\begin{align*}
\cos(2) &\leq -1 + \frac{4^4}{4!} \sum_{k=2}^\infty \frac{2^{2k}}{4^{2k}} \\
&= -1   + \frac{4^4}{4!} \sum_{k=2}^\infty \frac{1}{4^{k}} \\
&= -1   + \frac{4 }{3!} \sum_{k=0}^\infty \frac{1}{4^{k}} \\
&= -1 + \frac{4}{6} \frac{1}{1-\frac{1}4} = -1 + \frac{16}{24}  = - \frac{1}{3} < 0.
\end{align*}
Nach dem Zwischenwertsatz existiert damit eine Nullstelle im Intervall \((0,2)\). Da
\begin{equation*}
\begin{split} \sin(x)^2 + \cos(x)^2 = 1\end{split}
\end{equation*}
gilt, also ist für \(\cos(x) = 0\) automatisch \(\sin(x) \in \{\pm 1\}\).Ist \(\sin(x) = -1\), dann wäre \(e^{\i x} = -\i\) und damit \(e^{3\i x} = (-i)^2(-i) = i\). Also gibt es ein \(x\) mit \(e^{\i x} =\i\).
\end{sphinxadmonition}

Wir sehen, dass aus \( e^{\i \pi/2 } = \i\) auch folgt:
\begin{equation*}
\begin{split} e^{\i pi} = -1, \quad e^{2 \i pi} =  1.\end{split}
\end{equation*}
Damit erhalten wir auch
\begin{equation*}
\begin{split} e^{\i x + 2 \i \pi} = e^{\i x}e^{2 \i \pi} = e^{\i x},\end{split}
\end{equation*}
für alle \(x \in \R\). Separat für den Real\sphinxhyphen{} und Imaginärteil aufgeschrieben heisst das\textbackslash{}begin\{align*\}
\textbackslash{}cos(x+2\textbackslash{}pi) \&= \textbackslash{}cos(x) \textbackslash{}
\textbackslash{}sin(x+2\textbackslash{}pi) \&= \textbackslash{}sin(x).\textbackslash{}end\{align*\}
Analog erhalten wir übrigens auch die Eigenschaften
\begin{equation*}
\begin{split} \cos(x+ \pi) = -\cos(x) , \quad  \sin(x+\pi) = - \sin(x), \quad \sin(x+\frac{\pi}2) = \cos(x).\end{split}
\end{equation*}
Durch Quotienten könen wir auc
\begin{equation*}
\begin{split} \tan(x) = \frac{\sin(x)}{\cos(x)} , \qquad \cot(x) = \frac{\cos(x)}{\sin(x)}\end{split}
\end{equation*}
definieren.  Analog zu \(\sin\) und \(\cos\) definiert man die sogenannten hyperbolischen Funktionen
\begin{equation*}
\begin{split} \cosh(x) = \frac{1}2 (e^x+e^{-x}), \qquad \sinh(x) = \frac{1}2 (e^x-e^{-x}).\end{split}
\end{equation*}
Hier gil
\begin{equation*}
\begin{split} \cosh(x)^2 - \sinh(x)^2 = 1,\end{split}
\end{equation*}
wie man leicht nachrechnet.


\chapter{Differentialrechnung}
\label{\detokenize{differential/differential:differentialrechnung}}\label{\detokenize{differential/differential::doc}}
Im Folgenden wollen wir nun die Differentialrechnung betrachten, die auf einer lokal linearen Approximation von Funktionen basiert. Sei \(f: \R \rightarrow \R\) und \(x_0 \in \R\). Für \(\epsilon > 0\) betrachten wir eine lineare Approximation von \(f\), die an der Stelle \(x_0\) mit \(f\) übereinstimmt, d.h.
\begin{equation*}
\begin{split} f(x) \approx f(x_0) + a (x-x_0).\end{split}
\end{equation*}
Dies bedeutet die richtige Steigung \(a \in \R\) is
\begin{equation*}
\begin{split} a \approx \frac{f(x)-f(x_0)}{x-x_0} .\end{split}
\end{equation*}
Da wir an einer lokalen Approximation interessiert sind, ist es naheliegend den Grenzwert \(x \rightarrow x_0\) zu betrachten. Damit definieren wir die Ableitung an der Stelle \(x_0\):
\label{differential/differential:definition-0}
\begin{sphinxadmonition}{note}{Definition 6.1}



Sei \(f: D \subset \R \rightarrow \R\) mit \(x_0 \in D\) Häufungspunkt von \(D\). Falls der Grenzwert
\begin{equation*}
\begin{split} f'(x_0) = \lim_{x \rightarrow x_0} \frac{f(x) - f(x_0)}{x-x_0}\end{split}
\end{equation*}
existiert, heisst \(f\) differenzierbar in \(x_0\) und \(f'(x_0)\) Ableitung  (oder auch Differentialquotient \(f'(x) = \frac{dx}{dt}\), in der Physik auch \(\dot f(x)\)). Die Funktion \(f\) heisst differenzierbar in \(D\), wenn für alle \(x \in D\) die Ableitung \(f'(x)\) existiert.
Ist \(f\) differenzierbar in \(D\) und \(f':D \rightarrow \R\) eine stetige Funktion, dann heisst \(f\) stetig differenzierbar in \(D\).
\end{sphinxadmonition}

Üblicherweise betrachten wir Ableitungen auf offenen Mengen \(D\), dort ist jeder Punkt \(x_0 \in D\) ein Häufungspunkt von \(D\).
Aus der Definition sehen wir sofort, dass wenn \(f\) differenzierbar in \(x_0\) ist, eine Darstellung
\begin{equation*}
\begin{split} f(x) = f(x_0) + f'(x_0)(x-x_0) +R(x)(x-x_0)\end{split}
\end{equation*}
existiert mit einem sogenannten Restglied \(R(x)\), sodass \(R(x) \rightarrow 0 \) für \(x \rightarrow x_0\). Insbesondere sehen wir
\begin{equation*}
\begin{split} \lim_{x \rightarrow x_0} f(x) = f(x_0),\end{split}
\end{equation*}
d.h. \(f\) ist stetig in \(x_0\).
\label{differential/differential:example-1}
\begin{sphinxadmonition}{note}{Example 6.1}



Eine lineare Funktion \(f: \R \rightarrow \R, x \mapsto a x + b\) ist differenzierbar in \(\R\) mit Ableitung \(f'(x) = a\).
\end{sphinxadmonition}
\label{differential/differential:example-2}
\begin{sphinxadmonition}{note}{Example 6.2}



Ein Monom \(f: \R \rightarrow \R, x \mapsto x^m\), \(m \geq 1\) ist differenzierbar in \(\R\) mit Ableitung \(f'(x) = m x^{m-1}\). Dies sehen wir aus
\begin{equation*}
\begin{split} \frac{x^m - x_0^m}{x-x_0} = \sum_{j=0}^{m-1} x^j x_0^{m-1-j} .\end{split}
\end{equation*}
Damit existiert der Grenzwert wegen \(x^j \rightarrow x_0^j\) für \(x \rightarrow x_0\).
\end{sphinxadmonition}
\label{differential/differential:example-3}
\begin{sphinxadmonition}{note}{Example 6.3}



Die Exponentialfunktion \(f: \R \rightarrow \R, x \mapsto e^x\) ist differenzierbar in \(\R\) mit Ableitung \(f'(x) = e^x\), da
\begin{equation*}
\begin{split} \frac{e^x - e^{x_0}}{x-x_0} = e^{x_0}  \frac{e^{x-x_0}-1}{x-x_0}\end{split}
\end{equation*}
gilt und für \(h \in \R\)
\begin{equation*}
\begin{split} \frac{e^h - 1}h =  h \sum_{j=2}^\infty \frac{1}{j!}h^{j-2} \leq h  \sum_{j=0}^\infty \frac{1}{j!}h^{j} = h e^h \rightarrow 0\end{split}
\end{equation*}
für \(h \rightarrow 0\).
\end{sphinxadmonition}


\section{Ableitungsregeln für Kombinationen von Funktionen}
\label{\detokenize{differential/kombfkt:ableitungsregeln-fur-kombinationen-von-funktionen}}\label{\detokenize{differential/kombfkt::doc}}
Wir haben in den obigen Beispielen einige Ableitungen für elementare Funktionen ausgerechnet. Ähnlich wie auch sonst bei Grenzwerten können wir dies auf verschiedene Kombinationen (Summen, Produkte) von Funktionen erweitern:
\label{differential/kombfkt:theorem-0}
\begin{sphinxadmonition}{note}{Theorem 6.1}



Seien \(f: D\subset \R \rightarrow \R\) und \(g: D\subset \R \rightarrow \R\) in \(x_0 \in D\) differenzierbar. Dann gilt:
\begin{itemize}
\item {} 
\(i)\) Die Summe \(f+g\) ist differenzierbar in \(x_0\) mi

\end{itemize}
\begin{equation*}
\begin{split} (f+g)'(x_0) = f'(x_0) + g'(x_0).\end{split}
\end{equation*}\begin{itemize}
\item {} 
\(ii)\) Für \(c \in \R\) gilt

\end{itemize}
\begin{equation*}
\begin{split} (cf)'(x_0) = c f'(x_0).\end{split}
\end{equation*}\begin{itemize}
\item {} 
\(iii)\) Das Produkt \(fg\) ist differenzierbar mit Ableitung (Produktregel)

\end{itemize}
\begin{equation*}
\begin{split} (fg)'(x_0) = f'(x_0) g(x_0) +  f(x_0) g'(x_0).\end{split}
\end{equation*}\begin{itemize}
\item {} 
\(iv)\) Ist \(g(x_0) \neq 0\), dann ist der Quotient \(\frac{f}g\) differenzierbar mit Ableitung (Quotientenregel)

\end{itemize}
\begin{equation*}
\begin{split} (\frac{f}g)'(x_0) = \frac{f'(x_0)}{ g(x_0)} -  \frac{f(x_0) g'(x_0)}{g(x_0)^2}.\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. \(i)\) und \(ii)\) folgen direkt aus den Regeln für Grenzwerte, \(ii)\) würde auch aus drei Folgen, wenn man \(g(x) =c\) setzt. Um \(iii)\) zu beweisen, betrachten wir
\begin{align*} \frac{f(x)g(x) - f(x_0)g(x_0)}{x-x_0} &= \frac{f(x)g(x) - f(x_0)g(x)}{x-x_0} + \frac{f(x_0)g(x) - f(x_0)g(x_0)}{x-x_0} \\
 &= \frac{f(x)  - f(x_0) }{x-x_0} g(x) + f(x_0) \frac{g(x) - g(x_0)}{x-x_0}
\end{align*}
Mit den Regeln für Grenzwerte von Summen und Produkten folgt dann direkt die Existenz des Grenzwerts mit
\begin{equation*}
\begin{split} (fg)'(x_0) = f'(x_0) g(x_0) +  f(x_0) g'(x_0),\end{split}
\end{equation*}
wobei wir die Stetigkeit von \(g\) verwenden, um \(g(x) \rightarrow g(x_0)\) zu erhalten.

Für (iv) zeigen wir nur den Fall \(f=1\), der Rest ergibt sich dann aus der Produktregel. Es gilt\textbackslash{}begin\{align*\}
\textbackslash{}frac\{\textbackslash{}frac\{1\}\{g(x)\} \sphinxhyphen{} \textbackslash{}frac\{1\}\{g(x\_0)\}\}\{x\sphinxhyphen{}x\_0\} \&= \sphinxhyphen{} \textbackslash{}frac\{g(x) \sphinxhyphen{} g(x\_0)\}\{g(x) g(x\_0) (x\sphinxhyphen{}x\_0)\} \textbackslash{}end\{align*\}
Da \(g(x_0) \neq 0\) folgt wegen der Stetigkeit von \(g\) auch, dass \(g(x) \neq 0\) in einer Umgebung von \(x_0\) gilt, also ist der Quotient dort auch wohldefiniert und es gilt \(\frac{1}{g(x)} \rightarrow \frac{1}{g(x_0)}\). Daraus folgt direkt die Quotientenregel. \(\square\)
\end{sphinxadmonition}
\label{differential/kombfkt:example-1}
\begin{sphinxadmonition}{note}{Example 6.4}



Da wir schon gezeigt haben, dass jedes Monom differenzierbar ist, sehen wir mit \(i)\) und \(ii)\) auch, dass jedes Polynom
\begin{equation*}
\begin{split} p: \R \rightarrow \R, x \mapsto \sum_{j=0}^m a_j x^j\end{split}
\end{equation*}
differenzierbar ist mit
\begin{equation*}
\begin{split} p'(x) = \sum_{j=1}^m j a_j x^{j-1} .\end{split}
\end{equation*}\end{sphinxadmonition}
\label{differential/kombfkt:example-2}
\begin{sphinxadmonition}{note}{Example 6.5}



Aus der Quotientenregel sehen wir, dass jede rationale Funktion \(f= \frac{p}q\) mit Polynomen \(p\) und \(q\) überall dort differenzierbar ist, wo \(q\) keine Nullstelle hat.
\end{sphinxadmonition}

Eine weitere zentrale Eigenschaft für Ableitungen ist die Kettenregel:
\label{differential/kombfkt:theorem-3}
\begin{sphinxadmonition}{note}{Theorem 6.2 (Sei \protect\(f: D \rightarrow \R\protect\) in \protect\(x_0 \in D\protect\) differenzierbar und \protect\(g: \tilde D \rightarrow \R\protect\) in \protect\(f(x_0) \in \tilde D\protect\) differenzierbar. Dann ist \protect\(g \circ f: x \mapsto g(f(x))\protect\) differenzierbar in \protect\(x_0\protect\) und es gilt)}


\begin{equation*}
\begin{split} (g\circ f)'(x_0) = g(f(x_0)) f'(x_0).\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Ist \(f'(x_0) \neq 0\), dann ist für \(x\neq x_0\) nahe genug bei \(x_0\) auch \(f(x) \neq f(x_0)\) und es gilt
\begin{equation*}
\begin{split}\lim_{x \rightarrow x_0} \frac{g(f(x))-g(f(x_0))}{x-x_0} = \lim_{x \rightarrow x_0} \frac{g(f(x))-g(f(x_0))}{f(x)-f(x_0)}
 \frac{f(x)-f(x_0)}{x-x_0} = g'(f(x_0)) f'(x_0).\end{split}
\end{equation*}
Falls \(f'(x_0)=0\) gilt, dann ist
\begin{equation*}
\begin{split}f(x) = f(x_0) +  R(x)(x-x_0\end{split}
\end{equation*}
mit \(R(x) \rightarrow  0\) für \(x \rightarrow x_0\) und damit
\begin{equation*}
\begin{split}\lim_{x \rightarrow x_0} \frac{g(f(x))-g(f(x_0))}{x-x_0} =  
\lim_{x \rightarrow x_0} \frac{g(f(x_0)+R(x)(x-x_0)-g(f(x_0))}{R(x)(x-x_0)}  \lim_{x \rightarrow x_0} R(x) = 0,\end{split}
\end{equation*}
also gilt auch hier die Kettenregel. \(\square\).
\end{sphinxadmonition}
\label{differential/kombfkt:example-4}
\begin{sphinxadmonition}{note}{Example 6.6}



Sei \(h: \R \rightarrow \R, x\mapsto e^{-x^2/2}.\) Dann wenden wir die Kettenregel mit \(f(x) = -\frac{x^2}2\), \(f'(x) = -x\) und
\(g(x) = e^x\), \(g'(x)=e^x\) an und erhalten
\begin{equation*}
\begin{split}h'(x) = - x e^{-x^2/2}.\end{split}
\end{equation*}\end{sphinxadmonition}
\label{differential/kombfkt:theorem-5}
\begin{sphinxadmonition}{note}{Theorem 6.3}



Sei \(I\) ein Intervall in \(R\) und \(f: I \rightarrow \R\) streng monoton und differenzierbar. Dann ist die Umkehrfunktion \(f^{-1}: f(I) \rightarrow I\) in allen Punkten \(y=f(x)\) mit \(f'(x) \neq 0\) differenzierbar und es gilt
\begin{equation*}
\begin{split} (f^{-1})'(f(x)) = \frac{1}{f'(x)}, \qquad (f^{-1})'(y) = \frac{1}{f'(f^{-1}(y))}.\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Sei \(x_0 \in I\), \(f'(x_0) \neq 0\), \(y_0 =f(x_0)\). Dann gilt mit dem Restglied \(R\)
\begin{equation*}
\begin{split} \vert f(x) - f(x_0) \vert = \vert f'(x_0) + R(x) \vert ~\vert x- x_0\vert\end{split}
\end{equation*}
und wegen \(R(x) \rightarrow 0\) und \(f'(x_0) \neq 0\) folgt für \(|x-x_0|\) klein, \(x \neq x_0\)
\begin{equation*}
\begin{split} |f(x) - f(x_0)| \geq \frac{1}2 |f'(x_0)| ~|x-x_0| > 0.\end{split}
\end{equation*}
Also folgt
\begin{equation*}
\begin{split}\frac{f^{-1}(y) - f^{-1}(y_0)}{y-y_0} =  \frac{f^{-1}(y) - f^{-1}(y_0)}{f(f^{-1}(y)) - f(f^{-1}(y_0))} =\frac{x -x_0}{f(x) - f(x_0)}.\end{split}
\end{equation*}
Wegen der Differenzierbarkeit von \(f\) existiert der Grenzwert und es gilt
\begin{equation*}
\begin{split} (f^{-1})'(f(x_0)) = \frac{1}{f'(x_0)},\end{split}
\end{equation*}
die zweite Identität folgt aus \(f(x_0)=y_0\), \(x_0 = f^{-1}(y_0)\). \(\square\)
\end{sphinxadmonition}
\label{differential/kombfkt:example-6}
\begin{sphinxadmonition}{note}{Example 6.7}



Die Umkehrfunktion \(\log: \R^+ \rightarrow \R\) der Exponentialfunktion erfüllt
\begin{equation*}
\begin{split} \log'(y)  = \frac{1}{e^{\log(y)}} = \frac{1}y.\end{split}
\end{equation*}\end{sphinxadmonition}
\label{differential/kombfkt:example-7}
\begin{sphinxadmonition}{note}{Example 6.8}



Die Umkehrfunktion des Sinus \(\arcsin: (-\pi,\pi] \rightarrow \R\)   erfüllt wegen \(\sin'(x) = \cos(x)\)
\begin{equation*}
\begin{split} \arcsin'(y)  = \frac{1}{ \cos(\arcsin(y))} .\end{split}
\end{equation*}\end{sphinxadmonition}


\section{Der Mittelwertsatz der Differentialrechnung}
\label{\detokenize{differential/mws:der-mittelwertsatz-der-differentialrechnung}}\label{\detokenize{differential/mws::doc}}
Wir betrachten nun eine Version des Zwischenwertsatzes für Ableitungen, der einen Zusammenhang zwischen Ableitungen undDifferenzenquotienten herstellt. Als Grundlage dafür beweisen wir zunächst ein interessantes Resultat über Minimal\sphinxhyphen{} und Maximalstellen:
\label{differential/mws:lemma-0}
\begin{sphinxadmonition}{note}{}



Sei \(f:[a,b] \rightarrow \R\) stetig und \(x_0 \in (a,b)\) eine Minimal\sphinxhyphen{} oder Maximalstelle von \(f\). Ist \(f\) bei \(x_0\) differenzierbar, dann gilt \(f'(x_0) = 0\).````
**Beweis. ** Für \(x_0 \in (a,b)\) gibt es ein \(\epsilon > 0\) mit \((x_0 - \epsilon, x_0 + \epsilon) \subset (a,b)\). Ist \(x_0\) Minimalstelle, dann gilt für alle \(x \in (x_0 - \epsilon, x_0 + \epsilon)\) auch \(f(x) \geq f(x_0)\) und für \(\epsilon\) hinreichend klei
\begin{equation*}
\begin{split} f(x) - f(x_0) = (f'(x_0) + R(x))(x-x_0).\end{split}
\end{equation*}
Also folgt für \(x > x_0\) bzw. \(x < x_0\)
\begin{equation*}
\begin{split} f'(x_0) \leq -  R(x) \qquad \text{bzw.} \qquad f'(x_0) \geq  - R(x).\end{split}
\end{equation*}
Im Grenzwert \(x \rightarrow 0\) folgt dann\( 0 \leq f'(x_0) \leq 0\), also \(f'(x_0) = 0\). Der Beweis für eine Maximalstelle ist analog mit umgedrehten Ungleichungszeichen. \(\square\).
\label{differential/mws:theorem-1}
\begin{sphinxadmonition}{note}{Lemma 6.1Theorem 6.4}



Sei \(f:[a,b] \rightarrow \R\) differenzierbar auf \((a,b)\). Dann gibt es ein \(x_0 \in (a,b)\) mit
\begin{equation*}
\begin{split} \frac{f(b) - f(a)}{b-a} = f'(x_0) .\end{split}
\end{equation*}\end{sphinxadmonition}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Sei
\begin{equation*}
\begin{split} g(x) = f(x) - f(a) - \frac{f(b) - f(a)}{b-a}(x-a).\end{split}
\end{equation*}
Dann gilt \(g(a) = g(b) = 0\) un
\begin{equation*}
\begin{split} g'(x) = f'(x) -  \frac{f(b) - f(a)}{b-a}.\end{split}
\end{equation*}
\(g\) ist eine stetige Funktion und nimmt deshalb auf \([a,b]\) sowohl sein Minimum als auch sein Maximum an. Ist \(g\) konstant Null, dann ist \(g'(x)\) für alle \(x \in (a,b)\) und wir sind fertig. Andernfalls gibt es zumindest eine Minimal\sphinxhyphen{} oder Maximalstelle \(x_0\) von \(g\) im Intervall \((a,b)\), also folgt \(g'(x_0)=0\), was den Mittelwertsatz impliziert. \(\square\)
\end{sphinxadmonition}

Ähnlich beweist man folgende Verallgemeinerung des Mittelwertsatzes: Mit den gleichen Bedingungen an \(f\) und \(g\) wie oben, sowie \(g(b) \neq g(a)\) und \(g'(x) \neq 0\) für \(x \in (a,b)\) folgt die Existenz eines \(x_0 \in (a,b)\) mit
\begin{equation*}
\begin{split}\frac{f'(x_0)}{g'(x_0)} = \frac{f(b) - f(a)}{g(b) - g(a)}.\end{split}
\end{equation*}
Eine interessante Anwendung der Differentialrechnung, ähnlich zu dieser Art von Quotienten ist die Regel von de l’Hospital, mit der wir zunächst unbestimmte Grenzwerte von Quotienten charakterisieren können. Ein Beispiel ist\( \lim_{x \rightarrow 0} \frac{\sin(x)}x\), das zunächst auf ein unbestimmtes \(\frac{0}0\) führt. Nun ist aber
\begin{equation*}
\begin{split}\sin(x) = \sin(0) + \cos(0)x + R(x)x = (1+R(x))x,\end{split}
\end{equation*}
mit \(R(x) \rightarrow 0\), d.h.,
\begin{equation*}
\begin{split} \lim_{x \rightarrow 0} \frac{\sin(x)}x = \lim_{x \rightarrow 0} \frac{(1+R(x))x}x = 1.\end{split}
\end{equation*}
Die selbe Idee angewendet in Zähler und Nenner führt auf die allgemeinere Regel von de l’Hospital:
\label{differential/mws:theorem-2}
\begin{sphinxadmonition}{note}{Theorem 6.5}



Seien \(f\) und \(g\) differenzierbar auf \([a,b]\) und \(x_0 \in [a,b]\) mit \(f(x_0)=g(x_0) = 0\), sowie\(g'(x_0) \neq 0\). Dann gilt
\begin{equation*}
\begin{split} \lim_{x \rightarrow x_0} \frac{f(x)}{g(x)} = \frac{f'(x_0)}{g'(x_0)}.\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  Es gilt für \(x \neq x_0\) mit den entsprechenden Restgliedern \(R\) und \(S\)
\begin{align*}
f(x) &= (f'(x_0) + R(x))(x-x_0) \\
g(x) &= (g'(x_0) + S(x))(x-x_0),
\end{align*}
mit \(\lim_{x \rightarrow x_0} R(x) = \lim_{x \rightarrow x_0} S(x) =0. \)
Also folgt
\begin{equation*}
\begin{split}\lim_{x \rightarrow x_0} \frac{f(x)}{g(x)} = \lim_{x \rightarrow x_0} \frac{f'(x_0) + R(x)}{g'(x_0) + S(x)} = \frac{f'(x_0)}{g'(x_0)}.  \quad\square\end{split}
\end{equation*}\end{sphinxadmonition}
\label{differential/mws:example-3}
\begin{sphinxadmonition}{note}{Example 6.9}



Sei \(f(x) = e^x-1\) und \(g(x) = \sin(x)\). Dann ist
\begin{equation*}
\begin{split} \lim_{x \rightarrow 0} \frac{f(x)}{g(x)} = \frac{e^0}{\cos(0)} = 1.\end{split}
\end{equation*}\end{sphinxadmonition}


\section{Höhere Ableitungen}
\label{\detokenize{differential/hoehereOrdnung:hohere-ableitungen}}\label{\detokenize{differential/hoehereOrdnung::doc}}
In vielen Fällen kann man die Idee der Ableitung iterieren um höher Ableitungen zu erhalten. Ist \(f'\) wieder differenzierbar in einer Umgebung von \(x_0\) und existiert der Grenzwer
\begin{equation*}
\begin{split} f''(x_0) = \lim_{x \rightarrow x_0} \frac{f'(x) - f'(x_0)}{x-x_0} ,\end{split}
\end{equation*}
so nennen wir \(f'' = (f')'\) die zweite Ableitung von \(f\). In dieser Form erhalten wir, falls diese existieren, auch \(k\)\sphinxhyphen{}te Ableitungen \(f^{(k)}=(f^{(k-1)})'\). Wir nennen \(f\) in diesem Fall \(k\)\sphinxhyphen{}mal differenzierbar, bzw. unendlich oft differenzierbar wenn die Ableitung für jedes \(k \in \N\) existiert. Der Vollständigkeit halber setzen wir \(f^{(0)} = f\).
\label{differential/hoehereOrdnung:example-0}
\begin{sphinxadmonition}{note}{Example 6.10}



Sei \(f: \R \rightarrow \R, x \mapsto e^x.\) Dann ist \(f\) unendlich oft differenzierbar in \(\R\) und \(f^{(k)}(x)=f(x)\).
\end{sphinxadmonition}
\label{differential/hoehereOrdnung:example-1}
\begin{sphinxadmonition}{note}{Example 6.11}



Sei \(f: \R \rightarrow \R, x \mapsto x^2.\) Dann ist \(f\) unendlich oft differenzierbar in \(\R\) mit \(f'(x)=2x\), \(f''(x)=2\) und \(f^{(k)}(x)=0\) für \(k > 2\).
\end{sphinxadmonition}

Eine interessante Anwendung sind wieder Minimal\sphinxhyphen{} und Maximalstellen:
\label{differential/hoehereOrdnung:theorem-2}
\begin{sphinxadmonition}{note}{Theorem 6.6}



Sei \(f:[a,b]\rightarrow \R\) zweimal differenzierbar in einer Umgebung von \(x_0 \in (a,b)\). Dann gilt:
\begin{itemize}
\item {} 
\(i)\) Ist \(x_0\) Minimalstelle (Maximalstelle) dann gilt \(f'(x_0) = 0\), \(f''(x_0) \geq 0\) (\(f''(x_0) \leq 0\))

\item {} 
\(ii)\) Ist \(f'(x_0) = 0\) und \(f''(x_0) > 0\) (\(f''(x_0) < 0\)), dann ist \(x_0\) lokale Minimalstelle (lokale Maximalstelle), d.h. es gibt \(\epsilon > 0\) mit \(f(x) \leq f(x_0)\) (bzw. \(f(x) \geq f(x_0)\)) für alle \(x\in (x_0-\epsilon, x_0+\epsilon)\).

\end{itemize}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Wieder beweisen wir nur den Fall einer Minimalstelle, jener für Maximalstellen ist analog.
\(i)\) Ist \(x_0\) Minimalstelle, dann wissen wir bereits \(f'(x_0) = 0\). Nun gilt für \(x > x_0\) nach dem Mittelwertsatz für ein \(\xi(x) \in (x_0,x)\)
\begin{equation*}
\begin{split} 0 \leq  \frac{f(x)-f(x_0)}{x-x_0} = f'(\xi(x)).\end{split}
\end{equation*}
Also folgt auch
\begin{equation*}
\begin{split} 0 \leq \frac{f'(\xi(x)) - f'(x_0)}{\xi(x)-x_0}.\end{split}
\end{equation*}
Da \(\xi(x) \rightarrow \x_0\) für \(x \rightarrow x_0\) können wir den Grenzwert durchführen und erhalten \(f''(x_0) \geq 0\).

\(ii)\) Ist \(f'(x_0) = 0\) und \(f''(x_0) > 0\), dann folgt analog für \(x > x_0\)
\begin{equation*}
\begin{split} f(x) - f(x_0) = (x-x_0) ( f'(x_0) + (f''(x_0) + R_2(\xi(x)))(\xi(x)-x_0)) = (f''(x_0) + R_2(\xi(x)))(\xi(x)-x_0)(x-x_0),\end{split}
\end{equation*}
wobei \(R_2\) das Restglied bei der ersten Ableitung ist. Ist \(x-x_0\) klein genug, dann gilt
\(|R_2(\xi(x))| < \frac{1}2 f''(x_0)\). Damit folg
\begin{equation*}
\begin{split} f(x) - f(x_0) > \frac{1}2 f''(x_0) (\xi(x)-x_0)(x-x_0) > 0,\end{split}
\end{equation*}
also \(f(x) > f(x_0)\). Den Fall \(x < x_0\) behandeln wir analog. \(\square\)
\end{sphinxadmonition}


\chapter{Integralrechnung}
\label{\detokenize{integration/integration:integralrechnung}}\label{\detokenize{integration/integration::doc}}
Im Folgenden wollen wir uns mit der Integration von Funktionen \(f:[a,b] \rightarrow \R\) beschäftigen. Für nichtnegative Funktionen soll das Integral intuitiv der Fläche unter der Funktion entsprechen, für allgemeinere Funktionen wollen wir die Differenz aus der Flächen definiert durch den positiven und negativen Teil der Funktion (den wir später noch genau definieren werden).  Um eine Fläche zu approximieren, können wir eine Zerlegung in Rechtecke verwenden, deren Fläche wir elementar als Produkt der Seitenlängen erhalten. Die Rechtecksseiten legen wir parallel zu den Koordinatenachsen, d.h. die erste Seite ist ein Abstand der Argumente, die zweite Seite entspricht im wesentlichen einem Funktionswert.
Wir führen dazu Zerlegungen
\begin{equation*}
\begin{split} a = t_0 < t_1 < \ldots < t_n = b\end{split}
\end{equation*}
ein, und schreiben \(T=\{t_0,\ldots,t_n\}\). Die Menge der Zerlegungen eines Intervalls ist gegeben durch
\begin{equation*}
\begin{split} {\mathcal E} = \{ T \subset [a,b]~|~ a,b \in T, \vert T \vert < \infty\}.\end{split}
\end{equation*}
Eine Zerlegung \(T\) heißt feiner als \(T'\), wenn \(T' \subset T\). Wir denken uns \(T\) identifiziert mit den Teilintervallen \([t_i,t_{i+1}]\), \(i=0,\ldots,n-1\). Diese werden wir als erste Seiten der Rechtecke verwenden, die zweite erhalten wir aus den Funktionswerten. Der klassische Ansatz zum Riemann\sphinxhyphen{}Integral ist es dabei zwei Rechtecke zu verwenden, ein möglichst großes und ein möglichst kleines (intuitiv mit dem maximalen und dem minimalen Funktionswert in \([t_i,t_{i+1}]\)). Rechnen wir dazu (mit Vorzeichen) die Summe der Rechtecksflächen aus, dann erhalten wir die sogenannten Ober\sphinxhyphen{} und Untersummen
\begin{align*}
O(f,T) &= \sum_{i=0}^{n-1} (t_{i+1} - t_i) S_i(f) &S_i(f) =  \sup_{t \in  [t_i,t_{i+1}]} f(t), \\
U(f,T) &= \sum_{i=0}^{n-1} (t_{i+1} - t_i) s_i(f) &s_i(f) =  \inf_{t \in  [t_i,t_{i+1}]} f(t).
\end{align*}
Aus der Definition ist klar, dass \(s_i \leq S_i\) und damit
\begin{equation*}
\begin{split} U(f,T) \leq O(f,T) \qquad \forall T \in {\mathcal E}.\end{split}
\end{equation*}
Wir beachten, dass im Fall einer stetigen Funktion, bei der im kompakten Intervall \([t_i,t_{i+1}]\) ja immer Minimum und Maximum existieren, auch wirklich
\begin{equation*}
\begin{split} S_i(f) =  \max_{t \in  [t_i,t_{i+1}]} f(t), \qquad s_i(f) =  \min_{t \in  [t_i,t_{i+1}]} f(t)\end{split}
\end{equation*}
gilt, d.h. wir bilden wirklich Rechtecke mit dem größten bzw. kleinsten Funktionswert. Die Definition mit Supremum und Infimum ist aber auch für nichtstetige Funktionen geeignet.
\label{integration/integration:example-0}
\begin{sphinxadmonition}{note}{Example 7.1}



Sei \(f:[-1,1] \rightarrow \R\) die Heaviside\sphinxhyphen{}Funktion, d.h. \(f(x) =1 \) für \(x \geq 0\) and \(f(x) = 0\) für \(x < 0\). Hier können wir die Fläche unter der Funktion direkt als Rechteck berechnen und erhalten also \(1\) als korrekten Wert des Integrals.
Sei \(k\) der maximale Index, sodass \(t_k < 0\). Dann gilt
\begin{equation*}
\begin{split}  O(f,T) = \sum_{i=k}^{n-1} (t_{i+1} - t_i) = 1 - t_k, \qquad U(f,T) = \sum_{i=k+1}^{n-1} (t_{i+1} - t_i) = 1 - t_{k+1}. \end{split}
\end{equation*}
Damit sehen wir
\begin{equation*}
\begin{split} U(f,T) \leq 1 \leq O(f,T).\end{split}
\end{equation*}
Andererseits kann \(O(f,T)\) beliebig nahe an \(1\) kommen, wenn \(t_k\) nahe genug bei \(0\) liegt, während \(U(f,T)\) sogar exakt gleich \(1\) ist, wenn \(t_{k+1} = 0\) gilt. Also ist das Integral der größtmögliche Wert der Untersummen bei allen möglichen Zerlegungen \(T\) und das Infimum der Obersummen aller möglichen Zerlegungen. Dies werden wir später als Definition verwenden.
\end{sphinxadmonition}
\label{integration/integration:lemma-1}
\begin{sphinxadmonition}{note}{Lemma 7.1}



Die Unter\sphinxhyphen{} und Obersummen einer Funktion \(f\) haben die folgenden Eigenschaften:
\begin{itemize}
\item {} 
\(i)\) \(\forall T \in {\mathcal E}: U(-f,T) = - O(f,T)\).

\item {} 
\(ii)\) \(\forall T \in {\mathcal E}, c \geq 0: U(cf,T) = c U(f,T)\) und \(O(cf,T) = c O(f,T).\)

\item {} 
\(iii)\) \(\forall T' \subset T \in {\cal E}: U(f,T) \geq U(f,T')\) und \(O(f,T) \leq O(f,T')\).

\item {} 
\(iv)\) \(\forall T, T' \in {\mathcal E}: U(f,T) \leq O(f,T')\)

\item {} 
\(v)\) Für eine weitere Funktion \(g\) gilt

\end{itemize}
\begin{equation*}
\begin{split}\forall T \in {\mathcal E}: U(f+g,T) \geq U(f,T) + U(g,T), \quad O(f+g,T) \leq O(f,T) + O(g,T).\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. \sphinxstylestrong{Ad \(i)\):}

Folgt direkt aus
\begin{equation*}
\begin{split}\inf_{t \in [t_i,t_{i+1}]}(-f(t)) = - \sup_{t \in [t_i,t_{i+1}]}(f(t))\end{split}
\end{equation*}
\sphinxstylestrong{Ad \(ii)\):}

Gilt wegen
\begin{equation*}
\begin{split} \inf_{t \in [t_i,t_{i+1}]} (c f(t)) = c \inf_{t \in [t_i,t_{i+1}]} f(t) \qquad \sup_{t \in [t_i,t_{i+1}]} (c f(t)) = c \sup_{t \in [t_i,t_{i+1}]} f(t).\end{split}
\end{equation*}
\sphinxstylestrong{Ad \(iii)\):}

Gilt \(T' \subset T\), dann gibt es für jedes \(i\) ein \(j(i)\) mit \(t_i' =t_{j(i)}\). Damit folgt
\begin{align*} O(f,T') &= \sum_{i=0}^{n'-1} (t_{i+1}' - t_i') \sup_{t \in  [t_i',t_{i+1}']} f(t) =\sum_{i=0}^{n'-1} (t_{j(i+1)} - t_{j(i)}) \sup_{t \in  [t_{j(i)},t_{j(i+1)}]} f(t) \\
&= \sum_{i=0}^{n'-1} \sum_{k=j(i)}^{j(i+1)-1} (t_{k+1} - t_{k}) \sup_{t \in  [t_{j(i)},t_{j(i+1)}]} f(t)  \\
&\geq \sum_{i=0}^{n'-1} \sum_{k=j(i)}^{j(i+1)-1} (t_{k+1} - t_{k}) \sup_{t \in  [t_{k},t_{k+1}]} f(t)   =\sum_{i=0}^{n-1} (t_{k+1} - t_{k}) \sup_{t \in  [t_{k},t_{k+1}]} f(t)  \\ &= O(f,T).
\end{align*}
Der Beweis für Untersummen ist analog.

\sphinxstylestrong{Ad \(iv)\):}

Sei \(T''= T \cup T'\). Dann ist klarerweise \(U(f,T'') \leq O(f,T'')\) und mit \(iii)\) folgt
\begin{equation*}
\begin{split} U(f,T') \leq U(f,T'') \leq O(f,T'') \leq O(f,T) .\end{split}
\end{equation*}
\sphinxstylestrong{Ad \(v)\):}
Folgt aus
\begin{equation*}
\begin{split} \inf (f+g) \geq \inf f + \inf g, \qquad \sup (f+g) \geq \sup f + \sup g.  \square\end{split}
\end{equation*}\end{sphinxadmonition}


\section{Riemann Integral}
\label{\detokenize{integration/riemann:riemann-integral}}\label{\detokenize{integration/riemann::doc}}
Da die Obersummen mit feiner werdender Zerlegung kleiner und die Untersummen größer werden, ist es naheliegend im Grenzwert auf die kleinsten Ober\sphinxhyphen{} bzw. größten Untersummen zu schauen. Diese existieren nicht unbedingt, aber zumindest das Infimum bzw. Supremum.
\label{integration/riemann:definition-0}
\begin{sphinxadmonition}{note}{Definition 7.1}



Sei \(f:[a,b] \rightarrow \R\) beschränkt. Dann heißt
\begin{equation*}
\begin{split} O(f) = \inf\{ O(f,T)~|~ T \in {\cal E} \} \end{split}
\end{equation*}
Oberintegral und
\begin{equation*}
\begin{split} U(f) =  \sup\{ U(f,T)~|~ T \in {\cal E} \} \end{split}
\end{equation*}
Unterintegral.
Gilt \(O(f) = U(f)\), dann heißt \(f\) integrierbar (im Sinn von Riemann) und man nennt
\begin{equation*}
\begin{split} \int_a^b f(x)~dx := O(f) = U(f)\end{split}
\end{equation*}
das Integral von \(f\) im Intervall \([a,b]\).
\end{sphinxadmonition}

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Bernhard Riemann}

\sphinxhref{https://de.wikipedia.org/wiki/Bernhard\_Riemann}{Georg Friedrich Bernhard Riemann} (* 17. September 1826 in Breselenz bei Dannenberg (Elbe); † 20. Juli 1866 in Selasca bei Verbania am Lago Maggiore) war ein deutscher Mathematiker.
\end{sphinxShadowBox}

Im obigen Beispiel der Heaviside\sphinxhyphen{}Funktion sehen wir sofort \(O(f)=U(f)=1\) und damit die Integrierbarkeit und
\(\int_{-1}^1 f(x)~dx = 1\).
Wir beachten, dass allgemein bei einer beschränkten Funktion gilt
\begin{equation*}
\begin{split}O(f) \leq (b-a) \sup_{t \in [a,b]}f(t) \leq  (b-a) \sup_{t \in [a,b]} \vert f(t) \vert\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split}U(f) \geq (b-a) \inf_{t \in [a,b]}f(t) \leq -  (b-a) \sup_{t \in [a,b]} \vert f(t) \vert .\end{split}
\end{equation*}
Darüber hinaus gilt wegen der Eigenschaft \(U(f,T') \leq O(f,T)\) für alle \(T,T'\) auch \(U(f) \leq O(f)\).
\label{integration/riemann:example-1}
\begin{sphinxadmonition}{note}{Example 7.2}



Sei \(f:[0,1] \rightarrow \R\) definiert durch \(f(x) =1\) für \(x\in \Q\) und \(f(x)=0\) für \(x \notin \Q\). Dann ist bei jeder Zerlegung \(\sup_{t \in [t_i,t_{i+1}]}(f(t)) =1\) und \(\inf_{t \in [t_i,t_{i+1}]}(f(t))=0\), da jedes Teilintervall sowohl reelle als auch rationale Zahlen enthält. Damit ist auch \(O(f)=1\) und \(U(f) =0\), die Funktion ist also nicht integrierbar.
\end{sphinxadmonition}

Wir haben gesehen, dass Ober\sphinxhyphen{} und Untersummen einige Eigenschaften nahe an der Linearität erfüllen. Diese ist im Grenzwert des Integrals dann gegeben:
\label{integration/riemann:lemma-2}
\begin{sphinxadmonition}{note}{Lemma 7.2}



Sei \({\cal I}\) die Menge der integrierbaren Funktionen auf dem Intervall \([a,b]\). dann ist \({\cal I}\) ein Vektorraum. Die Abbildung
\begin{equation*}
\begin{split}I: {\cal I} \rightarrow \R, f \mapsto \int_a^b f(x)~dx\end{split}
\end{equation*}
ist linear, d.h.,
\begin{equation*}
\begin{split} \int_a^b (f(x)+g(x))~dx = \int_a^b f(x)~dx + \int_a^b g(x)~dx,\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split} \int_a^b c f(x)~dx = c \int_a^b f(x)~dx\end{split}
\end{equation*}
für alle \(c \in \R\). Darüber hinaus ist die Abbildung \(I\) monoton, d.h. falls \(f \geq g\) (d.h. \(f(x) \geq g(x)\) für alle \(x\in[a,b]\)) gilt, dann folgt \(\int_a^b f(x)~dx \geq \int_a^b g(x)~dx. \)
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Wie benutzen die Eigenschaften der Unter\sphinxhyphen{} und Obersummen und erhalten
\begin{equation*}
\begin{split} U(f,T') + U(g,T') \leq U(f+g,T') \leq O(f+g,T) \leq O(f,T) + O(g,T) .\end{split}
\end{equation*}
Mit der Integrierbarkeit von \(f\) und \(g\) folgt daraus
\begin{equation*}
\begin{split} \int_a^b f(x)~dx + \int_a^b g(x)~dx \leq U(f+g) \leq O(f+g) \leq \int_a^b f(x)~dx + \int_a^b g(x)~dx.\end{split}
\end{equation*}
Damit ist
\begin{equation*}
\begin{split} U(f+g) = O(f+g) = \int_a^b f(x)~dx + \int_a^b g(x)~dx,\end{split}
\end{equation*}
also ist \(f+g\) integrierbar und es gilt die gewünschte Eigenschaft für die Summe.
Ist \(c \geq 0\), dann wissen wir für alle Zerlegungen
\( U(cf,T) = c U(f,T)\), also auch \(U(cf) = c U(f)\). Analog folgt \(O(cf) = c O(f)\) und damit für integrierbare \(f\) auch die
Integrierbarkeit von \(c f\) mit
\begin{equation*}
\begin{split}  \int_a^b c f(x)~dx = U(cf) = O(cf) = c  \int_a^b f(x)~dx.\end{split}
\end{equation*}
Ist \(c \leq 0\), dann benutzen wir
\begin{equation*}
\begin{split} U(cf,T) = U(-(-c)f,T) = _- O((-c)f,T) = - (-c) O(f,T) = c O(f,T)\end{split}
\end{equation*}
und analog \(O(cf,T) = c U(f,T)\). Damit folgern wir
\begin{equation*}
\begin{split} U(cf) = \inf_T U(cf,T) = \inf_T (c O(f,T)) = c \sup_T O(f,t) = c \int_a^b f(x)~dx\end{split}
\end{equation*}
und analog \(O(cf) = \int_a^b f(x)~dx \). Also ist \(cf\) integrierbar und es gilt
\begin{equation*}
\begin{split}  \int_a^b c f(x)~dx = c \int_a^b f(x)~dx.\end{split}
\end{equation*}
Für die Monotonie genügt wegen der Linearität zu zeigen, dass \(\int_a^b f(x)~dx \geq 0\) für nichtnegative Funktionen \(f\) gilt. Dies folgt aber direkt aus \(U(f,T) \geq 0\) für alle \(T\). \(\square\)
\end{sphinxadmonition}

Wir haben vorher schon angekündigt, dass wir das Integral als Differenz der Flächen unter positivem und negativem Teil der Funktion sehen wollen. Um dies präziser zu machen definieren wir
\begin{equation*}
\begin{split} f_+(x) = \begin{pmatrix} f(x) & f(x) \geq 0 \\ 0 & \text{sonst} \end{pmatrix}.\end{split}
\end{equation*}
und
\begin{equation*}
\begin{split}  f_-(x) = \begin{pmatrix}-f(x) & f(x) \leq 0 \\ 0 & \text{sonst} \end{pmatrix}.\end{split}
\end{equation*}
Dann ist \(f = f_+ - f_-\) und \(|f|=f_+ + f_-\). Man kann zeigen, dass die Integrierbarkeit von \(f\) äquivalent zur Integrierbarkeit von \(f_+\) und \(f_-\) ist und es gilt wegen der Linearität auch
\begin{equation*}
\begin{split} \int_a^b f(x)~dx = \int_a^b f_+(x)~dx - \int_a^b f_-(x)~dx.\end{split}
\end{equation*}
Eine weitere interessante Eigenschaft, die wir hier ohne Beweis angeben, ist die Additivität des Integrals bezüglich Teilintervallen: Sei \(f\) auf \([a,c]\) integrierbar und \(b \in (a,c)\), dann sind die Einschränkungen von \(f\) auch auf \([a,b]\) und \([b,c]\) integrierbar und es gilt
\begin{equation*}
\begin{split} \int_a^c f(x)~dx =  \int_a^b f(x)~dx + \int_b^c f(x)~dx .\end{split}
\end{equation*}
Wir haben bisher die Klasse der integrierbaren Funktionen eingeführt und wenige Beispiele kennen gelernt. Der folgende Satz zeigt, dass diese Klasse sehr viele Funktionen enthält:
\label{integration/riemann:theorem-3}
\begin{sphinxadmonition}{note}{Theorem 7.1}



Sei \(f:[a,b] \rightarrow \R\) stetig. Dann ist \(f\) integrierbar.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof.  \(f\) ist stetig, damit auf dem kompakten Intervall \([a,b]\) sogar gleichmäßig stetig. Also gibt es für jedes \(\epsilon >0 \) ein \(\delta > 0\), sodass für alle \(x,y\) mit \(|x-y| < \delta\) gilt:
\begin{equation*}
\begin{split} \vert f(x) - f(y) \vert < \frac{\epsilon}{b-a}.\end{split}
\end{equation*}
Sei \(t_i = a + \frac{b-a}{n} i\) mit \(n > \frac{b-a}\delta\). Dann gilt für alle \(x,y \in [t_i,t_{i+1}]\) auch \(|x-y| < \delta\), somit folgt
\(S_i - s_i < \frac{\epsilon}{b-a}\). Also erhalten wir
\begin{equation*}
\begin{split} 0 \leq O(f,T) - U(f,T) = \sum_{i=0}^{n-1} (t_{i+1}-t_i)(S_i-s_i) < \sum_{i=0}^{n-1} \frac{b-a}{n} ~ \frac{\epsilon}{b-a} = \epsilon.\end{split}
\end{equation*}
Daraus folgern wir dann auch \(O(f) = U(f)\), also ist \(f\) integrierbar. \(\square\)
\end{sphinxadmonition}


\section{Der Hauptsatz der Differential\sphinxhyphen{} und Integralrechnung}
\label{\detokenize{integration/hdi:der-hauptsatz-der-differential-und-integralrechnung}}\label{\detokenize{integration/hdi::doc}}
Nun wollen wir uns noch mit dem Zusammenhang zwischen Differential\sphinxhyphen{} und Integralrechnung beschäftigen. Wir betrachten dazu lokal integrierbare Funktionen \(f:[a,b] \rightarrow \R\), d.h. Funktionen, die auf jedem Teilintervall von \([a,b]\) integrierbar sind.

Wir nennen \(F:[a,b] \rightarrow \R\) unbestimmtes Integral von \(f\), wenn für \(y,z \in [a,b]\) gilt
\begin{equation*}
\begin{split} F(z) - F(y) = \int_y^z f(x)~dx.\end{split}
\end{equation*}
(Wir beachten, dass wir zur Konsistenz ein Integral mit vertauschten Integralgrenzen mit negativem Vorzeichen definieren, d.h.
\(\int_z^y f(x)~dx = - \int_y^z f(x)~dx. \))

Wir nennen \(F:[a,b] \rightarrow \R\) Stammfunktion von \(f\), wenn \(F'(x)=f(x)\) für alle \(x \in [a,b]\) gilt. Der Inhalt des Hauptsatzes der Integralrechnung ist es zu zeigen, dass die Konzepte Stammfunktion und unbestimmtes Integral auf das Gleiche führen, d.h. die Integration ist damit die Umkehrung der Differentiation.
Zunächst wollen wir einige Resultate über Stammfunktionen herleiten:
\label{integration/hdi:lemma-0}
\begin{sphinxadmonition}{note}{Lemma 7.3}



Sei \(F\) eine Stammfunktion von \(f\), dann ist \(G\) genau dann Stammfunktion, wenn \(F-G\) konstant ist.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Ist \(G\) eine Stammfunktion, dann gilt \((F-G)'=F'-G'= f-f=0\) überall in \([a,b]\). Wenden wir den Mittelwertsatz der Differentialrechnung an, so folgt für alle \(y,z\) und ein \(x \in (y,z)\)
\begin{equation*}
\begin{split} \frac{(F-G)(z) - (F-G)(y)}{z-y} = (F-G)'(x) = 0.\end{split}
\end{equation*}
Also ist \((F-G)(z) = (F-G)(y)\) für alle \(y,z \in  [a,b]\), d.h. \(F-G\) ist konstant.
Ist umgekehrt \(F-G\) konstant, dann folgt sofort \(G' = F' + (G-F)' = F'=f\), also ist \(G\) Stammfunktion. \(\square\)
\end{sphinxadmonition}
\label{integration/hdi:lemma-1}
\begin{sphinxadmonition}{note}{Lemma 7.4}



Sei \(f:[a,b]\rightarrow \R\) beschränkt und \(F\) Stammfunktion oder unbestimmtes Integral. Dann ist \(F\) Lipschitz\sphinxhyphen{}stetig mit Konstante \(L=\sup_{x \in [a,b]} \vert f(x) \vert. \)
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Wir haben aus den Abschätzungen für Unter\sphinxhyphen{} und Obersummen schon gesehen, das
\begin{equation*}
\begin{split} \vert \int_y^z f(x)~dx \vert \leq \vert z -y \vert ~\sup_{x \in [a,b]} \vert f(x) \vert\end{split}
\end{equation*}
gilt, also folgt die Aussage für ein unbestimmtes Integral. Ist \(F\) Stammfunktion, so impliziert der Mittelwertsatz
\begin{equation*}
\begin{split} \vert F(y) - F(z) \vert = \vert y - z \vert ~\vert f(x) \vert,\end{split}
\end{equation*}
für ein \(x \in (y,z)\) und die letzte Term ist kleiner als das Supremum. \(\square\).
\end{sphinxadmonition}

Ein wichtiges Resultat, ähnlich bei der Differentiation, ist der Mittelwertsatz der Integralrechnung:
\label{integration/hdi:theorem-2}
\begin{sphinxadmonition}{note}{Theorem 7.2}



Sei \(f\) stetig auf \([a,b]\) und \(y,z \in [a,b]\), \(y < z\). Dann gibt es ein \(x_0 \in (y,z)\) mit
\begin{equation*}
\begin{split} f(x_0) = \frac{\int_y^z f(x)~dx}{z-y}.\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. \(f\) ist eine stetige Funktion auf dem kompakten Intervall \([y,z]\), nimmt dort also sein Maximum und Minimum an. Damit gibt es
\begin{equation*}
\begin{split} f(x_-) = \min_{x \in [y,z]} f(x) \leq \frac{\int_y^z f(x)~dx}{z-y} \leq  \max_{x \in [y,z]} f(x) = f(x_+).\end{split}
\end{equation*}
Nach dem Zwischenwertsatz für stetige Funktionen gibt es also ein \(x_0\) zwischen \(x_+\) und \(x_-\) mit
\begin{equation*}
\begin{split} f(x_0) = \frac{\int_y^z f(x)~dx}{z-y}. \square\end{split}
\end{equation*}\end{sphinxadmonition}

Nun können wir den wichtigsten Satz zum Zusammenhang zwischen Differentiation und Integration beweisen, den Hauptsatz der Differential\sphinxhyphen{} und Integralrechnung:
\label{integration/hdi:theorem-3}
\begin{sphinxadmonition}{note}{Theorem 7.3}



Sei \(f:[a,b] \rightarrow \R\) stetig. Dann ist\(F_a: [a,b] \rightarrow \R, x \mapsto \int_a^x f(y)~dy\)
eine Stammfunktion. Eine Funktion \(F:[a,b] \rightarrow \R\) ist genau dann Stammfunktion von \(f\), wenn \(F\) ein unbestimmtes Integral ist.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Es gilt nach dem Mittelwertsatz
\begin{equation*}
\begin{split} \frac{F_a(x)-F_a(x_0)}{x-x_0} = \frac{\int_{x_0}^x f(y)~dy }{x-x_0} = f(\xi(x))\end{split}
\end{equation*}
für ein \(\xi(x)\) zwischen \(x\) und \(x_0\). Damit folgt für den Grenzwert
\begin{equation*}
\begin{split} F_a'(x_0) = \lim_{x\rightarrow x_0}\frac{F_a(x)-F_a(x_0)}{x-x_0} =  \lim_{x\rightarrow x_0}f(\xi(x)) = f(x_0),\end{split}
\end{equation*}
da \(f\) stetig ist und \(\xi(x) \rightarrow x_0\) für \(x \rightarrow x_0\). Also ist \(F_a\) Stammfunktion.
Analog zeigt man für jedes unbestimmte Integral \(F\), dass \(F'=f\) gilt.
Sei umgekehrt \(F\) Stammfunktion, dann wissen wir, dass \(F-F_a\) konstant ist und damit
\begin{equation*}
\begin{split} F(z) - F(y) = F_a(z) - F_a(y) = \int_y^z f(y)~dy,\end{split}
\end{equation*}
also ist \(F\) auch unbestimmtes Integral. \(\square\)
\end{sphinxadmonition}

Wir sehen also, dass die Begriffe des unbestimmten Integrals und der Stammfunktionen übereinstimmen. Deshalb sprechen wir im Folgenden nur noch von Stammfunktionen. Bei Funktionen \(f\), von denen wir bereits wissen, dass sie die Ableitung einer Funktion \(F\) sind, können wir also den Hauptsatz der Differential\sphinxhyphen{} und Integralrechnung benutzen, um bestimmte Integrale zu berechnen, es gilt ja
\begin{equation*}
\begin{split} \int_a^b f(x)~dx = F(b) - F(a) =: F\vert_a^b.\end{split}
\end{equation*}\label{integration/hdi:example-4}
\begin{sphinxadmonition}{note}{Example 7.3}



Wir wissen das die Exponentialfunktion die Ableitung der Exponentialfunktion ist, also gilt
\begin{equation*}
\begin{split} \int_a^b e^x ~dx = e^x\vert_a^b = e^b -e^a.\end{split}
\end{equation*}
Allgemeiner ist \(e^{cx}\) die Ableitung von \(\frac{1}c e^{cx}\), als
\begin{equation*}
\begin{split} \int_a^b e^{cx} ~dx =\frac{1}c (e^{cx})\vert_a^b = \frac{1}c (e^{cb} -e^{ca}).\end{split}
\end{equation*}\end{sphinxadmonition}

Das letzte Beispiel können wir folgendermaßen verallgemeinern: Ist \(f\) die Ableitung von \(F\), dann ist\(x \mapsto f(c x)\) die Ableitung von \(x\mapsto \frac{1}c F(cx)\), als
\begin{equation*}
\begin{split} \int_a^b f(cx)~dx = \frac{1}c (F(cb) - F(ca)).\end{split}
\end{equation*}\label{integration/hdi:example-5}
\begin{sphinxadmonition}{note}{Example 7.4}



Sei \(f(x) = x^m\), dann ist \(F(x) = \frac{x^{m+1}}{m+1}\) Stammfunktion.
\begin{equation*}
\begin{split} \int_a^b x^m ~dx =\frac{1}{m+1}  (b^{m+1} -a^{m+1}).\end{split}
\end{equation*}
Im Fall \(m=0\) erhalten wir als Integral \(b-a\), also genau die Rechtecksfläche,im Fall \(m=1\) ist das Integral \(\frac{1}2 (b^2-a^2) = \frac{1}2 (b-a)(b+a)\) die Dreiecksfläche.
\end{sphinxadmonition}

Mit Hilfe von Differentiationsregeln können wir auch weitere Integrationsregeln erhalten. Die wahrscheinlich
wichtigste ist dabei die \{\textbackslash{}em partielle Integration\}, die wir aus der Produktregel erhalten:
\label{integration/hdi:theorem-6}
\begin{sphinxadmonition}{note}{Theorem 7.4}



Seien \(f:[a,b] \rightarrow \R\), \(g:[a,b] \rightarrow \R\) stetig differenzierbar. Dann gilt
\begin{equation*}
\begin{split} \int_a^b f(x) g'(x) ~dx = - \int_a^b f'(x) g(x)~dx + (fg)\vert_a^b.\end{split}
\end{equation*}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}
Proof. Aus der Produktregel und dem Hauptsatz der Differential\sphinxhyphen{} und Integralrechnung folgt
\begin{equation*}
\begin{split} (fg)\vert_a^b = \int_a^b (fg)'(x)~dx = \int_a^b (f'(x)g(x) + f(x)g'(x))~dx.\end{split}
\end{equation*}\end{sphinxadmonition}


\section{Vertauschung von Integralen und Taylor\sphinxhyphen{}Formel}
\label{\detokenize{integration/vertauschen:vertauschung-von-integralen-und-taylor-formel}}\label{\detokenize{integration/vertauschen::doc}}
Oft integriert man eine Funktion mehrmals, d.h. man hat eigentlich ein weiteres Integral der Stammfunktion. Wir wollen hier eine Formel für die zweifache Integration herleiten. Sei \(F\) Stammfunktion von \(f\), dann gilt
\begin{equation*}
\begin{split} \int_a^b (F(x) - F(a))~dx = \int_a^b \int_a^x f(y)~dy~dx.\end{split}
\end{equation*}
Intuitiv integrieren wir hier über das Dreieck \(0 \leq y \leq x \leq b\), also erwarten wir
\begin{equation*}
\begin{split} \int_a^b \int_a^x f(y)~dy = \int_a^b \int_y^b f(y)~dy = \int_a^b f(y) (b-y)~dy.\end{split}
\end{equation*}
Um die Gleichheit nachzuweisen, können wir partielle Integration verwenden, mit der Stammfunktion \(F_a\) wie oben und \(g(x) = b-x\) erhalten wir, da \(g'(x)=-1\), \(F_a(a) = 0 \) und \(g(b) =0\),
\begin{equation*}
\begin{split} \int_a^b f(y) (b-y)~dy = - \int_a^b F_a(y) (-1)~dy = \int_a^b F_a(x)~dx =  \int_a^b \int_a^x f(x)~dx .\end{split}
\end{equation*}
Eine Anwendung ist die bessere Charakterisierung von Restgliedern bei der Differentialrechnung. Sei \(f\) eine in \([a,b]\) stetig differenzierbare Funktion, dann gilt für \(x,x_0 \in [a,b]\)
\begin{align*}
f(x) - f(x_0) &= \int_{x_0}^x f'(y)~dy =  \int_{x_0}^x (f'(x_0) + f'(y) -f'(x_0))~dy \\
&= f'(x_0) (x-x_0) + \int_{x_0}^x ( f'(y) -f'(x_0))~dy \\&= f'(x_0) (x-x_0) + \int_{x_0}^x \frac{y-x_0}{x-x_0} \frac{ f'(y) -f'(x_0))}{y-x_0}~dy  (x-x_0) .
\end{align*}
Also ist das Restglied
\begin{equation*}
\begin{split}R(x) = \int_{x_0}^x \frac{y-x_0}{x-x_0} \frac{ f'(y) -f'(x_0))}{y-x_0}~dy .\end{split}
\end{equation*}
Ist \(f\) zweimal stetig differenzierbar, dann können wir den Mittelwertsatz auf \(f'\) anwenden und erhalten
\begin{equation*}
\begin{split} R(x) = \int_{x_0}^x \frac{y-x_0}{x-x_0} f''(\xi(y))~dy,\end{split}
\end{equation*}
damit auch
\begin{equation*}
\begin{split} |R(x)| \leq  \int_{x_0}^x \frac{y-x_0}{x-x_0} \sup_{z \in (x_0,x)} |f''(z)|~dy = \sup_{z \in (x_0,x)} |f''(z)| ~ \frac{|x-x_0|}2.\end{split}
\end{equation*}





\renewcommand{\indexname}{Proof Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{definition\sphinxhyphen{}0}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}0}\sphinxstyleindexextra{metrik/teilfolgen}\sphinxstyleindexpageref{metrik/teilfolgen:\detokenize{definition-0}}
\indexspace
\bigletter{definition\sphinxhyphen{}1}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}1}\sphinxstyleindexextra{metrik/teilfolgen}\sphinxstyleindexpageref{metrik/teilfolgen:\detokenize{definition-1}}
\indexspace
\bigletter{definition\sphinxhyphen{}10}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}10}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{definition-10}}
\indexspace
\bigletter{definition\sphinxhyphen{}11}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}11}\sphinxstyleindexextra{grundlagen/mengenlogik}\sphinxstyleindexpageref{grundlagen/mengenlogik:\detokenize{definition-11}}
\indexspace
\bigletter{definition\sphinxhyphen{}12}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}12}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{definition-12}}
\indexspace
\bigletter{definition\sphinxhyphen{}13}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}13}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{definition-13}}
\indexspace
\bigletter{definition\sphinxhyphen{}14}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}14}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{definition-14}}
\indexspace
\bigletter{definition\sphinxhyphen{}17}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}17}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{definition-17}}
\indexspace
\bigletter{definition\sphinxhyphen{}18}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}18}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{definition-18}}
\indexspace
\bigletter{definition\sphinxhyphen{}19}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}19}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{definition-19}}
\indexspace
\bigletter{definition\sphinxhyphen{}2}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}2}\sphinxstyleindexextra{vorkurs/stetigkeit}\sphinxstyleindexpageref{vorkurs/stetigkeit:\detokenize{definition-2}}
\indexspace
\bigletter{definition\sphinxhyphen{}27}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}27}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{definition-27}}
\indexspace
\bigletter{definition\sphinxhyphen{}3}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}3}\sphinxstyleindexextra{stetigkeit/stetigkeit}\sphinxstyleindexpageref{stetigkeit/stetigkeit:\detokenize{definition-3}}
\indexspace
\bigletter{definition\sphinxhyphen{}4}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}4}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{definition-4}}
\indexspace
\bigletter{definition\sphinxhyphen{}5}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}5}\sphinxstyleindexextra{vektorraeume/vektorraeume}\sphinxstyleindexpageref{vektorraeume/vektorraeume:\detokenize{definition-5}}
\indexspace
\bigletter{definition\sphinxhyphen{}6}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}6}\sphinxstyleindexextra{metrik/teilfolgen}\sphinxstyleindexpageref{metrik/teilfolgen:\detokenize{definition-6}}
\indexspace
\bigletter{definition\sphinxhyphen{}7}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}7}\sphinxstyleindexextra{metrik/konvfolgen}\sphinxstyleindexpageref{metrik/konvfolgen:\detokenize{definition-7}}
\indexspace
\bigletter{definition\sphinxhyphen{}8}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}8}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{definition-8}}
\indexspace
\bigletter{definition\sphinxhyphen{}9}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}9}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{definition-9}}
\indexspace
\bigletter{example\sphinxhyphen{}0}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}0}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{example-0}}
\indexspace
\bigletter{example\sphinxhyphen{}1}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}1}\sphinxstyleindexextra{metrik/reihen}\sphinxstyleindexpageref{metrik/reihen:\detokenize{example-1}}
\indexspace
\bigletter{example\sphinxhyphen{}10}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}10}\sphinxstyleindexextra{stetigkeit/stetigkeit}\sphinxstyleindexpageref{stetigkeit/stetigkeit:\detokenize{example-10}}
\indexspace
\bigletter{example\sphinxhyphen{}11}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}11}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{example-11}}
\indexspace
\bigletter{example\sphinxhyphen{}12}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}12}\sphinxstyleindexextra{vektorraeume/vektorraeume}\sphinxstyleindexpageref{vektorraeume/vektorraeume:\detokenize{example-12}}
\indexspace
\bigletter{example\sphinxhyphen{}13}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}13}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{example-13}}
\indexspace
\bigletter{example\sphinxhyphen{}14}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}14}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{example-14}}
\indexspace
\bigletter{example\sphinxhyphen{}16}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}16}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{example-16}}
\indexspace
\bigletter{example\sphinxhyphen{}2}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}2}\sphinxstyleindexextra{metrik/teilfolgen}\sphinxstyleindexpageref{metrik/teilfolgen:\detokenize{example-2}}
\indexspace
\bigletter{example\sphinxhyphen{}20}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}20}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{example-20}}
\indexspace
\bigletter{example\sphinxhyphen{}21}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}21}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{example-21}}
\indexspace
\bigletter{example\sphinxhyphen{}25}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}25}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{example-25}}
\indexspace
\bigletter{example\sphinxhyphen{}3}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}3}\sphinxstyleindexextra{metrik/konvfolgen}\sphinxstyleindexpageref{metrik/konvfolgen:\detokenize{example-3}}
\indexspace
\bigletter{example\sphinxhyphen{}32}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}32}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{example-32}}
\indexspace
\bigletter{example\sphinxhyphen{}33}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}33}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{example-33}}
\indexspace
\bigletter{example\sphinxhyphen{}4}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}4}\sphinxstyleindexextra{differential/kombfkt}\sphinxstyleindexpageref{differential/kombfkt:\detokenize{example-4}}
\indexspace
\bigletter{example\sphinxhyphen{}5}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}5}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{example-5}}
\indexspace
\bigletter{example\sphinxhyphen{}6}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}6}\sphinxstyleindexextra{metrik/reihen}\sphinxstyleindexpageref{metrik/reihen:\detokenize{example-6}}
\indexspace
\bigletter{example\sphinxhyphen{}7}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}7}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{example-7}}
\indexspace
\bigletter{example\sphinxhyphen{}8}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}8}\sphinxstyleindexextra{metrik/reihen}\sphinxstyleindexpageref{metrik/reihen:\detokenize{example-8}}
\indexspace
\bigletter{example\sphinxhyphen{}9}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}9}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{example-9}}
\indexspace
\bigletter{lemma\sphinxhyphen{}0}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}0}\sphinxstyleindexextra{stetigkeit/trig}\sphinxstyleindexpageref{stetigkeit/trig:\detokenize{lemma-0}}
\indexspace
\bigletter{lemma\sphinxhyphen{}1}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}1}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{lemma-1}}
\indexspace
\bigletter{lemma\sphinxhyphen{}13}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}13}\sphinxstyleindexextra{vektorraeume/vektorraeume}\sphinxstyleindexpageref{vektorraeume/vektorraeume:\detokenize{lemma-13}}
\indexspace
\bigletter{lemma\sphinxhyphen{}15}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}15}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{lemma-15}}
\indexspace
\bigletter{lemma\sphinxhyphen{}19}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}19}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{lemma-19}}
\indexspace
\bigletter{lemma\sphinxhyphen{}2}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}2}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{lemma-2}}
\indexspace
\bigletter{lemma\sphinxhyphen{}20}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}20}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{lemma-20}}
\indexspace
\bigletter{lemma\sphinxhyphen{}21}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}21}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{lemma-21}}
\indexspace
\bigletter{lemma\sphinxhyphen{}22}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}22}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{lemma-22}}
\indexspace
\bigletter{lemma\sphinxhyphen{}23}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}23}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{lemma-23}}
\indexspace
\bigletter{lemma\sphinxhyphen{}3}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}3}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{lemma-3}}
\indexspace
\bigletter{lemma\sphinxhyphen{}4}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}4}\sphinxstyleindexextra{metrik/teilfolgen}\sphinxstyleindexpageref{metrik/teilfolgen:\detokenize{lemma-4}}
\indexspace
\bigletter{lemma\sphinxhyphen{}9}
\item\relax\sphinxstyleindexentry{lemma\sphinxhyphen{}9}\sphinxstyleindexextra{metrik/reihen}\sphinxstyleindexpageref{metrik/reihen:\detokenize{lemma-9}}
\indexspace
\bigletter{nullfolgenmetrik}
\item\relax\sphinxstyleindexentry{nullfolgenmetrik}\sphinxstyleindexextra{metrik/konvfolgen}\sphinxstyleindexpageref{metrik/konvfolgen:\detokenize{nullfolgenmetrik}}
\indexspace
\bigletter{theorem\sphinxhyphen{}0}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}0}\sphinxstyleindexextra{differential/kombfkt}\sphinxstyleindexpageref{differential/kombfkt:\detokenize{theorem-0}}
\indexspace
\bigletter{theorem\sphinxhyphen{}1}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}1}\sphinxstyleindexextra{metrik/potenzreihen}\sphinxstyleindexpageref{metrik/potenzreihen:\detokenize{theorem-1}}
\indexspace
\bigletter{theorem\sphinxhyphen{}10}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}10}\sphinxstyleindexextra{vektorraeume/vektorraeume}\sphinxstyleindexpageref{vektorraeume/vektorraeume:\detokenize{theorem-10}}
\indexspace
\bigletter{theorem\sphinxhyphen{}11}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}11}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{theorem-11}}
\indexspace
\bigletter{theorem\sphinxhyphen{}12}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}12}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{theorem-12}}
\indexspace
\bigletter{theorem\sphinxhyphen{}15}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}15}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{theorem-15}}
\indexspace
\bigletter{theorem\sphinxhyphen{}16}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}16}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{theorem-16}}
\indexspace
\bigletter{theorem\sphinxhyphen{}17}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}17}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{theorem-17}}
\indexspace
\bigletter{theorem\sphinxhyphen{}18}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}18}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{theorem-18}}
\indexspace
\bigletter{theorem\sphinxhyphen{}2}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}2}\sphinxstyleindexextra{stetigkeit/glm}\sphinxstyleindexpageref{stetigkeit/glm:\detokenize{theorem-2}}
\indexspace
\bigletter{theorem\sphinxhyphen{}24}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}24}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{theorem-24}}
\indexspace
\bigletter{theorem\sphinxhyphen{}26}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}26}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{theorem-26}}
\indexspace
\bigletter{theorem\sphinxhyphen{}28}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}28}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{theorem-28}}
\indexspace
\bigletter{theorem\sphinxhyphen{}29}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}29}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{theorem-29}}
\indexspace
\bigletter{theorem\sphinxhyphen{}3}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}3}\sphinxstyleindexextra{metrik/teilfolgen}\sphinxstyleindexpageref{metrik/teilfolgen:\detokenize{theorem-3}}
\indexspace
\bigletter{theorem\sphinxhyphen{}30}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}30}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{theorem-30}}
\indexspace
\bigletter{theorem\sphinxhyphen{}31}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}31}\sphinxstyleindexextra{grundlagen/zahlensysteme}\sphinxstyleindexpageref{grundlagen/zahlensysteme:\detokenize{theorem-31}}
\indexspace
\bigletter{theorem\sphinxhyphen{}4}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}4}\sphinxstyleindexextra{stetigkeit/stetigkeit}\sphinxstyleindexpageref{stetigkeit/stetigkeit:\detokenize{theorem-4}}
\indexspace
\bigletter{theorem\sphinxhyphen{}5}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}5}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{theorem-5}}
\indexspace
\bigletter{theorem\sphinxhyphen{}6}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}6}\sphinxstyleindexextra{vektorraeume/LGS}\sphinxstyleindexpageref{vektorraeume/LGS:\detokenize{theorem-6}}
\indexspace
\bigletter{theorem\sphinxhyphen{}7}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}7}\sphinxstyleindexextra{metrik/reihen}\sphinxstyleindexpageref{metrik/reihen:\detokenize{theorem-7}}
\indexspace
\bigletter{theorem\sphinxhyphen{}8}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}8}\sphinxstyleindexextra{metrik/konvfolgen}\sphinxstyleindexpageref{metrik/konvfolgen:\detokenize{theorem-8}}
\indexspace
\bigletter{theorem\sphinxhyphen{}9}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}9}\sphinxstyleindexextra{vektorraeume/vektorraeume}\sphinxstyleindexpageref{vektorraeume/vektorraeume:\detokenize{theorem-9}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}